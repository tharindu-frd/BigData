{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e503f2b5-fae9-4a52-aae9-f7cf98ea18d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [FileInfo(path='dbfs:/FileStore/tables/baby_names-2.csv', name='baby_names-2.csv', size=7447879, modificationTime=1721198338000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/baby_names-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850a0618-9c82-447e-8ed8-cb2ce6c10b96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258000\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"inferSchema\",True).option(\"header\",True).option(\"sep\",\",\").load(\"/FileStore/tables/baby_names-2.csv\")\n",
    "# display(df)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ff996cd-e462-4645-bf7a-ee91e1f90eaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "### Read multiple csv files \n",
    " df = spark.read.format(\"csv\").option(\"inferSchema\",True).option(\"header\",True).option(\"sep\",\",\").load(\"/FileStore/tables/baby_names-2.csv\",\"/FileStore/tables/baby_names-2.csv\") \n",
    "\n",
    "### Read all csv files under a folder \n",
    "df = spark.read.format(\"csv\").option(\"inferSchema\",True).option(\"header\",True).option(\"sep\",\",\").load(\"/FileStore/tables/\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7b2dbf-47d3-490f-98b1-81cc08ac118b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- year: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- percent: double (nullable = true)\n |-- sex: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aefda581-9d5d-4a6d-a3d6-15ce420182fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+---+\n|year|   name| percent|sex|\n+----+-------+--------+---+\n|1880|   John|0.081541|boy|\n|1880|William|0.080511|boy|\n|1880|  James|0.050057|boy|\n|1880|Charles|0.045167|boy|\n|1880| George|0.043292|boy|\n|1880|  Frank| 0.02738|boy|\n|1880| Joseph|0.022229|boy|\n|1880| Thomas|0.021401|boy|\n|1880|  Henry|0.020641|boy|\n|1880| Robert|0.020404|boy|\n|1880| Edward|0.019965|boy|\n|1880|  Harry|0.018175|boy|\n|1880| Walter|0.014822|boy|\n|1880| Arthur|0.013504|boy|\n|1880|   Fred|0.013251|boy|\n|1880| Albert|0.012609|boy|\n|1880| Samuel|0.008648|boy|\n|1880|  David|0.007339|boy|\n|1880|  Louis|0.006993|boy|\n|1880|    Joe|0.006174|boy|\n+----+-------+--------+---+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "########  Create own shcema ######### \n",
    "from pyspark.sql.types import StructType , StructField , IntegerType , StringType,FloatType\n",
    "\n",
    "schema_defined = StructType([\n",
    "    StructField('year',IntegerType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('percent',FloatType(),True),\n",
    "    StructField('sex',StringType(),True),\n",
    "                             ])\n",
    "'''\n",
    "## Similarly we can also define the schema as ,\n",
    "schema_alternate = 'year INTEGER , name STRING,percent FLOAT , sex STRING'\n",
    "'''\n",
    "\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(schema_defined).option(\"header\",True).option(\"sep\",\",\").load(\"/FileStore/tables/baby_names-2.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04752f6-cfd0-4373-8231-727da4805ace",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- year: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- percent: float (nullable = true)\n |-- sex: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653fe81f-2b23-474a-836a-2fdb3a429c3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "###############################  Fileter Condition   ##############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "employee_data = [\n",
    "    (10,\"Raj Kumar\",\"1999\",\"100\",\"M\",2000),\n",
    "    (20,\"Rahul Rajhan\",\"2002\",\"200\",\"F\",8000),\n",
    "    (30,\"Raghav\",\"2010\",\"100\",None,6000),\n",
    "    (40,\"Raja Singh\",\"2004\",\"100\",\"F\",7000),\n",
    "    (50,\"Rama Krish\",\"2008\",\"400\",\"M\",1000),\n",
    "    (60,\"Rasul\",\"2014\",\"500\",\"M\",5000),\n",
    "    (70,\"Kumar Chand\",\"2004\",\"600\",\"M\",5000),\n",
    "]\n",
    "\n",
    "employee_schema = ['employee_id','name','doj','employee_dept_id','gender','salary']\n",
    "employeeDF = spark.createDataFrame(data=employee_data ,schema=employee_schema)\n",
    "display(employeeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55340ac-99b6-41c8-8466-443f3fadd07d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(employeeDF)\n",
    "display(employeeDF.filter(employeeDF.salary<=5000))   ### == , > , < , <= , >= , !="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f271f6-52f4-4b6f-9e03-bbfc46efaec6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(employeeDF)\n",
    "display(employeeDF.filter((employeeDF.gender==\"F\") | (employeeDF.doj==204))) # & , | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb712117-5eb4-4417-b073-b29672b62d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(employeeDF)\n",
    "display(employeeDF.filter(employeeDF.name.endswith(\"h\")))  # startswith , endswith , contains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be649b95-d160-48aa-b590-e1b902a4a498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(employeeDF)\n",
    "display(employeeDF.filter(employeeDF.gender.isNull()))  # isNull , isNotNull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e567d4-c7db-4b14-9576-1b3647f67fb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(employeeDF)\n",
    "display(employeeDF.filter(~employeeDF.employee_dept_id.isin(100,500))) # isin , ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a91dffe-43da-43bf-b856-9e28b656d2be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(employeeDF.filter(employeeDF.name.like(\"%Kumar%\")))  # contain 'Kumar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd4340c5-8b25-4d73-af6c-51fd5183b0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721ce474-b931-47cc-9b54-e2671a032392",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################  Add , Drop and Rename Columns   #######################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994e1bdf-1a53-455d-9cc4-b59aef847dea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966d30c4-8419-4eff-b283-651d815f74e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----+----------------+------+------+--------+\n|employee_id|        name| doj|employee_dept_id|gender|salary|Location|\n+-----------+------------+----+----------------+------+------+--------+\n|         10|   Raj Kumar|1999|             100|     M|  2000|  Mumbai|\n|         20|Rahul Rajhan|2002|             200|     F|  8000|  Mumbai|\n|         30|      Raghav|2010|             100|  null|  6000|  Mumbai|\n|         40|  Raja Singh|2004|             100|     F|  7000|  Mumbai|\n|         50|  Rama Krish|2008|             400|     M|  1000|  Mumbai|\n|         60|       Rasul|2014|             500|     M|  5000|  Mumbai|\n|         70| Kumar Chand|2004|             600|     M|  5000|  Mumbai|\n+-----------+------------+----+----------------+------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_data = [\n",
    "    (10,\"Raj Kumar\",\"1999\",\"100\",\"M\",2000),\n",
    "    (20,\"Rahul Rajhan\",\"2002\",\"200\",\"F\",8000),\n",
    "    (30,\"Raghav\",\"2010\",\"100\",None,6000),\n",
    "    (40,\"Raja Singh\",\"2004\",\"100\",\"F\",7000),\n",
    "    (50,\"Rama Krish\",\"2008\",\"400\",\"M\",1000),\n",
    "    (60,\"Rasul\",\"2014\",\"500\",\"M\",5000),\n",
    "    (70,\"Kumar Chand\",\"2004\",\"600\",\"M\",5000),\n",
    "]\n",
    "\n",
    "employee_schema = ['employee_id','name','doj','employee_dept_id','gender','salary']\n",
    "empDF = spark.createDataFrame(data=employee_data ,schema=employee_schema)\n",
    "\n",
    "#### Add a column using constant literal \n",
    "empDF_AddColumn = empDF.withColumn('Location',lit(\"Mumbai\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c339378f-35b5-4441-b56e-58b6ad0d41bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th><th>Bonus</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td><td>200.0</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td><td>800.0</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td><td>600.0</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td><td>700.0</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td><td>100.0</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td><td>500.0</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td><td>500.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000,
         200.0
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000,
         800.0
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000,
         600.0
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000,
         700.0
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000,
         100.0
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000,
         500.0
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000,
         500.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Bonus",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######  Add a new column by calculation\n",
    "empDF_AddColumn = empDF.withColumn('Bonus',empDF.salary*0.1)\n",
    "display(empDF_AddColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa10725-93ee-4c81-b758-04985228b4e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----+----------------+------+------+-----+\n|employee_id|   Full_Name| doj|employee_dept_id|gender|salary|Bonus|\n+-----------+------------+----+----------------+------+------+-----+\n|         10|   Raj Kumar|1999|             100|     M|  2000|200.0|\n|         20|Rahul Rajhan|2002|             200|     F|  8000|800.0|\n|         30|      Raghav|2010|             100|  null|  6000|600.0|\n|         40|  Raja Singh|2004|             100|     F|  7000|700.0|\n|         50|  Rama Krish|2008|             400|     M|  1000|100.0|\n|         60|       Rasul|2014|             500|     M|  5000|500.0|\n|         70| Kumar Chand|2004|             600|     M|  5000|500.0|\n+-----------+------------+----+----------------+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "######### Rename a ccolumn \n",
    "empDF_RenameColumn = empDF_AddColumn.withColumnRenamed(\"Name\",\"Full_Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27222b5b-5cb7-42f8-baf3-bb4f6c3de114",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------------+------+------+-----+\n|employee_id| doj|employee_dept_id|gender|salary|Bonus|\n+-----------+----+----------------+------+------+-----+\n|         10|1999|             100|     M|  2000|200.0|\n|         20|2002|             200|     F|  8000|800.0|\n|         30|2010|             100|  null|  6000|600.0|\n|         40|2004|             100|     F|  7000|700.0|\n|         50|2008|             400|     M|  1000|100.0|\n|         60|2014|             500|     M|  5000|500.0|\n|         70|2004|             600|     M|  5000|500.0|\n+-----------+----+----------------+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "########## Drop a column \n",
    "empDF_DropColumn = empDF_AddColumn.drop(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7826287e-4c9a-4e8b-a765-5457c567aae1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1943407b-b69a-4bc2-b044-ee9e7565e63e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################        Join Methods        #############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8e3a6a-f9a7-4eed-bd00-fca565f98bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####  Join Methods : InNNER / LEFT OUTER / RIGHT OUTER /FULL OUTER / LEFT ANTI / LEFT SEMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be47c802-2559-422a-aef5-c5522dd841bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_data = [\n",
    "    (10,\"Raj Kumar\",\"1999\",\"100\",\"M\",2000),\n",
    "    (20,\"Rahul Rajhan\",\"2002\",\"200\",\"F\",8000),\n",
    "    (30,\"Raghav\",\"2010\",\"100\",None,6000),\n",
    "    (40,\"Raja Singh\",\"2004\",\"100\",\"F\",7000),\n",
    "    (50,\"Rama Krish\",\"2008\",\"400\",\"M\",1000),\n",
    "    (60,\"Rasul\",\"2014\",\"500\",\"M\",5000),\n",
    "    (70,\"Kumar Chand\",\"2004\",\"600\",\"M\",5000),\n",
    "]\n",
    "\n",
    "employee_schema = ['employee_id','name','doj','employee_dept_id','gender','salary']\n",
    "employeeDF = spark.createDataFrame(data=employee_data ,schema=employee_schema)\n",
    "\n",
    "\n",
    "department_data = [\n",
    "    (\"Supply\",200),\n",
    "    (\"Sales\",300),\n",
    "    (\"Stock\",400)\n",
    "]\n",
    "department_schema = ['dept_name','dept_id']\n",
    "departmentDF =spark.createDataFrame(data=department_data,schema = department_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8733c3a1-a465-4f5b-b778-a868c0d864c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dept_name</th><th>dept_id</th></tr></thead><tbody><tr><td>Supply</td><td>200</td></tr><tr><td>Sales</td><td>300</td></tr><tr><td>Stock</td><td>400</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Supply",
         200
        ],
        [
         "Sales",
         300
        ],
        [
         "Stock",
         400
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(departmentDF)\n",
    "display(employeeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da41440e-4892-4fad-9dc1-1722dd6e2710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th><th>dept_name</th><th>dept_id</th></tr></thead><tbody><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td><td>Supply</td><td>200</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td><td>Stock</td><td>400</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000,
         "Supply",
         200
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000,
         "Stock",
         400
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### Inner Join ##########\n",
    "df_join = employeeDF.join(departmentDF,employeeDF.employee_dept_id==departmentDF.dept_id,'inner')\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0c163c-f0b9-4609-aad3-b8dd0e69502f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th><th>dept_name</th><th>dept_id</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td><td>null</td><td>null</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td><td>null</td><td>null</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td><td>null</td><td>null</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td><td>Supply</td><td>200</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Sales</td><td>300</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td><td>Stock</td><td>400</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td><td>null</td><td>null</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000,
         null,
         null
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000,
         null,
         null
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000,
         null,
         null
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000,
         "Supply",
         200
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         "Sales",
         300
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000,
         "Stock",
         400
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000,
         null,
         null
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ Full outer join ######\n",
    "df_join = employeeDF.join(departmentDF,employeeDF.employee_dept_id == departmentDF.dept_id,'full')\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88d238d-c81b-4785-b355-182777286a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th><th>dept_name</th><th>dept_id</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td><td>null</td><td>null</td></tr><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td><td>Supply</td><td>200</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td><td>null</td><td>null</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td><td>null</td><td>null</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td><td>Stock</td><td>400</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td><td>null</td><td>null</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000,
         null,
         null
        ],
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000,
         "Supply",
         200
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000,
         null,
         null
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000,
         null,
         null
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000,
         "Stock",
         400
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000,
         null,
         null
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ Left outer join ######\n",
    "df_join = employeeDF.join(departmentDF,employeeDF.employee_dept_id == departmentDF.dept_id,'left')\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e771c57-cc7c-4914-9d4d-2ee266f31011",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th><th>dept_name</th><th>dept_id</th></tr></thead><tbody><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td><td>Supply</td><td>200</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Sales</td><td>300</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td><td>Stock</td><td>400</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000,
         "Supply",
         200
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         "Sales",
         300
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000,
         "Stock",
         400
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ Right outer join ######\n",
    "df_join = employeeDF.join(departmentDF,employeeDF.employee_dept_id == departmentDF.dept_id,'right')\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99aef5ae-00d0-45d4-9f40-64d2c67581f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>20</td><td>Rahul Rajhan</td><td>2002</td><td>200</td><td>F</td><td>8000</td></tr><tr><td>50</td><td>Rama Krish</td><td>2008</td><td>400</td><td>M</td><td>1000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20,
         "Rahul Rajhan",
         "2002",
         "200",
         "F",
         8000
        ],
        [
         50,
         "Rama Krish",
         "2008",
         "400",
         "M",
         1000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "################### Left Semi Join ###########\n",
    "df_join = employeeDF.join(departmentDF,employeeDF.employee_dept_id == departmentDF.dept_id,'semi')\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60fde06-fb7c-4ef2-b070-8eeda8e4de53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj Kumar</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td>null</td><td>6000</td></tr><tr><td>40</td><td>Raja Singh</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>60</td><td>Rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>70</td><td>Kumar Chand</td><td>2004</td><td>600</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj Kumar",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         null,
         6000
        ],
        [
         40,
         "Raja Singh",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         60,
         "Rasul",
         "2014",
         "500",
         "M",
         5000
        ],
        [
         70,
         "Kumar Chand",
         "2004",
         "600",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############# Left anti join \n",
    "df_join = employeeDF.join(departmentDF,employeeDF.employee_dept_id == departmentDF.dept_id,'anti')\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99b39005-76e9-4949-8ba4-57a5676a31ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e084eb78-b00d-4a32-b811-cb524b1bb7af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################        Utility Commands       ##########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c80950-e4fc-44aa-bcd6-c6463f7fba94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\n",
       "this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\n",
       "another FileSystem URI.\n",
       "\n",
       "For more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n",
       "\n",
       "In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\n",
       "straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\n",
       "translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n",
       "    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ab2145-c605-49b4-bdc8-6df770537c99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "The notebook module.\n",
       "  <h3></h3><b>exit(value: String): void</b> -> This method lets you exit a notebook with a value<br /><b>run(path: String, timeoutSeconds: int, arguments: Map): String</b> -> This method runs a notebook and returns its exit value<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\">\nThe notebook module.\n  <h3></h3><b>exit(value: String): void</b> -> This method lets you exit a notebook with a value<br /><b>run(path: String, timeoutSeconds: int, arguments: Map): String</b> -> This method runs a notebook and returns its exit value<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.notebook.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba0399d-e6dc-4916-a15d-83f220469699",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\"><b>dbutils.widgets</b> provides utilities for working with notebook widgets. You can create\n",
       "different types of widgets and get their bound value.\n",
       "\n",
       "For more info about a method, use <b>dbutils.widgets.help(\"methodName\")</b>.\n",
       "    <h3></h3><b>combobox(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a combobox input widget with a given name, default value and choices<br /><b>dropdown(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a dropdown input widget a with given name, default value and choices<br /><b>get(name: String): String</b> -> Retrieves current value of an input widget<br /><b>getArgument(name: String, optional: String): String</b> -> (DEPRECATED) Equivalent to get<br /><b>multiselect(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a multiselect input widget with a given name, default value and choices<br /><b>remove(name: String): void</b> -> Removes an input widget from the notebook<br /><b>removeAll: void</b> -> Removes all widgets in the notebook<br /><b>text(name: String, defaultValue: String, label: String): void</b> -> Creates a text input widget with a given name and default value<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\"><b>dbutils.widgets</b> provides utilities for working with notebook widgets. You can create\ndifferent types of widgets and get their bound value.\n\nFor more info about a method, use <b>dbutils.widgets.help(\"methodName\")</b>.\n    <h3></h3><b>combobox(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a combobox input widget with a given name, default value and choices<br /><b>dropdown(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a dropdown input widget a with given name, default value and choices<br /><b>get(name: String): String</b> -> Retrieves current value of an input widget<br /><b>getArgument(name: String, optional: String): String</b> -> (DEPRECATED) Equivalent to get<br /><b>multiselect(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a multiselect input widget with a given name, default value and choices<br /><b>remove(name: String): void</b> -> Removes an input widget from the notebook<br /><b>removeAll: void</b> -> Removes all widgets in the notebook<br /><b>text(name: String, defaultValue: String, label: String): void</b> -> Creates a text input widget with a given name and default value<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e488bdd1-3915-4efe-8a54-454520b4c7d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "Provides utilities for leveraging secrets within notebooks.\n",
       "Databricks documentation for more info.\n",
       "    <h3></h3><b>get(scope: String, key: String): String</b> -> Gets the string representation of a secret value with scope and key<br /><b>getBytes(scope: String, key: String): byte[]</b> -> Gets the bytes representation of a secret value with scope and key<br /><b>list(scope: String): Seq</b> -> Lists secret metadata for secrets within a scope<br /><b>listScopes: Seq</b> -> Lists secret scopes<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\">\nProvides utilities for leveraging secrets within notebooks.\nDatabricks documentation for more info.\n    <h3></h3><b>get(scope: String, key: String): String</b> -> Gets the string representation of a secret value with scope and key<br /><b>getBytes(scope: String, key: String): byte[]</b> -> Gets the bytes representation of a secret value with scope and key<br /><b>list(scope: String): Seq</b> -> Lists secret metadata for secrets within a scope<br /><b>listScopes: Seq</b> -> Lists secret scopes<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.secrets.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e8f1f2-5bef-4808-a4e2-246c055d129e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########### Suppose we have a folder called 'assets' and we want to get the details of all the files inside that folder \n",
    "dbutils.fs.ls(\"/FileStore/tables/assets\")\n",
    "\n",
    "\n",
    "##### To see the content of a file \n",
    "dbutils.fs.head(\"/FileStore/tables/assets/myfile.csv\")\n",
    "\n",
    "\n",
    "#### to create a new folder \n",
    "dbutils.fs.mkdir(\"/FileStore/tables/assets/newfolder/\")\n",
    "# dbutils.fs.ls(\"/FileStore/tables/assets/\")\n",
    "\n",
    "\n",
    "#### Move the file from its existing location to a new location\n",
    "dbutils.fs.cp(\"/FileStore/tables/assets/myfile.csv\",\"/FileStore/tables/assets/newfolder\")\n",
    "\n",
    "\n",
    "####### Permenantly move the file \n",
    "dbutils.fs.mv(\"/FileStore/tables/assets/myfile.csv\",\"/FileStore/tables/assets/newfolder\")\n",
    "\n",
    "########  Create a new file \n",
    "dbutils.fs.put(\"/FileStore/tables/assets/test.txt\",\"hello world\")  # name of the file : test.txt   , content : hello world\n",
    "\n",
    "\n",
    "\n",
    "##### Remove a file \n",
    "dbutils.fs.rm(\"/FileStore/tables/assets/test.txt\") \n",
    "\n",
    "#### Remove an entire folder\n",
    "dbutils.fs.rm(\"/FileStore/tables/assets/ \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df00de3-cc6a-427a-a924-a26b6aa8cd21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################  Notebook commands ####################\n",
    "\n",
    "#### Command to execute a notebook\n",
    "dbutils.notebook.run('/06-training',60)   # 60 how long to wait  till notebook opens \n",
    "## so this way by passing the name of another notebook , we can open it here and execute some commands in that notebook over here as well \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be372f8a-aa92-4ecf-8269-27fb6de287be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###################################      widgets    #####################################################\n",
    "\n",
    "\n",
    "'''\n",
    "In Apache Spark, widgets are used primarily within Databricks notebooks to create interactive notebook controls. These widgets allow you to add user interface elements such as dropdowns, text boxes, and sliders, enabling users to interact with and control their notebooks dynamically. Here are some reasons why widgets are useful in Spark:\n",
    "\n",
    "1. Interactive Data Exploration:\n",
    "Dynamic Parameter Adjustment: Widgets allow users to change parameters dynamically without modifying the code, making it easier to explore different scenarios or data subsets.\n",
    "On-the-Fly Filtering: Users can filter data on the fly, which is especially useful when dealing with large datasets.\n",
    "2. Improved Usability:\n",
    "User-Friendly Interface: Widgets provide a more user-friendly interface for non-technical users to interact with notebooks, making data analysis accessible to a broader audience.\n",
    "Simplifies Complex Tasks: For complex tasks that require multiple parameters, widgets simplify the process by providing an easy way to input and adjust values.\n",
    "3. Parameterization of Notebooks:\n",
    "Reusability: Parameterizing notebooks with widgets makes them reusable for different datasets or parameters without changing the code.\n",
    "Automation: Widgets can be used to automate workflows by setting up default parameters that can be modified as needed.\n",
    "4. Visualization and Reporting:\n",
    "Dynamic Visualizations: Widgets can be used to create dynamic visualizations that update based on user input, enhancing the interactivity and usability of reports.\n",
    "Dashboards: By using widgets, you can create interactive dashboards within notebooks, allowing for real-time data exploration and analysis.'''\n",
    "\n",
    "\n",
    "\n",
    "dbutils.widgets.text(\"Folder_name\",\"\",\"\")\n",
    "dbutils.widgets.text(\"File_name\",\"\",\"\")\n",
    "dbutils.widgets.dropdown(\"Drop_down\",'1',[str(x) for x in range(1,10)])\n",
    "dbutils.widgets.combobox(\"combo_box\",'1',[str(x) for x in range(1,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0caf37a3-81d6-4cee-9e0e-0a9fe7027178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################       Explode Function       ##########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770f26ab-e39e-42a1-830e-3bd9512c800f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- Appliances: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Appliances</th></tr></thead><tbody><tr><td>Raja</td><td>List(TV, Refrigarator, Oven, AC)</td></tr><tr><td>Raghav</td><td>List(AC, Washing machine, null)</td></tr><tr><td>Ram</td><td>List(Grinder, TV)</td></tr><tr><td>Ramesh</td><td>List(Refrigarator, TV, null)</td></tr><tr><td>Rajesh</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         [
          "TV",
          "Refrigarator",
          "Oven",
          "AC"
         ]
        ],
        [
         "Raghav",
         [
          "AC",
          "Washing machine",
          null
         ]
        ],
        [
         "Ram",
         [
          "Grinder",
          "TV"
         ]
        ],
        [
         "Ramesh",
         [
          "Refrigarator",
          "TV",
          null
         ]
        ],
        [
         "Rajesh",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Appliances",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################  Create a dataframe with array column ##########\n",
    "array_appliance = [\n",
    "    ('Raja',['TV','Refrigarator','Oven','AC']),\n",
    "    ('Raghav',['AC','Washing machine',None]),\n",
    "    ('Ram',['Grinder','TV']),\n",
    "    ('Ramesh',['Refrigarator','TV',None]),\n",
    "    ('Rajesh',None)\n",
    "]\n",
    "\n",
    "df_app = spark.createDataFrame(data=array_appliance ,schema=['name','Appliances'])\n",
    "df_app.printSchema()\n",
    "display(df_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0524b4-b9e2-413e-b328-b79ae7948fd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Brand</th></tr></thead><tbody><tr><td>Raja</td><td>Map(AC -> Voltas, TV -> LG, oven -> philipps, Regrigarator -> samsung)</td></tr><tr><td>Raghav</td><td>Map(AC -> samsung, Washing machine -> LG)</td></tr><tr><td>Ram</td><td>Map(TV -> , Grinder -> Preethi)</td></tr><tr><td>Rajesh</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         {
          "AC": "Voltas",
          "Regrigarator": "samsung",
          "TV": "LG",
          "oven": "philipps"
         }
        ],
        [
         "Raghav",
         {
          "AC": "samsung",
          "Washing machine": "LG"
         }
        ],
        [
         "Ram",
         {
          "Grinder": "Preethi",
          "TV": ""
         }
        ],
        [
         "Rajesh",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Brand",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############  Create df with map column ############\n",
    "map_brand = [\n",
    "    ('Raja',{'TV':'LG','Regrigarator':'samsung','oven':'philipps','AC':'Voltas'}),\n",
    "    ('Raghav',{'AC':'samsung','Washing machine':'LG'}),\n",
    "    ('Ram',{'Grinder':'Preethi','TV':''}),\n",
    "     ('Rajesh',None)\n",
    "]\n",
    "\n",
    "df_brand = spark.createDataFrame(data=map_brand,schema=['name','Brand'])\n",
    "display(df_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ecf6564-f0c1-444c-b479-904a52078793",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>col</th></tr></thead><tbody><tr><td>Raja</td><td>TV</td></tr><tr><td>Raja</td><td>Refrigarator</td></tr><tr><td>Raja</td><td>Oven</td></tr><tr><td>Raja</td><td>AC</td></tr><tr><td>Raghav</td><td>AC</td></tr><tr><td>Raghav</td><td>Washing machine</td></tr><tr><td>Raghav</td><td>null</td></tr><tr><td>Ram</td><td>Grinder</td></tr><tr><td>Ram</td><td>TV</td></tr><tr><td>Ramesh</td><td>Refrigarator</td></tr><tr><td>Ramesh</td><td>TV</td></tr><tr><td>Ramesh</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "TV"
        ],
        [
         "Raja",
         "Refrigarator"
        ],
        [
         "Raja",
         "Oven"
        ],
        [
         "Raja",
         "AC"
        ],
        [
         "Raghav",
         "AC"
        ],
        [
         "Raghav",
         "Washing machine"
        ],
        [
         "Raghav",
         null
        ],
        [
         "Ram",
         "Grinder"
        ],
        [
         "Ram",
         "TV"
        ],
        [
         "Ramesh",
         "Refrigarator"
        ],
        [
         "Ramesh",
         "TV"
        ],
        [
         "Ramesh",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############### Explode Array Field ########\n",
    "\n",
    "from pyspark.sql.functions import explode \n",
    "df2 = df_app.select(df_app.name,explode(df_app.Appliances))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67180b2d-66d4-4e8c-8347-a8c2509c5ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>key</th><th>value</th></tr></thead><tbody><tr><td>Raja</td><td>AC</td><td>Voltas</td></tr><tr><td>Raja</td><td>TV</td><td>LG</td></tr><tr><td>Raja</td><td>oven</td><td>philipps</td></tr><tr><td>Raja</td><td>Regrigarator</td><td>samsung</td></tr><tr><td>Raghav</td><td>AC</td><td>samsung</td></tr><tr><td>Raghav</td><td>Washing machine</td><td>LG</td></tr><tr><td>Ram</td><td>TV</td><td></td></tr><tr><td>Ram</td><td>Grinder</td><td>Preethi</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "AC",
         "Voltas"
        ],
        [
         "Raja",
         "TV",
         "LG"
        ],
        [
         "Raja",
         "oven",
         "philipps"
        ],
        [
         "Raja",
         "Regrigarator",
         "samsung"
        ],
        [
         "Raghav",
         "AC",
         "samsung"
        ],
        [
         "Raghav",
         "Washing machine",
         "LG"
        ],
        [
         "Ram",
         "TV",
         ""
        ],
        [
         "Ram",
         "Grinder",
         "Preethi"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################### Explode Map function  ############\n",
    "df3 = df_brand.select(df_brand.name,explode(df_brand.Brand))\n",
    "display(df3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234aff2c-f95c-4b8e-ab91-4f69f4cc0232",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>col</th></tr></thead><tbody><tr><td>Raja</td><td>TV</td></tr><tr><td>Raja</td><td>Refrigarator</td></tr><tr><td>Raja</td><td>Oven</td></tr><tr><td>Raja</td><td>AC</td></tr><tr><td>Raghav</td><td>AC</td></tr><tr><td>Raghav</td><td>Washing machine</td></tr><tr><td>Raghav</td><td>null</td></tr><tr><td>Ram</td><td>Grinder</td></tr><tr><td>Ram</td><td>TV</td></tr><tr><td>Ramesh</td><td>Refrigarator</td></tr><tr><td>Ramesh</td><td>TV</td></tr><tr><td>Ramesh</td><td>null</td></tr><tr><td>Rajesh</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "TV"
        ],
        [
         "Raja",
         "Refrigarator"
        ],
        [
         "Raja",
         "Oven"
        ],
        [
         "Raja",
         "AC"
        ],
        [
         "Raghav",
         "AC"
        ],
        [
         "Raghav",
         "Washing machine"
        ],
        [
         "Raghav",
         null
        ],
        [
         "Ram",
         "Grinder"
        ],
        [
         "Ram",
         "TV"
        ],
        [
         "Ramesh",
         "Refrigarator"
        ],
        [
         "Ramesh",
         "TV"
        ],
        [
         "Ramesh",
         null
        ],
        [
         "Rajesh",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "col",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>key</th><th>value</th></tr></thead><tbody><tr><td>Raja</td><td>AC</td><td>Voltas</td></tr><tr><td>Raja</td><td>TV</td><td>LG</td></tr><tr><td>Raja</td><td>oven</td><td>philipps</td></tr><tr><td>Raja</td><td>Regrigarator</td><td>samsung</td></tr><tr><td>Raghav</td><td>AC</td><td>samsung</td></tr><tr><td>Raghav</td><td>Washing machine</td><td>LG</td></tr><tr><td>Ram</td><td>TV</td><td></td></tr><tr><td>Ram</td><td>Grinder</td><td>Preethi</td></tr><tr><td>Rajesh</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "AC",
         "Voltas"
        ],
        [
         "Raja",
         "TV",
         "LG"
        ],
        [
         "Raja",
         "oven",
         "philipps"
        ],
        [
         "Raja",
         "Regrigarator",
         "samsung"
        ],
        [
         "Raghav",
         "AC",
         "samsung"
        ],
        [
         "Raghav",
         "Washing machine",
         "LG"
        ],
        [
         "Ram",
         "TV",
         ""
        ],
        [
         "Ram",
         "Grinder",
         "Preethi"
        ],
        [
         "Rajesh",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################  Explode Outer to consider NULL valeus ###########\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "display(df_app.select(df_app.name,explode_outer(df_app.Appliances)))\n",
    "display(df_brand.select(df_brand.name,explode_outer(df_brand.Brand)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c30d95-0d0a-4c8e-9c59-8e94347ad621",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>pos</th><th>col</th></tr></thead><tbody><tr><td>Raja</td><td>0</td><td>TV</td></tr><tr><td>Raja</td><td>1</td><td>Refrigarator</td></tr><tr><td>Raja</td><td>2</td><td>Oven</td></tr><tr><td>Raja</td><td>3</td><td>AC</td></tr><tr><td>Raghav</td><td>0</td><td>AC</td></tr><tr><td>Raghav</td><td>1</td><td>Washing machine</td></tr><tr><td>Raghav</td><td>2</td><td>null</td></tr><tr><td>Ram</td><td>0</td><td>Grinder</td></tr><tr><td>Ram</td><td>1</td><td>TV</td></tr><tr><td>Ramesh</td><td>0</td><td>Refrigarator</td></tr><tr><td>Ramesh</td><td>1</td><td>TV</td></tr><tr><td>Ramesh</td><td>2</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         0,
         "TV"
        ],
        [
         "Raja",
         1,
         "Refrigarator"
        ],
        [
         "Raja",
         2,
         "Oven"
        ],
        [
         "Raja",
         3,
         "AC"
        ],
        [
         "Raghav",
         0,
         "AC"
        ],
        [
         "Raghav",
         1,
         "Washing machine"
        ],
        [
         "Raghav",
         2,
         null
        ],
        [
         "Ram",
         0,
         "Grinder"
        ],
        [
         "Ram",
         1,
         "TV"
        ],
        [
         "Ramesh",
         0,
         "Refrigarator"
        ],
        [
         "Ramesh",
         1,
         "TV"
        ],
        [
         "Ramesh",
         2,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pos",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "col",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############  Positional Explode  ( add another column to indicate the position )  ###########  \n",
    "from pyspark.sql.functions import posexplode\n",
    "\n",
    "display(df_app.select(df_app.name,posexplode(df_app.Appliances)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df284274-b5ee-4fb1-b468-de6518a7161d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>pos</th><th>col</th></tr></thead><tbody><tr><td>Raja</td><td>0</td><td>TV</td></tr><tr><td>Raja</td><td>1</td><td>Refrigarator</td></tr><tr><td>Raja</td><td>2</td><td>Oven</td></tr><tr><td>Raja</td><td>3</td><td>AC</td></tr><tr><td>Raghav</td><td>0</td><td>AC</td></tr><tr><td>Raghav</td><td>1</td><td>Washing machine</td></tr><tr><td>Raghav</td><td>2</td><td>null</td></tr><tr><td>Ram</td><td>0</td><td>Grinder</td></tr><tr><td>Ram</td><td>1</td><td>TV</td></tr><tr><td>Ramesh</td><td>0</td><td>Refrigarator</td></tr><tr><td>Ramesh</td><td>1</td><td>TV</td></tr><tr><td>Ramesh</td><td>2</td><td>null</td></tr><tr><td>Rajesh</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         0,
         "TV"
        ],
        [
         "Raja",
         1,
         "Refrigarator"
        ],
        [
         "Raja",
         2,
         "Oven"
        ],
        [
         "Raja",
         3,
         "AC"
        ],
        [
         "Raghav",
         0,
         "AC"
        ],
        [
         "Raghav",
         1,
         "Washing machine"
        ],
        [
         "Raghav",
         2,
         null
        ],
        [
         "Ram",
         0,
         "Grinder"
        ],
        [
         "Ram",
         1,
         "TV"
        ],
        [
         "Ramesh",
         0,
         "Refrigarator"
        ],
        [
         "Ramesh",
         1,
         "TV"
        ],
        [
         "Ramesh",
         2,
         null
        ],
        [
         "Rajesh",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pos",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "col",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######### Positional explode with NULL #####\n",
    "from pyspark.sql.functions import posexplode_outer \n",
    "display(df_app.select(df_app.name,posexplode_outer(df_app.Appliances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88d06ee6-5e18-445f-af17-d04b1c164122",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dc97779-f899-46cc-ad48-8e8957096e69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################       Case Function (When , otherwise , case )     ####################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3cf55a6-09d6-4a32-8862-00ee052ff551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th></tr></thead><tbody><tr><td>Raja</td><td>scinece</td><td>80</td><td>p</td><td>90</td></tr><tr><td>Rakesh</td><td>Maths</td><td>90</td><td>p</td><td>70</td></tr><tr><td>Rama</td><td>English</td><td>20</td><td>F</td><td>80</td></tr><tr><td>Ramesh</td><td>Science</td><td>45</td><td>F</td><td>75</td></tr><tr><td>Rajesh</td><td>Maths</td><td>30</td><td>F</td><td>50</td></tr><tr><td>Raghav</td><td>Maths</td><td>null</td><td>NA</td><td>70</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "scinece",
         80,
         "p",
         90
        ],
        [
         "Rakesh",
         "Maths",
         90,
         "p",
         70
        ],
        [
         "Rama",
         "English",
         20,
         "F",
         80
        ],
        [
         "Ramesh",
         "Science",
         45,
         "F",
         75
        ],
        [
         "Rajesh",
         "Maths",
         30,
         "F",
         50
        ],
        [
         "Raghav",
         "Maths",
         null,
         "NA",
         70
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_student = [\n",
    "    ('Raja',\"scinece\",80,\"p\",90),\n",
    "    ('Rakesh','Maths',90,'p',70),\n",
    "    ('Rama','English',20,'F',80),\n",
    "    ('Ramesh','Science',45,'F',75),\n",
    "    ('Rajesh','Maths',30,'F',50),\n",
    "    ('Raghav','Maths',None,'NA',70)\n",
    "]\n",
    "\n",
    "schema = ['name','Subject','Mark','Status','Attendance']\n",
    "df = spark.createDataFrame(data=data_student,schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95cf6476-c9f6-47e9-b66f-a96402d5ce7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>status</th><th>Attendance</th></tr></thead><tbody><tr><td>Raja</td><td>scinece</td><td>80</td><td>pass</td><td>90</td></tr><tr><td>Rakesh</td><td>Maths</td><td>90</td><td>pass</td><td>70</td></tr><tr><td>Rama</td><td>English</td><td>20</td><td>Fail</td><td>80</td></tr><tr><td>Ramesh</td><td>Science</td><td>45</td><td>Fail</td><td>75</td></tr><tr><td>Rajesh</td><td>Maths</td><td>30</td><td>Fail</td><td>50</td></tr><tr><td>Raghav</td><td>Maths</td><td>null</td><td>Absentee</td><td>70</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "scinece",
         80,
         "pass",
         90
        ],
        [
         "Rakesh",
         "Maths",
         90,
         "pass",
         70
        ],
        [
         "Rama",
         "English",
         20,
         "Fail",
         80
        ],
        [
         "Ramesh",
         "Science",
         45,
         "Fail",
         75
        ],
        [
         "Rajesh",
         "Maths",
         30,
         "Fail",
         50
        ],
        [
         "Raghav",
         "Maths",
         null,
         "Absentee",
         70
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########## Update the existing columns ######\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df1 = df.withColumn('status',when(df.Mark>=50,'pass').when(df.Mark <50,'Fail').otherwise(\"Absentee\"))\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0f9365-7eac-4c0d-8a06-83d03fada40b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th><th>New_status</th></tr></thead><tbody><tr><td>Raja</td><td>scinece</td><td>80</td><td>p</td><td>90</td><td>pass</td></tr><tr><td>Rakesh</td><td>Maths</td><td>90</td><td>p</td><td>70</td><td>pass</td></tr><tr><td>Rama</td><td>English</td><td>20</td><td>F</td><td>80</td><td>Fail</td></tr><tr><td>Ramesh</td><td>Science</td><td>45</td><td>F</td><td>75</td><td>Fail</td></tr><tr><td>Rajesh</td><td>Maths</td><td>30</td><td>F</td><td>50</td><td>Fail</td></tr><tr><td>Raghav</td><td>Maths</td><td>null</td><td>NA</td><td>70</td><td>Absentee</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "scinece",
         80,
         "p",
         90,
         "pass"
        ],
        [
         "Rakesh",
         "Maths",
         90,
         "p",
         70,
         "pass"
        ],
        [
         "Rama",
         "English",
         20,
         "F",
         80,
         "Fail"
        ],
        [
         "Ramesh",
         "Science",
         45,
         "F",
         75,
         "Fail"
        ],
        [
         "Rajesh",
         "Maths",
         30,
         "F",
         50,
         "Fail"
        ],
        [
         "Raghav",
         "Maths",
         null,
         "NA",
         70,
         "Absentee"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "New_status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################  Create a new column ###############\n",
    "from pyspark.sql.functions import when\n",
    "df2 = df.withColumn(\"New_status\",when(df.Mark>=50,'pass').when(df.Mark<50,'Fail').otherwise('Absentee'))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d923387-366c-4a3e-a0df-7696f4bfbd57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th><th>new_status</th></tr></thead><tbody><tr><td>Raja</td><td>scinece</td><td>80</td><td>p</td><td>90</td><td>Pass</td></tr><tr><td>Rakesh</td><td>Maths</td><td>90</td><td>p</td><td>70</td><td>Pass</td></tr><tr><td>Rama</td><td>English</td><td>20</td><td>F</td><td>80</td><td>Fail</td></tr><tr><td>Ramesh</td><td>Science</td><td>45</td><td>F</td><td>75</td><td>Fail</td></tr><tr><td>Rajesh</td><td>Maths</td><td>30</td><td>F</td><td>50</td><td>Fail</td></tr><tr><td>Raghav</td><td>Maths</td><td>null</td><td>NA</td><td>70</td><td>Absentee</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "scinece",
         80,
         "p",
         90,
         "Pass"
        ],
        [
         "Rakesh",
         "Maths",
         90,
         "p",
         70,
         "Pass"
        ],
        [
         "Rama",
         "English",
         20,
         "F",
         80,
         "Fail"
        ],
        [
         "Ramesh",
         "Science",
         45,
         "F",
         75,
         "Fail"
        ],
        [
         "Rajesh",
         "Maths",
         30,
         "F",
         50,
         "Fail"
        ],
        [
         "Raghav",
         "Maths",
         null,
         "NA",
         70,
         "Absentee"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "new_status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################  Anthoer way to do it #############\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df3 = df.withColumn('new_status',expr(\"CASE WHEN Mark>= 50 THEN 'Pass' \"+ \"WHEN Mark < 50 THEN 'Fail' \" + \"ELSE 'Absentee' END \"))\n",
    "display(df3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6bbd74-1568-47a8-bb7a-fcea35b6eb6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th><th>Grade</th></tr></thead><tbody><tr><td>Raja</td><td>scinece</td><td>80</td><td>p</td><td>90</td><td>Distinction</td></tr><tr><td>Rakesh</td><td>Maths</td><td>90</td><td>p</td><td>70</td><td>Good</td></tr><tr><td>Rama</td><td>English</td><td>20</td><td>F</td><td>80</td><td>Average</td></tr><tr><td>Ramesh</td><td>Science</td><td>45</td><td>F</td><td>75</td><td>Average</td></tr><tr><td>Rajesh</td><td>Maths</td><td>30</td><td>F</td><td>50</td><td>Average</td></tr><tr><td>Raghav</td><td>Maths</td><td>null</td><td>NA</td><td>70</td><td>Average</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Raja",
         "scinece",
         80,
         "p",
         90,
         "Distinction"
        ],
        [
         "Rakesh",
         "Maths",
         90,
         "p",
         70,
         "Good"
        ],
        [
         "Rama",
         "English",
         20,
         "F",
         80,
         "Average"
        ],
        [
         "Ramesh",
         "Science",
         45,
         "F",
         75,
         "Average"
        ],
        [
         "Rajesh",
         "Maths",
         30,
         "F",
         50,
         "Average"
        ],
        [
         "Raghav",
         "Maths",
         null,
         "NA",
         70,
         "Average"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Grade",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################## Multi conditions using AND and OR operators ###################\n",
    "from pyspark.sql.functions import when\n",
    "df4 = df.withColumn(\"Grade\",when((df.Mark>=50) & (df.Attendance >= 80),\"Distinction\").when((df.Mark>=50) & (df.Attendance>=50),\"Good\").otherwise(\"Average\"))\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca611a4-a96d-499c-8347-f70d84abc3d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9577786-91ad-4cce-ba19-66fed09b9b52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################     Union & UnionAll       ############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f5e3af-4d3e-45dc-8b64-8c487c9af87c",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "print(spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f207c3-be61-4a6e-b99d-d8464d9448b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>200</td><td>philipp</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>300</td><td>jhon</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>100</td><td>Stephan</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         200,
         "philipp",
         "2002",
         "200",
         "M",
         8000
        ],
        [
         300,
         "jhon",
         "2010",
         "100",
         "",
         6000
        ],
        [
         100,
         "Stephan",
         "1999",
         "100",
         "M",
         2000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data = [\n",
    "    (200,'philipp','2002','200','M',8000),\n",
    "    (300,'jhon','2010','100','',6000),\n",
    "    (100,'Stephan','1999','100','M',2000)\n",
    "]\n",
    "\n",
    "employee_schema = ['employee_id','name','doj','employee_dept_id','gender','salary']\n",
    "DF1 = spark.createDataFrame(data=employee_data,schema = employee_schema)\n",
    "display(DF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1889abc-7d7b-4d1d-b248-c0aa829813bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>300</td><td>Jhon</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>400</td><td>Mancy</td><td>2008</td><td>400</td><td></td><td>1000</td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         300,
         "Jhon",
         "2010",
         "100",
         "",
         6000
        ],
        [
         400,
         "Mancy",
         "2008",
         "400",
         "",
         1000
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data = [\n",
    "    (300,'Jhon','2010','100','',6000),\n",
    "    (400,'Mancy','2008','400','',1000),\n",
    "    (500,'Rosy','2014','500','M',5000)\n",
    "]\n",
    "\n",
    "employee_schema = ['employee_id','name','doj','employee_dept_id','gender','salary']\n",
    "DF2 = spark.createDataFrame(data=employee_data,schema = employee_schema)\n",
    "display(DF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63cf6f1-b734-46e5-8268-3507ffd700f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>200</td><td>philipp</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>300</td><td>jhon</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>100</td><td>Stephan</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>300</td><td>Jhon</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>400</td><td>Mancy</td><td>2008</td><td>400</td><td></td><td>1000</td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         200,
         "philipp",
         "2002",
         "200",
         "M",
         8000
        ],
        [
         300,
         "jhon",
         "2010",
         "100",
         "",
         6000
        ],
        [
         100,
         "Stephan",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         300,
         "Jhon",
         "2010",
         "100",
         "",
         6000
        ],
        [
         400,
         "Mancy",
         "2008",
         "400",
         "",
         1000
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### Union ########### \n",
    "df_union = DF1.union(DF2)\n",
    "display(df_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a7264e-e11d-4977-86d3-83c7143a6022",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>200</td><td>philipp</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>300</td><td>jhon</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>100</td><td>Stephan</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>300</td><td>Jhon</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>400</td><td>Mancy</td><td>2008</td><td>400</td><td></td><td>1000</td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         200,
         "philipp",
         "2002",
         "200",
         "M",
         8000
        ],
        [
         300,
         "jhon",
         "2010",
         "100",
         "",
         6000
        ],
        [
         100,
         "Stephan",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         300,
         "Jhon",
         "2010",
         "100",
         "",
         6000
        ],
        [
         400,
         "Mancy",
         "2008",
         "400",
         "",
         1000
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### drop duplicates ######\n",
    "display(df_union.dropDuplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d1ae0a-4657-4495-a918-9c91f0e108b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#############  Union ALL ###############\n",
    "df_union_all = DF1.unionAll(DF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2582e24b-805a-4ecd-b45e-04ff0ae17713",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th></tr></thead><tbody><tr><td>300</td><td>Jhon</td><td>2010</td><td>100</td><td></td></tr><tr><td>400</td><td>Mancy</td><td>2008</td><td>400</td><td></td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         300,
         "Jhon",
         "2010",
         "100",
         ""
        ],
        [
         400,
         "Mancy",
         "2008",
         "400",
         ""
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################  Schema Mismatch ########### \n",
    "DF3 = DF2.select(DF2.employee_id,DF2.name,DF2.doj,DF2.employee_dept_id,DF2.gender)\n",
    "display(DF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d191a0-30af-4cb9-8cdf-0a25c6e31928",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dedcf0d5-17da-4096-9e1c-66558d0594c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################     Pivot and Unpivot       ############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a863cd-f0eb-45fd-ae3d-f3861084ba35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Company</th><th>Quarter</th><th>Revenue</th></tr></thead><tbody><tr><td>ABC</td><td>Q1</td><td>2000</td></tr><tr><td>ABC</td><td>Q2</td><td>3000</td></tr><tr><td>ABC</td><td>Q3</td><td>6000</td></tr><tr><td>ABC</td><td>Q4</td><td>1000</td></tr><tr><td>XYZ</td><td>Q1</td><td>5000</td></tr><tr><td>XYZ</td><td>Q2</td><td>4000</td></tr><tr><td>XYZ</td><td>Q3</td><td>1000</td></tr><tr><td>XYZ</td><td>Q4</td><td>2000</td></tr><tr><td>KLM</td><td>Q1</td><td>2000</td></tr><tr><td>KLM</td><td>Q2</td><td>3000</td></tr><tr><td>KLM</td><td>Q3</td><td>1000</td></tr><tr><td>KLM</td><td>Q4</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ABC",
         "Q1",
         2000
        ],
        [
         "ABC",
         "Q2",
         3000
        ],
        [
         "ABC",
         "Q3",
         6000
        ],
        [
         "ABC",
         "Q4",
         1000
        ],
        [
         "XYZ",
         "Q1",
         5000
        ],
        [
         "XYZ",
         "Q2",
         4000
        ],
        [
         "XYZ",
         "Q3",
         1000
        ],
        [
         "XYZ",
         "Q4",
         2000
        ],
        [
         "KLM",
         "Q1",
         2000
        ],
        [
         "KLM",
         "Q2",
         3000
        ],
        [
         "KLM",
         "Q3",
         1000
        ],
        [
         "KLM",
         "Q4",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Company",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quarter",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Revenue",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    ('ABC',\"Q1\",2000),\n",
    "    ('ABC',\"Q2\",3000),\n",
    "    ('ABC',\"Q3\",6000),\n",
    "    ('ABC',\"Q4\",1000),\n",
    "    ('XYZ',\"Q1\",5000),\n",
    "    ('XYZ',\"Q2\",4000),\n",
    "    ('XYZ',\"Q3\",1000),\n",
    "    ('XYZ',\"Q4\",2000),\n",
    "    ('KLM',\"Q1\",2000),\n",
    "    ('KLM',\"Q2\",3000),\n",
    "    ('KLM',\"Q3\",1000),\n",
    "    ('KLM',\"Q4\",5000),\n",
    "\n",
    "]\n",
    "\n",
    "column = ['Company','Quarter','Revenue']\n",
    "DF = spark.createDataFrame(data=data , schema=column)\n",
    "display(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a627a1-8101-4ad2-a9bd-4373eed45964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Company</th><th>Q1</th><th>Q2</th><th>Q3</th><th>Q4</th></tr></thead><tbody><tr><td>KLM</td><td>2000</td><td>3000</td><td>1000</td><td>5000</td></tr><tr><td>XYZ</td><td>5000</td><td>4000</td><td>1000</td><td>2000</td></tr><tr><td>ABC</td><td>2000</td><td>3000</td><td>6000</td><td>1000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "KLM",
         2000,
         3000,
         1000,
         5000
        ],
        [
         "XYZ",
         5000,
         4000,
         1000,
         2000
        ],
        [
         "ABC",
         2000,
         3000,
         6000,
         1000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Company",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Q1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Q2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Q3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Q4",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Company</th><th>Quarter</th><th>Revenue</th></tr></thead><tbody><tr><td>ABC</td><td>Q1</td><td>Q1</td></tr><tr><td>ABC</td><td>Q2</td><td>Q2</td></tr><tr><td>ABC</td><td>Q3</td><td>Q3</td></tr><tr><td>ABC</td><td>Q4</td><td>Q4</td></tr><tr><td>XYZ</td><td>Q1</td><td>Q1</td></tr><tr><td>XYZ</td><td>Q2</td><td>Q2</td></tr><tr><td>XYZ</td><td>Q3</td><td>Q3</td></tr><tr><td>XYZ</td><td>Q4</td><td>Q4</td></tr><tr><td>KLM</td><td>Q1</td><td>Q1</td></tr><tr><td>KLM</td><td>Q2</td><td>Q2</td></tr><tr><td>KLM</td><td>Q3</td><td>Q3</td></tr><tr><td>KLM</td><td>Q4</td><td>Q4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ABC",
         "Q1",
         "Q1"
        ],
        [
         "ABC",
         "Q2",
         "Q2"
        ],
        [
         "ABC",
         "Q3",
         "Q3"
        ],
        [
         "ABC",
         "Q4",
         "Q4"
        ],
        [
         "XYZ",
         "Q1",
         "Q1"
        ],
        [
         "XYZ",
         "Q2",
         "Q2"
        ],
        [
         "XYZ",
         "Q3",
         "Q3"
        ],
        [
         "XYZ",
         "Q4",
         "Q4"
        ],
        [
         "KLM",
         "Q1",
         "Q1"
        ],
        [
         "KLM",
         "Q2",
         "Q2"
        ],
        [
         "KLM",
         "Q3",
         "Q3"
        ],
        [
         "KLM",
         "Q4",
         "Q4"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Company",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quarter",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Revenue",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### Pivot a dataframe #########\n",
    "Pivot_DF = DF.groupBy('Company').pivot('Quarter').sum('Revenue')\n",
    "display(Pivot_DF)\n",
    "\n",
    "\n",
    "########### Unpivot a dataframe #########\n",
    "DF = Pivot_DF.selectExpr(\"Company\",\"stack(4,'Q1','Q1','Q2','Q2','Q3','Q3','Q4','Q4') as (Quarter,Revenue)\")\n",
    "display(DF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ce450fe-90da-4d17-b986-b7befd53fe31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee16bc2-842e-4afd-ab6c-54552ffeb016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33248bf8-d877-497f-b0f4-6294e28a64d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################     Working with json      ############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1a7583-df6c-46e9-acf0-aaa771263f5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.option('multiline','true').json('file_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e001060-20b6-41ce-8f40-0859e98750d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_17_154750.jpg)\n",
    "<br>\n",
    "![test image](files/tables/Screenshot_2024_07_17_154902.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db03d9bc-3d4f-4cad-a6c9-1313af894259",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d31190ee-c4ec-4f29-8e28-98f391fcddd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "663a5b7c-3beb-4628-8f8f-4ed12d088c3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########################     Bad Record Handling        ########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edc73a1c-557d-490d-af0d-5a06e8687e96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_17_155307.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_155326.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cadfd2e5-39ca-4654-a685-f64002e627d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########## Suppose following is our csv file ########## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9ec4838-a785-4d51-ab34-befcdc1d5ce5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_17_155841.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf812004-80ea-4bbb-95e7-7b66c7e9fd46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>Emp_count</th><th>Production_unit</th><th>Expenses</th></tr></thead><tbody><tr><td>JAN</td><td>340</td><td>2000</td><td>30</td></tr><tr><td>FEB</td><td>318</td><td>2500</td><td>29</td></tr><tr><td>MAR</td><td>362</td><td>1500</td><td>32</td></tr><tr><td>APR</td><td>348</td><td>3000</td><td>26</td></tr><tr><td>MAY</td><td>363</td><td>2200</td><td>35</td></tr><tr><td>JUN</td><td>435</td><td>3300</td><td>27</td></tr><tr><td>JUL</td><td>491</td><td>1600</td><td>23</td></tr><tr><td>AUG</td><td>505</td><td>Thousand</td><td>33</td></tr><tr><td>SEP</td><td>404</td><td>3500</td><td>36</td></tr><tr><td>OCT</td><td>359</td><td>2900</td><td>28</td></tr><tr><td>NOV</td><td>310</td><td>4000</td><td>25</td></tr><tr><td>DEC</td><td>337</td><td>3400</td><td>31</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "JAN",
         340,
         "2000",
         30
        ],
        [
         "FEB",
         318,
         "2500",
         29
        ],
        [
         "MAR",
         362,
         "1500",
         32
        ],
        [
         "APR",
         348,
         "3000",
         26
        ],
        [
         "MAY",
         363,
         "2200",
         35
        ],
        [
         "JUN",
         435,
         "3300",
         27
        ],
        [
         "JUL",
         491,
         "1600",
         23
        ],
        [
         "AUG",
         505,
         "Thousand",
         33
        ],
        [
         "SEP",
         404,
         "3500",
         36
        ],
        [
         "OCT",
         359,
         "2900",
         28
        ],
        [
         "NOV",
         310,
         "4000",
         25
        ],
        [
         "DEC",
         337,
         "3400",
         31
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Emp_count",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Production_unit",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Expenses",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header','true').option('inferschema','true').load(\"/FileStore/tables/corrupt.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54f2373-7720-467d-b1cb-b6d52d65d8f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>Emp_count</th><th>Production_unit</th><th>Expense</th><th>_corrupt_record</th></tr></thead><tbody><tr><td>JAN</td><td>340</td><td>2000</td><td>30</td><td>JAN,340,2000,30,</td></tr><tr><td>FEB</td><td>318</td><td>2500</td><td>29</td><td>FEB,318,2500,29,</td></tr><tr><td>MAR</td><td>362</td><td>1500</td><td>32</td><td>MAR,362,1500,32,</td></tr><tr><td>APR</td><td>348</td><td>3000</td><td>26</td><td>APR,348,3000,26,</td></tr><tr><td>MAY</td><td>363</td><td>2200</td><td>35</td><td>MAY,363,2200,35,test_msg,</td></tr><tr><td>JUN</td><td>435</td><td>3300</td><td>27</td><td>JUN,435,3300,27,</td></tr><tr><td>JUL</td><td>491</td><td>1600</td><td>23</td><td>JUL,491,1600,23,</td></tr><tr><td>AUG</td><td>505</td><td>Thousand</td><td>33</td><td>AUG,505,Thousand,33,</td></tr><tr><td>SEP</td><td>404</td><td>3500</td><td>36</td><td>SEP,404,3500,36,</td></tr><tr><td>OCT</td><td>359</td><td>2900</td><td>28</td><td>OCT,359,2900,28,</td></tr><tr><td>NOV</td><td>310</td><td>4000</td><td>25</td><td>null</td></tr><tr><td>DEC</td><td>337</td><td>3400</td><td>31</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "JAN",
         "340",
         "2000",
         "30",
         "JAN,340,2000,30,"
        ],
        [
         "FEB",
         "318",
         "2500",
         "29",
         "FEB,318,2500,29,"
        ],
        [
         "MAR",
         "362",
         "1500",
         "32",
         "MAR,362,1500,32,"
        ],
        [
         "APR",
         "348",
         "3000",
         "26",
         "APR,348,3000,26,"
        ],
        [
         "MAY",
         "363",
         "2200",
         "35",
         "MAY,363,2200,35,test_msg,"
        ],
        [
         "JUN",
         "435",
         "3300",
         "27",
         "JUN,435,3300,27,"
        ],
        [
         "JUL",
         "491",
         "1600",
         "23",
         "JUL,491,1600,23,"
        ],
        [
         "AUG",
         "505",
         "Thousand",
         "33",
         "AUG,505,Thousand,33,"
        ],
        [
         "SEP",
         "404",
         "3500",
         "36",
         "SEP,404,3500,36,"
        ],
        [
         "OCT",
         "359",
         "2900",
         "28",
         "OCT,359,2900,28,"
        ],
        [
         "NOV",
         "310",
         "4000",
         "25",
         null
        ],
        [
         "DEC",
         "337",
         "3400",
         "31",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Emp_count",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Production_unit",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Expense",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_corrupt_record",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## define the schema \n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('Month',StringType(),True),\n",
    "    StructField('Emp_count',StringType(),True),\n",
    "    StructField('Production_unit',StringType(),True),\n",
    "    StructField('Expense',StringType(),True),\n",
    "    StructField('_corrupt_record',StringType(),True),\n",
    "])\n",
    "\n",
    "df = spark.read.format('csv').option('mode','PERMISIVE').option('header','true').schema(schema).load(\"/FileStore/tables/corrupt.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3153a703-8cb1-4a3a-89a6-db05b21b42b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>Emp_count</th><th>Production_unit</th><th>Expense</th><th>_corrupt_record</th></tr></thead><tbody><tr><td>NOV</td><td>310</td><td>4000</td><td>25</td><td>null</td></tr><tr><td>DEC</td><td>337</td><td>3400</td><td>31</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "NOV",
         "310",
         "4000",
         "25",
         null
        ],
        [
         "DEC",
         "337",
         "3400",
         "31",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Emp_count",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Production_unit",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Expense",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_corrupt_record",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Drop mal form mode\n",
    "df = spark.read.format('csv').option('mode','DROPMALFORMED').option('header','true').schema(schema).load(\"/FileStore/tables/corrupt.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49330657-56aa-48b9-a4ab-3b3b844d3a6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 208.0 failed 1 times, most recent failure: Lost task 0.0 in stage 208.0 (TID 544) (ip-10-172-186-147.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/corrupt.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:699)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:668)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:791)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:614)\n",
       "\t... 29 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\n",
       "Caused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1703)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:346)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:614)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:791)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3424)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3346)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3335)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3335)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1444)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1444)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1444)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3635)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3573)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3561)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1193)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1181)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3449)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3448)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:723)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:887)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:286)\n",
       "\tat sun.reflect.GeneratedMethodAccessor759.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/corrupt.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:699)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:668)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:791)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:614)\n",
       "\t... 29 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\n",
       "Caused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1703)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:346)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:614)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:791)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 208.0 failed 1 times, most recent failure: Lost task 0.0 in stage 208.0 (TID 544) (ip-10-172-186-147.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/corrupt.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:699)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:668)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:791)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:492)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:614)\n\t... 29 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1703)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:346)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:614)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:791)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:492)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3424)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3346)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3335)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3335)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1444)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3635)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3573)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3561)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1193)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1181)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3449)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3448)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:723)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:887)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:286)\n\tat sun.reflect.GeneratedMethodAccessor759.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/corrupt.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:699)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:668)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:791)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:492)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:614)\n\t... 29 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1703)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:346)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:614)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:791)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:492)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "FileReadException: Error while reading file dbfs:/FileStore/tables/corrupt.csv.\nCaused by: SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\nCaused by: BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,\nCaused by: SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30,",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('mode','FAILFAST').option('header','true').schema(schema).load(\"/FileStore/tables/corrupt.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7618bb7-0fcb-4f2d-a532-60454862190d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e0b2f2f-1462-4bb1-a278-24f4f042969b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82fd3803-761c-474f-ab37-40469fd6ff3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################  Azure DataLake (ASDL) integration with Data bricks      #######################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e94843d-2091-446f-b689-18329bc08496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################# There are 04 methods that we can connect azure datalake to databricks\n",
    "## Method 01 : Using a service principle\n",
    "## Method 02 : Using a Azure Active Directory credentials known as a credential passthrough\n",
    "## Method 03 : Using ADLS access key directly\n",
    "## Mthod 04 : Creating Mount point using ADLS Access Key \n",
    "\n",
    "\n",
    "################ High level overview of procedure\n",
    "## Step o1 : Create Azure Data Lake Storage \n",
    "## Step 02 : Create a Mount Point\n",
    "## Step 03 : Read files from ADLS in Databricks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1bc225f-e103-4111-9c40-4b5c39b4d6fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################  Steps  ##############################\n",
    "\n",
    "## Step 01 :  Goto Azure console -> Storage accounts -> create  (stick to default 'Subscription' & default 'Resource group') , Give a name for ' Storage account name '  (I gave it as tharindu35687)  -> click on  'Next:Networkin'  and in there  put a tick on 'Enable hierarchical namespaces' -> Now create a one \n",
    "\n",
    "## step 02 : Now go to that created resource ->  In there go to containers under data storage -> now click on  \n",
    "#  '+container'  and give a name for that  (I gave it as  tharindu) , after that upload any files that you need inside that container (Here i have uploaded world_population.csv in there)\n",
    "\n",
    "## step 03 : Now go to that resource that we created -> on the left handside go Security+Networking ->Accesskeys-> show keys , now copy the key (0wf9ZiipolRjLG7v6oOFrE4qaiBMCNuxpBtrya1CjLfGymeLEi7MFDSov7XdqkjUu8tUlnyVe8ws+AStrZ9ctA==)\n",
    "\n",
    "### Now continue with the following processs \n",
    "\n",
    "'''\n",
    "spark.conf.set(\n",
    "    'fs.azure.account.key.<storage account>.dfs.core.windows.net',\n",
    "    '<access key >'\n",
    ")\n",
    "'''\n",
    "spark.conf.set(\n",
    "    'fs.azure.account.key.tharindu35687.dfs.core.windows.net',\n",
    "    '0wf9ZiipolRjLG7v6oOFrE4qaiBMCNuxpBtrya1CjLfGymeLEi7MFDSov7XdqkjUu8tUlnyVe8ws+AStrZ9ctA=='\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfc1402-af31-40a2-8844-7325caf493cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[84]: [FileInfo(path='abfss://tharindu@tharindu35687.dfs.core.windows.net/world_population.csv', name='world_population.csv', size=29247, modificationTime=1721217436000)]"
     ]
    }
   ],
   "source": [
    "########## How to get the file details that we stores under our container \n",
    "# dbutils.fs.ls(\"abfss://<container-name>@<resource-name>.dfs.core.windows.net/\")\n",
    "dbutils.fs.ls(\"abfss://tharindu@tharindu35687.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581c315d-66b4-4043-bf13-b1487c998bb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-73937790193581>:5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m file_location \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mabfss://tharindu@tharindu35687.dfs.core.windows.net/\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m## read the data into a data frame\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mload(file_location)\n",
       "\u001B[1;32m      6\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
       "\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o475.load.\n",
       ": Failure to initialize configuration for storage account tharindu35687.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:670)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2056)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:268)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:225)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:64)\n",
       "\tat com.databricks.common.filesystem.Cache.getOrCompute(Cache.scala:38)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:61)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:87)\n",
       "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
       "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n",
       "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
       "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
       "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:378)\n",
       "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:334)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:334)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: Invalid configuration value detected for fs.azure.account.key\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n",
       "\t... 30 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-73937790193581>:5\u001B[0m\n\u001B[1;32m      2\u001B[0m file_location \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mabfss://tharindu@tharindu35687.dfs.core.windows.net/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m## read the data into a data frame\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mload(file_location)\n\u001B[1;32m      6\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o475.load.\n: Failure to initialize configuration for storage account tharindu35687.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:670)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2056)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:268)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:225)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:64)\n\tat com.databricks.common.filesystem.Cache.getOrCompute(Cache.scala:38)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:61)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:87)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:378)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:334)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:334)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 30 more\n",
       "errorSummary": "Failure to initialize configuration for storage account tharindu35687.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## set the data lake file location\n",
    "file_location = 'abfss://tharindu@tharindu35687.dfs.core.windows.net/'\n",
    "\n",
    "## read the data into a data frame\n",
    "df = spark.read.format('csv').option('inferSchema','true').option('header','true').option('delimiter',',').load(file_location)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e07b6d0-2a86-4c3d-9b76-fc72f3f6be20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[88]: True"
     ]
    }
   ],
   "source": [
    "#############################   Method 02 :  Create a mount point using ADLS Access key  ##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Goto our resource -> settings -> Endpoint and copy  the primary endpoint (https://tharindu35687.blob.core.windows.net/)\n",
    "\n",
    "'''\n",
    "dbutils.fs.mount(\n",
    "    source = 'wasbs://tharindu@<endpoint>',\n",
    "    mount_point = '/mnt/tharindu_test'   # /mnt  , must be given and then we can give any name\n",
    "    extra_configs = {'fs.azure.account.key.<resourcename>.blob.core.windows.net':\"<access key>\"}\n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source = 'wasbs://tharindu@tharindu35687.blob.core.windows.net',\n",
    "    mount_point = '/mnt/tharindu_test' ,  # /mnt  , must be given and then we can give any name\n",
    "    extra_configs = {\"fs.azure.account.key.tharindu35687.blob.core.windows.net\":\"0wf9ZiipolRjLG7v6oOFrE4qaiBMCNuxpBtrya1CjLfGymeLEi7MFDSov7XdqkjUu8tUlnyVe8ws+AStrZ9ctA==\"}\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5cd34e2-34ea-43a9-bcc8-7bcd0c4e5bb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[89]: [FileInfo(path='dbfs:/mnt/tharindu_test/world_population.csv', name='world_population.csv', size=29247, modificationTime=1721217436000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/mnt/tharindu_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf82f87-f2fd-4dc1-ac6f-de90c14e3b89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[90]: [MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType='sse-s3'),\n MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType='sse-s3'),\n MountInfo(mountPoint='/mnt/tharindu_test', source='wasbs://tharindu@tharindu35687.blob.core.windows.net', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType='sse-s3'),\n MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType='sse-s3')]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0fca31b-c63c-4403-b1e5-47c3c96d67ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70050502-8fed-47e3-bc94-3aadae417291",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57407c4f-aea1-4ee5-bade-cde0b3c8dbb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################   Ingest  dat afro mazure SQL Data base      ###################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ececdeb9-cebb-48d4-aed6-4544529f549f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52915f7-2f8d-4cc7-ab95-60af7a762202",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################### Steps #########################################\n",
    "## Step 01 : Azure console -> azure databases ->  Create Sql databse ->  Give a db name (chandima35687) and create a new server and give it a name too (chandima)  , give the following credentials as well\n",
    "##         server admin login : tharicha\n",
    "##         password : !test1234\n",
    "##       Now click on next -> next -> next  , and now click on 'sample' under data source   and create it \n",
    "\n",
    "##  step 02 : Now  goto that resource that login use those credentails . Then it will give us an error as our ip is not added . So copy that IP address in the error -> Go to our resource -> selelct our sql server -> show firewall setting -> allow azure services and resources to access this server  and give name to Rule and paste that ip in both Start IP and End IP  and click on save . Now login to the db .\n",
    "\n",
    "### Now to connect with the tables inside that db in azure using databricks , continue with the following process\n",
    "\n",
    "\n",
    "jbdcHostname =  ''  # This can be found inside our mysql resource that we created in azure \n",
    "jdbcPort = 1433\n",
    "jdbDatabase = ''\n",
    "jdbcUserName = ''\n",
    "jdbcPassword =  ''\n",
    "jdbcDriver = 'com.microsoft.sqlserver.jdbc.SQLServerDrive'  #  this is a constant \n",
    "\n",
    "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\"\n",
    "\n",
    "df1 = spark.read.format('jdbc').option(\"url\",jdbcUrl).option('dbtable','').load()\n",
    "display(df1)\n",
    "\n",
    "\n",
    "\n",
    "#### inside our sql resource that we created , we can see an option called connection string . So using that we can get a connection string .  First go inside that and then select JDBC and copy the link inside that \n",
    "connectionString = \" \"\n",
    "\n",
    "df =  spark.read.jdbc(connectionString,'')\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c5f8b4-9424-4aeb-880d-6f988fa73d40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d2851bd-f261-43bc-a178-df531b06ac50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "951deab3-bca4-4297-9fbc-d7d39591495d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################  Real Time ETL Pipeline ( Loading data from azure sql to azure data lake )   ###\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45e18a25-4fde-4a1e-9059-57ac6bd271c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##############  ETL ############# \n",
    "## Extract :\n",
    "##    * Read tables from azure sql\n",
    "##    *  JDBC connector is used\n",
    "##    *  Dataframes are created\n",
    "\n",
    "##  Transform :\n",
    "##   * Business transformations are applied\n",
    "##   * Null are replaced with default with values\n",
    "##   * Duplicates are dropped\n",
    "##   * join and aggregate are performed\n",
    "\n",
    "## Load :\n",
    "##   * Mount point is created for ADLS integration\n",
    "##   * Data is loaded in parquet file format to target location \n",
    "\n",
    "\n",
    "##########  For the demo we need a container and a sql db in azure .  ########## \n",
    "###### **** Keep in mind that to attach the same resource group for both the resources \n",
    "\n",
    "####################################  Procedure - 01 ######################################################\n",
    "# ## Step 01 :  Goto Azure console -> Storage accounts -> create  (stick to default 'Subscription' & default 'Resource group') , Give a name for ' Storage account name '  (I gave it as tharindu35687)  -> click on  'Next:Networkin'  and in there  put a tick on 'Enable hierarchical namespaces' -> Now create a one \n",
    "\n",
    "## step 02 : Now go to that created resource ->  In there go to containers under data storage -> now click on  \n",
    "#  '+container'  and give a name for that  (I gave it as  tharindu) , after that upload any files that you need inside that container (Here i have uploaded world_population.csv in there)\n",
    "\n",
    "## step 03 : Now go to that resource that we created -> on the left handside go Security+Networking ->Accesskeys-> show keys , now copy the key (0wf9ZiipolRjLG7v6oOFrE4qaiBMCNuxpBtrya1CjLfGymeLEi7MFDSov7XdqkjUu8tUlnyVe8ws+AStrZ9ctA==) \n",
    "\n",
    "\n",
    "\n",
    "#######################################  Procedure - 02 ######################################################\n",
    "## Step 01 : Azure console -> azure databases ->  Create Sql databse ->  Give a db name (chandima35687) and create a new server and give it a name too (chandima)  , give the following credentials as well\n",
    "##         server admin login : tharicha\n",
    "##         password : !test1234\n",
    "##       Now click on next -> next -> next  , and now click on 'sample' under data source   and create it \n",
    "\n",
    "##  step 02 : Now  goto that resource that login use those credentails . Then it will give us an error as our ip is not added . So copy that IP address in the error -> Go to our resource -> selelct our sql server -> show firewall setting -> allow azure services and resources to access this server  and give name to Rule and paste that ip in both Start IP and End IP  and click on save . Now login to the db ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16394ee2-dee6-4b93-91f8-6bec63a48f12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "####################    Step 01 : Establish the db connection    #######################################\n",
    "###################################################################################################### \n",
    "jbdcHostname =  ''  # This can be found inside our mysql resource that we created in azure \n",
    "jdbcPort = 1433\n",
    "jdbDatabase = ''\n",
    "jdbcUserName = ''\n",
    "jdbcPassword =  ''\n",
    "jdbcDriver = 'com.microsoft.sqlserver.jdbc.SQLServerDrive'  #  this is a constant \n",
    "\n",
    "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\"\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "##################################   Step 02 : Read the  tables ############################################\n",
    "#############################################################################################################\n",
    "## Read the product table \n",
    "df_product = spark.read.format('jdbc').option(\"url\",jdbcUrl).option('dbtable','SalesLT.product').load()\n",
    "display(df_product)\n",
    "\n",
    "## Read the sales table \n",
    "df_product = spark.read.format('jdbc').option(\"url\",jdbcUrl).option('dbtable','SalesLT.SalesOrderDetails').load()\n",
    "display(df_product)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "##############  Step 03 :  Transform the data as per business rules ########################################\n",
    "################################################################################################################\n",
    "###  cleansing fact dataframe - drop duplicate records\n",
    "\n",
    "\n",
    "## cleansing fact dataframe - drop duplicate records\n",
    "df_sales_cleansed = df_sales.dropDuplicates()\n",
    "display(df_sales_cleansed)\n",
    "\n",
    "\n",
    "\n",
    "#### join fact and dimension dataframes\n",
    "df_join = df_sales_cleansed.join(df_prodcut_cleansed,df_sales_cleansed.ProductID== df_product_cleansed.ProductID, 'leftouter').select(df_sales_cleansed.ProductID, \n",
    "                    df_sales_cleansed.UnitPrice , \n",
    "                    df_sales_cleansed.LineTotal,\n",
    "                    df_product_cleansed.Name,\n",
    "                    df_product_cleansed.Color,\n",
    "                    df_product_cleansed.Size,\n",
    "                    df_product_cleansed.Weight )\n",
    "\n",
    "display(df)\n",
    "\n",
    "\n",
    "df_agg = df_join.groupby(['ProductID','Name','Color','Size','Weight']).sum('LineTotal').withColumnRenamed(\"sum(LineTotal)\",\"sum_total_sales\")\n",
    "display(df_agg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "##############  Step 04 : Load transformed data into Azure Data Lake Storage   #################################\n",
    "###############################################################################################################\n",
    "## Create Mount Point for ADLS Integration\n",
    "dbutils.fs.mount(\n",
    "    source = 'wasbs://<container-name>@<resource-name>.blob.core.windows.net',\n",
    "    mount_point = '/mnt/new_data',\n",
    "    extra_configs = {'fs.azure.account.key.<resourcename>.blob.core.windows.net':\"<access key>\"}\n",
    ")\n",
    "\n",
    "\n",
    "## List the files under mount point \n",
    "dbutils.fs.ls(\"/mnt/new_data\")\n",
    "\n",
    "## Write the delta in Parquet Format\n",
    "df_agg.write.format(\"parquet\").save(\"/mnt/new_data/adv_work_parquet/\")\n",
    "\n",
    "\n",
    "## Write the data in csv format \n",
    "df_agg.write.format(\"csv\").option('header','true').save(\"/mnt/new_data/adv_work_csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbe9f2cb-7704-4050-adaf-a5b3b13e220d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "640d90d3-26b4-4559-a1b9-080c1a325460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2d1ac6a-ad75-4782-9d05-ad253dca2409",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################       Azure Key valut integration      #########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "324e7a72-964a-4aca-a15a-5c0c9e459edb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jbdcHostname =  ''  # This can be found inside our mysql resource that we created in azure \n",
    "jdbcPort = 1433\n",
    "jdbDatabase = ''\n",
    "jdbcUserName = ''\n",
    "jdbcPassword =  ''\n",
    "jdbcDriver = 'com.microsoft.sqlserver.jdbc.SQLServerDrive'  #  this is a constant \n",
    "\n",
    "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\"\n",
    "\n",
    "##########  This is how made the connection  early on . But it is not a good practice to hard code the credentials . Thats why we integrate Azure Key Vault .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17f1a117-fd78-4fc0-a1c5-86b1d605d096",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "######################### Procedure ############################################################\n",
    "## step 01 : Azure console -> create resource > key valut -> create key valut ->  give any name -> and create a one . Now go inside this created resources -> settings -> secrets  -> generate ;\n",
    "#                    Name : sql_password\n",
    "#                    value : test123\n",
    "##   Now we have to create a databricks scoped credentials . For that go for the following link : https://community.cloud.databricks.com/?0=3418180493285762&o=1898845374819542#secrets/createScope \n",
    "# As the scope name give any name .  Now go inside the valut resource we created -> settings -> properties -> \n",
    "#      copy  Valut URL and Resource ID and paste them  over DNS Name and Resource ID \n",
    "\n",
    "## Now to retrieve the key\n",
    "jdbcPassword = dbutils.scretes.get(scope='<scope name>',key='<key names')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1154ba94-83e8-4c8d-920d-db7c1b347482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2614fa0e-a108-4686-ab47-b73bb3517c04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################       Structured Straming             #########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dc5cb58-62a1-4830-b62a-a7394b0135ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################## Terminology ############\n",
    "# Read Stream : to read the streaming data\n",
    "# Writestream : to write the streaming data\n",
    "# Checkpoint : Checkpointing plays key role in fault-tolerant and incremental stram processing pipelines. It maintains intermediate state on HDFS compatible file systems to recover from failures . \n",
    "## Trigger : data continously flows into a straming system. The special event trigget initiates the streaming. Default , fixed interval , one-time\n",
    "## Output mode : Append , Complete , Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc42c8c2-ca52-4b9f-9bc3-ccfa30c57687",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_17_221116.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_221318.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94252cd6-c772-4d01-bc0f-f6491f61ccd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############# Define the schema structure for our sample streaming data \n",
    "from pyspark.sql.types import StructType , StructField , IntegerType , StringType\n",
    "\n",
    "schema_defined = StructType([\n",
    "    StructField('File',StringType(),True),\n",
    "    StructField('Shop',StringType(),True),\n",
    "    StructField('Sale_count',IntegerType(),True),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "###############   Create the fodler structure   ######################\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/stream_checkpoint/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/stream_read/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/stream_write/\")\n",
    "\n",
    "\n",
    "###########     Now lets read the streaming data       #################\n",
    "df = spark.readStream.format('csv').schema(schema_defined).option('header',True).option('sep',\";\").load(\"/FileStore/tables/stream_read\")   ## this will not only read the existing files , whenever new files comes in those files also be read by this  . So this process keeps on running \n",
    "\n",
    "df1 = df .groupBy('Shop').sum(\"Sale_count\")\n",
    "\n",
    "\n",
    "\n",
    "############ Write Streaming Data ################\n",
    "df4 = df.writeStream.format('parquet').outputMode('append').option('path','/FileStore/tables/stram_writter/').option('checkpointLocation','/FileStore/tables/stram_checkpoint/').start().awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "########### Verify the  written stream output data #############\n",
    "display(spark.read.format(\"parquet\").load(\"/FileStore/tables/stream_write/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53cde21c-72ff-40f2-a2ff-2969b3d5df91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0077be7e-8063-4fb6-9c34-d2abcbdb6705",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################       Repartition vs Coalesce           ########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab844c7-9bb3-4b7a-8b1e-bb0b49adac0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_17_222802.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_222751.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_222722.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_223117.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_223206.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c150f2a2-46dd-4c3a-a63b-8348984540e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In Apache Spark, repartition and coalesce are methods used to change the number of partitions of a DataFrame or RDD. They help in optimizing the performance of your Spark job by adjusting the distribution of data across partitions. Here's a simple explanation of each:\n",
    "\n",
    "Repartition\n",
    "Purpose: Increase or decrease the number of partitions.\n",
    "Mechanism: Shuffles data across the network to evenly distribute it into the specified number of partitions.\n",
    "Use Case: When you want to increase the number of partitions for better parallelism or when the current partitioning is skewed (some partitions have more data than others).\n",
    "Cost: High, because it involves a full shuffle of the data across the cluster.\n",
    "\n",
    "\n",
    "Coalesce\n",
    "Purpose: Decrease the number of partitions.\n",
    "Mechanism: Merges existing partitions without a full shuffle, minimizing data movement.\n",
    "Use Case: When you want to decrease the number of partitions, especially after a filter operation that reduces the dataset size significantly.\n",
    "Cost: Lower than repartition since it avoids a full shuffle, but it only reduces the number of partitions.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25724f1c-61c8-4063-b334-c3fb46b0117a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac825af-7b40-4d2a-ada5-3866cc404f83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'134217728b'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b84f144-8a2f-43fc-adfd-225733aa0919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Generating  data within spark environment\n",
    "\n",
    "## rdd = sc.parallelize(range(1,11))\n",
    "## rdd.getNumPartitions()\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "df = spark.createDataFrame(range(10),IntegerType())\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cab4552-a6d0-48bf-9d08-4433b10ae38b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[Row(value=0)],\n",
       " [Row(value=1)],\n",
       " [Row(value=2)],\n",
       " [Row(value=3), Row(value=4)],\n",
       " [Row(value=5)],\n",
       " [Row(value=6)],\n",
       " [Row(value=7)],\n",
       " [Row(value=8), Row(value=9)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Verify the data within all partitions \n",
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4706342d-2a77-4f14-91c2-80609a566455",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########  Read external files in spark\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")\n",
    "\n",
    "df = spark.read.format('csv').option('inferschema',True).option('header',True).option('sep',\",\").load(\"/FileStore/tables/\")  # read all the csv files under this folder\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c069addb-a076-469e-9e4a-541cff86e7f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'200000'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### Change the maxpartitionbytes parameter which changes in no of partitions\n",
    "spark.conf.set('spark.sql.files.maxPartitionBytes',200000)\n",
    "spark.conf.get('spark.sql.files.maxPartitionBytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f455401-9cd7-4c5a-9d0a-c3095897fb48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('inferschema',True).option('header',True).option('sep',\",\").load(\"/FileStore/tables/\")  # read all the csv files under this folder\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c05f9a-67db-4dcf-9583-aa8789a468cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Create a single partition with all the data.This is not good for performance ,as one core would process entire data while all other cores are kept idle\n",
    "\n",
    "rdd2 = sc.parallelize(range(100),1)\n",
    "\n",
    "rdd2.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39410c54-ce38-40c4-8b3c-d3f81361c8ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[Row(value=0)],\n",
       " [Row(value=1)],\n",
       " [Row(value=2)],\n",
       " [Row(value=3), Row(value=4)],\n",
       " [Row(value=5)],\n",
       " [Row(value=6)],\n",
       " [Row(value=7)],\n",
       " [Row(value=8), Row(value=9)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################################################################################################  Repartition ##############################################################\n",
    "###############################################################################################################\n",
    "from pyspark.sql.types import IntegerType\n",
    "df = spark.createDataFrame(range(10),IntegerType())\n",
    "\n",
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a6867d-6218-487b-987e-4c82527f7fa2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.repartition(20)\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c4cc9c-3bb2-4564-811f-bdba4e430add",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[],\n",
       " [Row(value=8)],\n",
       " [Row(value=9)],\n",
       " [],\n",
       " [Row(value=1)],\n",
       " [],\n",
       " [Row(value=6)],\n",
       " [],\n",
       " [Row(value=3)],\n",
       " [Row(value=0), Row(value=2), Row(value=4)],\n",
       " [],\n",
       " [Row(value=7)],\n",
       " [],\n",
       " [],\n",
       " [Row(value=5)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37db8ae4-c362-4b1b-9ee3-13c308a8fcdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.repartition(2)\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea43b19-f0fb-4c01-bbeb-1ece1d74b7a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[],\n",
       " [Row(value=8)],\n",
       " [Row(value=9)],\n",
       " [],\n",
       " [Row(value=1)],\n",
       " [],\n",
       " [Row(value=6)],\n",
       " [],\n",
       " [Row(value=3)],\n",
       " [Row(value=0), Row(value=2), Row(value=4)],\n",
       " [],\n",
       " [Row(value=7)],\n",
       " [],\n",
       " [],\n",
       " [Row(value=5)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f951a8e3-7d3e-41df-bc0a-cd76d438cdd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[Row(value=0), Row(value=1), Row(value=2), Row(value=3), Row(value=4)],\n",
       " [Row(value=5), Row(value=6), Row(value=7), Row(value=8), Row(value=9)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Coalesce\n",
    "df2 = df.coalesce(2)\n",
    "df2.rdd.getNumPartitions()\n",
    "df2.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74011719-693e-448b-944a-da005d4bc533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdc4947a-c73b-42ef-97f7-7a12d782576d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ec2a0f3-dff0-4201-8d28-19237deaf762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c6ea783-c894-433d-b063-c70093870f60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################         Cache vs Persist          ##############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dddc2b31-1412-4ff1-ad88-32f3b67a9da2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_17_233542.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_233559.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_233625.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2718a50-1528-4182-a3ec-f9ff7e33bb4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0117cc76-b31d-4eb8-a4df-a5c7f934d90b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2886b50-ad7d-497a-aae3-235531822817",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################        Broadcast Variables        ##############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d80d84-ea12-4df6-930f-70ca368689a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_17_233847.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_233859.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_17_233910.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579a9116-abbe-4646-9b4d-9c27a42e1cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n|Store_id|    Item|Amount|\n+--------+--------+------+\n|     100|Cosmetic|   150|\n|     200| Apparel|   250|\n|     300|   Shirt|   400|\n|     400| Trouser|   500|\n|     500|   Socks|    20|\n|     100|    Belt|    70|\n|     200|Cosmetic|   250|\n|     300|    Shoe|   400|\n|     400|   Socks|    25|\n|     500|  Shorts|   100|\n+--------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "Transaction = [\n",
    "    (100,'Cosmetic',150),\n",
    "    (200,'Apparel',250),\n",
    "    (300,'Shirt',400),\n",
    "    (400,'Trouser',500),\n",
    "    (500,'Socks',20),\n",
    "    (100,'Belt',70),\n",
    "    (200,'Cosmetic',250),\n",
    "    (300,'Shoe',400),\n",
    "    (400,'Socks',25),\n",
    "    (500,'Shorts',100),\n",
    "]\n",
    "\n",
    "transactionDF = spark.createDataFrame(data=Transaction,schema=['Store_id','Item','Amount'])\n",
    "transactionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a082997c-d0d5-4f27-a685-22c1a05d1126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n|Store_id|      Store_name|\n+--------+----------------+\n|     100|    Store_London|\n|     200|     Store_Paris|\n|     300|Store_Frankfruit|\n|     400| Store_Stockholm|\n|     500|      Store_Oslo|\n+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#### Create smaple dimension table \n",
    "store = [\n",
    "    (100,'Store_London'),\n",
    "    (200,'Store_Paris'),\n",
    "    (300,'Store_Frankfruit'),\n",
    "    (400,'Store_Stockholm'),\n",
    "    (500,'Store_Oslo')\n",
    "]\n",
    "\n",
    "storeDF = spark.createDataFrame(data=store,schema=['Store_id','Store_name'])\n",
    "storeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36208061-fb65-4f45-9beb-50f4949634b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+--------+----------------+\n|Store_id|    Item|Amount|Store_id|      Store_name|\n+--------+--------+------+--------+----------------+\n|     100|Cosmetic|   150|     100|    Store_London|\n|     200| Apparel|   250|     200|     Store_Paris|\n|     300|   Shirt|   400|     300|Store_Frankfruit|\n|     400| Trouser|   500|     400| Store_Stockholm|\n|     500|   Socks|    20|     500|      Store_Oslo|\n|     100|    Belt|    70|     100|    Store_London|\n|     200|Cosmetic|   250|     200|     Store_Paris|\n|     300|    Shoe|   400|     300|Store_Frankfruit|\n|     400|   Socks|    25|     400| Store_Stockholm|\n|     500|  Shorts|   100|     500|      Store_Oslo|\n+--------+--------+------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinDF = transactionDF.join(broadcast(storeDF),transactionDF['Store_id']==storeDF['Store_id'])\n",
    "joinDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da917c93-945a-4088-90e8-385c75f1f00c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   ResultQueryStage 1, Statistics(sizeInBytes=72.7 EiB, ColumnStat: N/A, isRuntime=true)\n   +- *(2) BroadcastHashJoin [Store_id#122L], [Store_id#145L], Inner, BuildRight, false, true\n      :- *(2) Filter isnotnull(Store_id#122L)\n      :  +- *(2) Scan ExistingRDD[Store_id#122L,Item#123,Amount#124L]\n      +- ShuffleQueryStage 0, Statistics(sizeInBytes=200.0 B, rowCount=5, ColumnStat: N/A, isRuntime=true)\n         +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=384]\n            +- *(1) Filter isnotnull(Store_id#145L)\n               +- *(1) Scan ExistingRDD[Store_id#145L,Store_name#146]\n+- == Initial Plan ==\n   BroadcastHashJoin [Store_id#122L], [Store_id#145L], Inner, BuildRight, false, true\n   :- Filter isnotnull(Store_id#122L)\n   :  +- Scan ExistingRDD[Store_id#122L,Item#123,Amount#124L]\n   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=350]\n      +- Filter isnotnull(Store_id#145L)\n         +- Scan ExistingRDD[Store_id#145L,Store_name#146]\n\n\n"
     ]
    }
   ],
   "source": [
    "joinDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c658dcd-20ec-4ea0-9b59-0286ff2ae73b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nJoin Inner, (Store_id#122L = Store_id#145L)\n:- LogicalRDD [Store_id#122L, Item#123, Amount#124L], false\n+- ResolvedHint (strategy=broadcast)\n   +- LogicalRDD [Store_id#145L, Store_name#146], false\n\n== Analyzed Logical Plan ==\nStore_id: bigint, Item: string, Amount: bigint, Store_id: bigint, Store_name: string\nJoin Inner, (Store_id#122L = Store_id#145L)\n:- LogicalRDD [Store_id#122L, Item#123, Amount#124L], false\n+- ResolvedHint (strategy=broadcast)\n   +- LogicalRDD [Store_id#145L, Store_name#146], false\n\n== Optimized Logical Plan ==\nJoin Inner, (Store_id#122L = Store_id#145L), rightHint=(strategy=broadcast), joinId=2\n:- Filter isnotnull(Store_id#122L)\n:  +- LogicalRDD [Store_id#122L, Item#123, Amount#124L], false\n+- Filter isnotnull(Store_id#145L)\n   +- LogicalRDD [Store_id#145L, Store_name#146], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   ResultQueryStage 1, Statistics(sizeInBytes=72.7 EiB, ColumnStat: N/A, isRuntime=true)\n   +- *(2) BroadcastHashJoin [Store_id#122L], [Store_id#145L], Inner, BuildRight, false, true\n      :- *(2) Filter isnotnull(Store_id#122L)\n      :  +- *(2) Scan ExistingRDD[Store_id#122L,Item#123,Amount#124L]\n      +- ShuffleQueryStage 0, Statistics(sizeInBytes=200.0 B, rowCount=5, ColumnStat: N/A, isRuntime=true)\n         +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=384]\n            +- *(1) Filter isnotnull(Store_id#145L)\n               +- *(1) Scan ExistingRDD[Store_id#145L,Store_name#146]\n+- == Initial Plan ==\n   BroadcastHashJoin [Store_id#122L], [Store_id#145L], Inner, BuildRight, false, true\n   :- Filter isnotnull(Store_id#122L)\n   :  +- Scan ExistingRDD[Store_id#122L,Item#123,Amount#124L]\n   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=350]\n      +- Filter isnotnull(Store_id#145L)\n         +- Scan ExistingRDD[Store_id#145L,Store_name#146]\n\n"
     ]
    }
   ],
   "source": [
    "joinDF.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c9a9c73-396b-46bc-ab34-605fecbb7c5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4314b4d7-b13b-40ae-bb3c-9f7e3ea2e0cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3db2a90-5a8b-4c41-a0be-0b0bff0b011b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################       Adaptive Query Execution      ############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3194e9a1-228d-40a5-9196-6ff20f793291",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## spark 1.x  : Introduced catalyst optimizer\n",
    "## spark 2.x : Introduced cost based optimizer\n",
    "## spark 3.x : Introduced adaptive query execution\n",
    "\n",
    "\n",
    "## syntax : spark.conf.set('spark.sql.adaptive.enabled',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c9e93c1-d8cd-4582-a178-345711f89edb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_18_002851.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_18_003006.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_18_003023.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6582f819-b2ed-4c30-984e-a14cb1c83f5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### AEQ Featuers \n",
    "## (01) Dynamically coalescing shuffle partitions\n",
    "## (02) Dynamically switching oin strategies\n",
    "## (03) Dynamically optimizing skew joins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4b5709-b601-4af1-bfb1-6eb9d9e33dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### shuffle join is costly and not suitable for all the use-cases\n",
    "#### Broadcast is high-performant but again not suitable for all use-cases\n",
    "### If AQE enabled , the best join strategy is selected automatically\n",
    "\n",
    "\n",
    "\n",
    "######### Dynamically optimizing skew joins\n",
    "## * When we distribute data unevenly , it causes data skew by producing uneven size of partitions. So the processing time for each task would vary so skew can significantly downgrade query performance , especially with joins \n",
    "\n",
    "## * AQE skew join optimization detects such skew automatically from shuffle files statistics.It then splits the skewed partitions into smaller subpartitions , which will be joined to the corresponding partition from the other side respectively . \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd84a55d-1d30-445b-acb0-0a4a60f6fd41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_18_003559.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a2aaea7-6eeb-4607-a0bc-b6a501e1d883",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "700a3e8d-1b00-48f6-a3ab-a2445a200d55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44111e29-5b00-407c-8ab6-d244de7cf7fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "193015b1-c542-426c-8c41-2c090b186749",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################            Handling Null        ################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47167afd-9d29-43b1-a193-4e82f707f898",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th></tr></thead><tbody><tr><td>Micheal</td><td>Science</td><td>80</td><td>p</td><td>90</td></tr><tr><td>Nancy</td><td>Maths</td><td>90</td><td>p</td><td>null</td></tr><tr><td>Dvid</td><td>English</td><td>20</td><td>F</td><td>80</td></tr><tr><td>Jhon</td><td>Science</td><td>null</td><td>p</td><td>null</td></tr><tr><td>Blessy</td><td>null</td><td>30</td><td>F</td><td>50</td></tr><tr><td>Martin</td><td>Maths</td><td>null</td><td>null</td><td>70</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Micheal",
         "Science",
         80,
         "p",
         90
        ],
        [
         "Nancy",
         "Maths",
         90,
         "p",
         null
        ],
        [
         "Dvid",
         "English",
         20,
         "F",
         80
        ],
        [
         "Jhon",
         "Science",
         null,
         "p",
         null
        ],
        [
         "Blessy",
         null,
         30,
         "F",
         50
        ],
        [
         "Martin",
         "Maths",
         null,
         null,
         70
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## isNull() : returns all the rows where certain column on which we apply this function contains Null values\n",
    "##  isNotNUll() : creates a new dataframe by eliminating all the rows where certain column on which we apply this function \n",
    "\n",
    "data_student = [\n",
    "    ('Micheal','Science',80,'p',90),\n",
    "    ('Nancy','Maths',90,'p',None),\n",
    "    ('Dvid','English',20,'F',80),\n",
    "    ('Jhon','Science',None,'p',None),\n",
    "    ('Blessy',None,30,'F',50),\n",
    "    ('Martin','Maths',None,None,70),\n",
    "]\n",
    "\n",
    "schema = ['name','Subject','Mark','Status','Attendance']\n",
    "df = spark.createDataFrame(data=data_student,schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9640e7a7-d866-4b8c-80a3-5fc9757859c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th></tr></thead><tbody><tr><td>Jhon</td><td>Science</td><td>null</td><td>p</td><td>null</td></tr><tr><td>Martin</td><td>Maths</td><td>null</td><td>null</td><td>70</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jhon",
         "Science",
         null,
         "p",
         null
        ],
        [
         "Martin",
         "Maths",
         null,
         null,
         70
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### Filter all records with NUll values \n",
    "display(df.filter(df.Mark.isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c8ad6d-4128-4162-85b2-562733ff719a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th></tr></thead><tbody><tr><td>Micheal</td><td>Science</td><td>80</td><td>p</td><td>90</td></tr><tr><td>Nancy</td><td>Maths</td><td>90</td><td>p</td><td>null</td></tr><tr><td>Dvid</td><td>English</td><td>20</td><td>F</td><td>80</td></tr><tr><td>Blessy</td><td>null</td><td>30</td><td>F</td><td>50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Micheal",
         "Science",
         80,
         "p",
         90
        ],
        [
         "Nancy",
         "Maths",
         90,
         "p",
         null
        ],
        [
         "Dvid",
         "English",
         20,
         "F",
         80
        ],
        [
         "Blessy",
         null,
         30,
         "F",
         50
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######### Filter all records without Null values\n",
    "display(df.filter(df.Mark.isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18ca4401-ff62-4803-97d9-3758a2095709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## na.drop()  : Drops the rows if any or all columns contain Null value\n",
    "'''\n",
    "df1 = df.na.drop('parameter')\n",
    "parameter : 'any','all',\"subset=['column1,columns2']\"\n",
    "'''\n",
    "\n",
    "# na.fill() : Populates dummy value for all Null values given as parameter to this fucntion\n",
    "# df1 = df.na.fill(value=\"Dummy value\",subset=[\"Column\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c0b862-2403-41f7-9b01-93f6f8f9834a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b92d856-e944-41ec-b2eb-3b2205cbe75b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49297a6c-1fc4-47c2-8117-961bb3e85161",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#################            UDF (user defined functions )        ################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fe48c54-888a-450e-8b0a-0d98f47566ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def UDF_NAME(parameters):\n",
    "    --code to perform the task \n",
    "    return RETURN_OUTPUT \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9bfcbbc-207b-4b1e-ab7c-6f06fd693e0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>Name</th><th>doj</th><th>employee_dept_id</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Michael Robinson</td><td>1999-06-01</td><td>100</td><td>2000</td></tr><tr><td>20</td><td>James Wood</td><td>2003-01-01</td><td>200</td><td>8000</td></tr><tr><td>30</td><td>Chris Andrews</td><td>2005-04-01</td><td>100</td><td>6000</td></tr><tr><td>40</td><td>Mark Bond</td><td>2008-10-01</td><td>100</td><td>7000</td></tr><tr><td>50</td><td>Steve Watson</td><td>1996-02-01</td><td>400</td><td>1000</td></tr><tr><td>60</td><td>Mathews Simon</td><td>1998-11-01</td><td>500</td><td>5000</td></tr><tr><td>70</td><td>Peter Paul</td><td>2011-04-01</td><td>600</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Michael Robinson",
         "1999-06-01",
         "100",
         2000
        ],
        [
         20,
         "James Wood",
         "2003-01-01",
         "200",
         8000
        ],
        [
         30,
         "Chris Andrews",
         "2005-04-01",
         "100",
         6000
        ],
        [
         40,
         "Mark Bond",
         "2008-10-01",
         "100",
         7000
        ],
        [
         50,
         "Steve Watson",
         "1996-02-01",
         "400",
         1000
        ],
        [
         60,
         "Mathews Simon",
         "1998-11-01",
         "500",
         5000
        ],
        [
         70,
         "Peter Paul",
         "2011-04-01",
         "600",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data= [\n",
    "    (10,'Michael Robinson',\"1999-06-01\",\"100\",2000),\n",
    "    (20,'James Wood',\"2003-01-01\",\"200\",8000),\n",
    "    (30,'Chris Andrews',\"2005-04-01\",\"100\",6000),\n",
    "    (40,'Mark Bond',\"2008-10-01\",\"100\",7000),\n",
    "    (50,'Steve Watson',\"1996-02-01\",\"400\",1000),\n",
    "    (60,'Mathews Simon',\"1998-11-01\",\"500\",5000),\n",
    "    (70,'Peter Paul',\"2011-04-01\",\"600\",5000),\n",
    "]\n",
    "\n",
    "employee_schema = ['employee_id','Name','doj','employee_dept_id','salary']\n",
    "empDF = spark.createDataFrame(data=employee_data,schema=employee_schema)\n",
    "display(empDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba462f1-05f7-4dff-a64f-19fbcb2a6770",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Define  a UDF to rename Columns \n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "def rename_columns(rename_df):\n",
    "    for column in rename_df.columns:\n",
    "        new_column = \"Col_\" + column\n",
    "        rename_df = rename_df.withColumnRenamed(column,new_column)\n",
    "\n",
    "        return rename_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591d48e8-bc6e-4e44-9e4a-826db13e3a0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Col_employee_id</th><th>Name</th><th>doj</th><th>employee_dept_id</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Michael Robinson</td><td>1999-06-01</td><td>100</td><td>2000</td></tr><tr><td>20</td><td>James Wood</td><td>2003-01-01</td><td>200</td><td>8000</td></tr><tr><td>30</td><td>Chris Andrews</td><td>2005-04-01</td><td>100</td><td>6000</td></tr><tr><td>40</td><td>Mark Bond</td><td>2008-10-01</td><td>100</td><td>7000</td></tr><tr><td>50</td><td>Steve Watson</td><td>1996-02-01</td><td>400</td><td>1000</td></tr><tr><td>60</td><td>Mathews Simon</td><td>1998-11-01</td><td>500</td><td>5000</td></tr><tr><td>70</td><td>Peter Paul</td><td>2011-04-01</td><td>600</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Michael Robinson",
         "1999-06-01",
         "100",
         2000
        ],
        [
         20,
         "James Wood",
         "2003-01-01",
         "200",
         8000
        ],
        [
         30,
         "Chris Andrews",
         "2005-04-01",
         "100",
         6000
        ],
        [
         40,
         "Mark Bond",
         "2008-10-01",
         "100",
         7000
        ],
        [
         50,
         "Steve Watson",
         "1996-02-01",
         "400",
         1000
        ],
        [
         60,
         "Mathews Simon",
         "1998-11-01",
         "500",
         5000
        ],
        [
         70,
         "Peter Paul",
         "2011-04-01",
         "600",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Col_employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rename_df = rename_columns(empDF)\n",
    "display(rename_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b796cf-ee9e-4bf1-ba8b-0176a085144f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###### UDF to convert name into upper case \n",
    "from pyspark.sql.functions import upper,col  \n",
    "\n",
    "def upperCase_col(df):\n",
    "    empDF_upper = df.withColumn('name_upper',upper(df.Name))\n",
    "    return empDF_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a0ca4a-012e-4827-ad3f-3e694d705219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebbd82cf-2b35-4dce-b686-c298463031aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################            Skew Optimization          #######################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52e1f2ca-f677-443a-91d4-98fe650f5283",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_18_012217.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_18_012314.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_18_012416.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_18_012456.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7500532f-8120-413e-8241-dc906de98d85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###  How to handle skew data \n",
    "## * Salt the skewed column with a random number creating better distribution across each partition.\n",
    "## * Apply skew-hint , with the information from these hints , Spark can construct a better query plan , one that does not suffer from data skew\n",
    "## * Use Broadcast join for smaller tables\n",
    "## * Enable Adaptive query execution if you are using spark 3 which will balance out thep artitions for us automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ce7ee14-dc7c-4de7-ab1c-1137d282eb5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_18_015330.jpg)\n",
    "![test image](files/tables/Screenshot_2024_07_18_015421.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3063969d-8c74-45cb-8595-31bc1ccfdec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb67df8a-6d33-4128-960e-21e40f81255d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30ee69fb-9826-4659-9985-3ebd0abeb543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################         AutoScaling              ############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84d6e53-f984-4ca1-8d0c-a019c8f5e51b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### What is autoscaling ?\n",
    "## * Databricks choose dynamically  the appropriate number of workers required to run the job based on range of number of workers\n",
    "## * It is one of the performance optimization techniques\n",
    "## * It is also one of cost saving technique\n",
    "\n",
    "\n",
    "##### There are 2 types of autoscaling \n",
    "##    -> Optimized\n",
    "##    -> Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "284e4de3-1de0-46cc-be69-0b4421133043",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_18_015944.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeac1859-0419-48f4-84f6-f39ed36dd903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202fb693-e291-430b-af93-34c25ef3c686",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################        Dataframe Checkpointing                ###############################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dbf0841-8471-4d63-b012-4c3730ef9f68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### When we use it : When the computations takes so long\n",
    "######## What is dataframe checkpointing \n",
    "##  * Checkpointint is process of materializing the dataframe resultant data into storage directory\n",
    "##  * It is one of the performance optimization technique\n",
    "##  * Though it is different from Cache , looks similar in some aspects\n",
    "##  * It boosts the performance for  use-cases where particular dataframe gets iterated mutiple times . It is useful in Machine Learning iterative algorithms where the plan may grow exponentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "175193bf-4d12-4695-99bc-2666793ff827",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![test image](files/tables/Screenshot_2024_07_18_020131.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c8718c7-5290-4880-90d5-8590a96c5f80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49ba6f66-93d5-4019-9b17-c6bd0316b826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2e60b2c-e129-4eb3-b021-6e0829d29d69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852b0178-9ced-4135-8c74-28f47ef7f841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################        Functions                  ###########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b08cc07a-ac24-4935-b873-cfd982acf346",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>Name</th><th>doj</th><th>employee_dept_id</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Michael Robinson</td><td>1999-06-01</td><td>100</td><td>2000</td></tr><tr><td>20</td><td>James Wood</td><td>2003-01-01</td><td>200</td><td>8000</td></tr><tr><td>30</td><td>Chris Andrews</td><td>2005-04-01</td><td>100</td><td>6000</td></tr><tr><td>40</td><td>Mark Bond</td><td>2008-10-01</td><td>100</td><td>7000</td></tr><tr><td>50</td><td>Steve Watson</td><td>1996-02-01</td><td>400</td><td>1000</td></tr><tr><td>60</td><td>Mathews Simon</td><td>1998-11-01</td><td>500</td><td>5000</td></tr><tr><td>70</td><td>Peter Paul</td><td>2011-04-01</td><td>600</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Michael Robinson",
         "1999-06-01",
         "100",
         2000
        ],
        [
         20,
         "James Wood",
         "2003-01-01",
         "200",
         8000
        ],
        [
         30,
         "Chris Andrews",
         "2005-04-01",
         "100",
         6000
        ],
        [
         40,
         "Mark Bond",
         "2008-10-01",
         "100",
         7000
        ],
        [
         50,
         "Steve Watson",
         "1996-02-01",
         "400",
         1000
        ],
        [
         60,
         "Mathews Simon",
         "1998-11-01",
         "500",
         5000
        ],
        [
         70,
         "Peter Paul",
         "2011-04-01",
         "600",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### split : Pyspark function that splits a single column into multiple columns based on certain logic \n",
    "###  syntax : pyspark.sql.functions.splt(str,pattern,limit=1)\n",
    "\n",
    "employee_data= [\n",
    "    (10,'Michael Robinson',\"1999-06-01\",\"100\",2000),\n",
    "    (20,'James Wood',\"2003-01-01\",\"200\",8000),\n",
    "    (30,'Chris Andrews',\"2005-04-01\",\"100\",6000),\n",
    "    (40,'Mark Bond',\"2008-10-01\",\"100\",7000),\n",
    "    (50,'Steve Watson',\"1996-02-01\",\"400\",1000),\n",
    "    (60,'Mathews Simon',\"1998-11-01\",\"500\",5000),\n",
    "    (70,'Peter Paul',\"2011-04-01\",\"600\",5000),\n",
    "]\n",
    "\n",
    "employee_schema = ['employee_id','Name','doj','employee_dept_id','salary']\n",
    "empDF = spark.createDataFrame(data=employee_data,schema=employee_schema)\n",
    "display(empDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f646e1-8649-47d0-8234-913466ff2bf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>Name</th><th>doj</th><th>employee_dept_id</th><th>salary</th><th>First_Name</th><th>Last_Name</th></tr></thead><tbody><tr><td>10</td><td>Michael Robinson</td><td>1999-06-01</td><td>100</td><td>2000</td><td>Michael</td><td>Robinson</td></tr><tr><td>20</td><td>James Wood</td><td>2003-01-01</td><td>200</td><td>8000</td><td>James</td><td>Wood</td></tr><tr><td>30</td><td>Chris Andrews</td><td>2005-04-01</td><td>100</td><td>6000</td><td>Chris</td><td>Andrews</td></tr><tr><td>40</td><td>Mark Bond</td><td>2008-10-01</td><td>100</td><td>7000</td><td>Mark</td><td>Bond</td></tr><tr><td>50</td><td>Steve Watson</td><td>1996-02-01</td><td>400</td><td>1000</td><td>Steve</td><td>Watson</td></tr><tr><td>60</td><td>Mathews Simon</td><td>1998-11-01</td><td>500</td><td>5000</td><td>Mathews</td><td>Simon</td></tr><tr><td>70</td><td>Peter Paul</td><td>2011-04-01</td><td>600</td><td>5000</td><td>Peter</td><td>Paul</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Michael Robinson",
         "1999-06-01",
         "100",
         2000,
         "Michael",
         "Robinson"
        ],
        [
         20,
         "James Wood",
         "2003-01-01",
         "200",
         8000,
         "James",
         "Wood"
        ],
        [
         30,
         "Chris Andrews",
         "2005-04-01",
         "100",
         6000,
         "Chris",
         "Andrews"
        ],
        [
         40,
         "Mark Bond",
         "2008-10-01",
         "100",
         7000,
         "Mark",
         "Bond"
        ],
        [
         50,
         "Steve Watson",
         "1996-02-01",
         "400",
         1000,
         "Steve",
         "Watson"
        ],
        [
         60,
         "Mathews Simon",
         "1998-11-01",
         "500",
         5000,
         "Mathews",
         "Simon"
        ],
        [
         70,
         "Peter Paul",
         "2011-04-01",
         "600",
         5000,
         "Peter",
         "Paul"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "First_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df1 = empDF.withColumn('First_Name',split(empDF['Name'],' ').getItem(0)).withColumn('Last_Name',split(empDF['Name'],' ').getItem(1))\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c57d9c-0c33-4133-8cd0-ba317fabcc1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>Name</th><th>doj</th><th>employee_dept_id</th><th>salary</th><th>First_Name</th><th>Last_Name</th></tr></thead><tbody><tr><td>10</td><td>Michael Robinson</td><td>1999-06-01</td><td>100</td><td>2000</td><td>Michael</td><td>Robinson</td></tr><tr><td>20</td><td>James Wood</td><td>2003-01-01</td><td>200</td><td>8000</td><td>James</td><td>Wood</td></tr><tr><td>30</td><td>Chris Andrews</td><td>2005-04-01</td><td>100</td><td>6000</td><td>Chris</td><td>Andrews</td></tr><tr><td>40</td><td>Mark Bond</td><td>2008-10-01</td><td>100</td><td>7000</td><td>Mark</td><td>Bond</td></tr><tr><td>50</td><td>Steve Watson</td><td>1996-02-01</td><td>400</td><td>1000</td><td>Steve</td><td>Watson</td></tr><tr><td>60</td><td>Mathews Simon</td><td>1998-11-01</td><td>500</td><td>5000</td><td>Mathews</td><td>Simon</td></tr><tr><td>70</td><td>Peter Paul</td><td>2011-04-01</td><td>600</td><td>5000</td><td>Peter</td><td>Paul</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Michael Robinson",
         "1999-06-01",
         "100",
         2000,
         "Michael",
         "Robinson"
        ],
        [
         20,
         "James Wood",
         "2003-01-01",
         "200",
         8000,
         "James",
         "Wood"
        ],
        [
         30,
         "Chris Andrews",
         "2005-04-01",
         "100",
         6000,
         "Chris",
         "Andrews"
        ],
        [
         40,
         "Mark Bond",
         "2008-10-01",
         "100",
         7000,
         "Mark",
         "Bond"
        ],
        [
         50,
         "Steve Watson",
         "1996-02-01",
         "400",
         1000,
         "Steve",
         "Watson"
        ],
        [
         60,
         "Mathews Simon",
         "1998-11-01",
         "500",
         5000,
         "Mathews",
         "Simon"
        ],
        [
         70,
         "Peter Paul",
         "2011-04-01",
         "600",
         5000,
         "Peter",
         "Paul"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "First_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ ANother way of splitting\n",
    "import pyspark\n",
    "split_col = pyspark.sql.functions.split(empDF['Name'],' ')\n",
    "df2 = empDF.withColumn('First_Name',split_col.getItem(0)).withColumn('Last_Name',split_col.getItem(1))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5501a0c9-d9fa-40ea-a141-8b0704d1b2c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: (Column<'split(doj, -, -1)[0] AS joining_year[split_col][1] AS joiing_month'>,\n Column<'split(doj, -, -1)[1] AS joining_day'>)"
     ]
    }
   ],
   "source": [
    "############ ANother way of splitting\n",
    "split_col = pyspark.sql.functions.split(empDF['doj'],'-')\n",
    "\n",
    "df3 = empDF.select('employee_id','Name','employee_dept_id','salary')\n",
    "split_col.getItem(0).alias('joining_year').split_col.getItem(1).alias('joiing_month'),split_col.getItem(1).alias('joining_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe82e3a-3ca3-4afe-b77f-5fc1e9c49813",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>Name</th><th>doj</th><th>employee_dept_id</th><th>salary</th><th>First_Name</th><th>Last_Name</th><th>Joining_Year</th><th>Joining_Month</th><th>Joining_Day</th></tr></thead><tbody><tr><td>10</td><td>Michael Robinson</td><td>1999-06-01</td><td>100</td><td>2000</td><td>Michael</td><td>Robinson</td><td>1999-06-01</td><td>null</td><td>null</td></tr><tr><td>20</td><td>James Wood</td><td>2003-01-01</td><td>200</td><td>8000</td><td>James</td><td>Wood</td><td>2003-01-01</td><td>null</td><td>null</td></tr><tr><td>30</td><td>Chris Andrews</td><td>2005-04-01</td><td>100</td><td>6000</td><td>Chris</td><td>Andrews</td><td>2005-04-01</td><td>null</td><td>null</td></tr><tr><td>40</td><td>Mark Bond</td><td>2008-10-01</td><td>100</td><td>7000</td><td>Mark</td><td>Bond</td><td>2008-10-01</td><td>null</td><td>null</td></tr><tr><td>50</td><td>Steve Watson</td><td>1996-02-01</td><td>400</td><td>1000</td><td>Steve</td><td>Watson</td><td>1996-02-01</td><td>null</td><td>null</td></tr><tr><td>60</td><td>Mathews Simon</td><td>1998-11-01</td><td>500</td><td>5000</td><td>Mathews</td><td>Simon</td><td>1998-11-01</td><td>null</td><td>null</td></tr><tr><td>70</td><td>Peter Paul</td><td>2011-04-01</td><td>600</td><td>5000</td><td>Peter</td><td>Paul</td><td>2011-04-01</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Michael Robinson",
         "1999-06-01",
         "100",
         2000,
         "Michael",
         "Robinson",
         "1999-06-01",
         null,
         null
        ],
        [
         20,
         "James Wood",
         "2003-01-01",
         "200",
         8000,
         "James",
         "Wood",
         "2003-01-01",
         null,
         null
        ],
        [
         30,
         "Chris Andrews",
         "2005-04-01",
         "100",
         6000,
         "Chris",
         "Andrews",
         "2005-04-01",
         null,
         null
        ],
        [
         40,
         "Mark Bond",
         "2008-10-01",
         "100",
         7000,
         "Mark",
         "Bond",
         "2008-10-01",
         null,
         null
        ],
        [
         50,
         "Steve Watson",
         "1996-02-01",
         "400",
         1000,
         "Steve",
         "Watson",
         "1996-02-01",
         null,
         null
        ],
        [
         60,
         "Mathews Simon",
         "1998-11-01",
         "500",
         5000,
         "Mathews",
         "Simon",
         "1998-11-01",
         null,
         null
        ],
        [
         70,
         "Peter Paul",
         "2011-04-01",
         "600",
         5000,
         "Peter",
         "Paul",
         "2011-04-01",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "First_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Joining_Year",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Joining_Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Joining_Day",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######  Combine multiple split\n",
    "\n",
    "df4 = empDF.withColumn('First_Name',split(empDF['Name'],' ').getItem(0)).withColumn('Last_Name',split(empDF['Name'],' ').getItem(1)).withColumn('Joining_Year',split(empDF['doj'],' ').getItem(0)).withColumn('Joining_Month',split(empDF['doj'],' ').getItem(1)).withColumn('Joining_Day',split(empDF['doj'],' ').getItem(2))\n",
    "\n",
    "\n",
    "\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506aea45-da14-43c4-8cd4-251a874ae094",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>Name</th><th>doj</th><th>employee_dept_id</th><th>salary</th><th>First_Name</th><th>Last_Name</th><th>Joining_Year</th><th>Joining_Month</th><th>Joining_Day</th></tr></thead><tbody><tr><td>10</td><td>Michael Robinson</td><td>1999-06-01</td><td>100</td><td>2000</td><td>Michael</td><td>Robinson</td><td>1999-06-01</td><td>null</td><td>null</td></tr><tr><td>20</td><td>James Wood</td><td>2003-01-01</td><td>200</td><td>8000</td><td>James</td><td>Wood</td><td>2003-01-01</td><td>null</td><td>null</td></tr><tr><td>30</td><td>Chris Andrews</td><td>2005-04-01</td><td>100</td><td>6000</td><td>Chris</td><td>Andrews</td><td>2005-04-01</td><td>null</td><td>null</td></tr><tr><td>40</td><td>Mark Bond</td><td>2008-10-01</td><td>100</td><td>7000</td><td>Mark</td><td>Bond</td><td>2008-10-01</td><td>null</td><td>null</td></tr><tr><td>50</td><td>Steve Watson</td><td>1996-02-01</td><td>400</td><td>1000</td><td>Steve</td><td>Watson</td><td>1996-02-01</td><td>null</td><td>null</td></tr><tr><td>60</td><td>Mathews Simon</td><td>1998-11-01</td><td>500</td><td>5000</td><td>Mathews</td><td>Simon</td><td>1998-11-01</td><td>null</td><td>null</td></tr><tr><td>70</td><td>Peter Paul</td><td>2011-04-01</td><td>600</td><td>5000</td><td>Peter</td><td>Paul</td><td>2011-04-01</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Michael Robinson",
         "1999-06-01",
         "100",
         2000,
         "Michael",
         "Robinson",
         "1999-06-01",
         null,
         null
        ],
        [
         20,
         "James Wood",
         "2003-01-01",
         "200",
         8000,
         "James",
         "Wood",
         "2003-01-01",
         null,
         null
        ],
        [
         30,
         "Chris Andrews",
         "2005-04-01",
         "100",
         6000,
         "Chris",
         "Andrews",
         "2005-04-01",
         null,
         null
        ],
        [
         40,
         "Mark Bond",
         "2008-10-01",
         "100",
         7000,
         "Mark",
         "Bond",
         "2008-10-01",
         null,
         null
        ],
        [
         50,
         "Steve Watson",
         "1996-02-01",
         "400",
         1000,
         "Steve",
         "Watson",
         "1996-02-01",
         null,
         null
        ],
        [
         60,
         "Mathews Simon",
         "1998-11-01",
         "500",
         5000,
         "Mathews",
         "Simon",
         "1998-11-01",
         null,
         null
        ],
        [
         70,
         "Peter Paul",
         "2011-04-01",
         "600",
         5000,
         "Peter",
         "Paul",
         "2011-04-01",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "First_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Joining_Year",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Joining_Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Joining_Day",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### split and drop splitted columns\n",
    "df5 = empDF.withColumn('First_Name',split(empDF['Name'],' ').getItem(0)).withColumn('Last_Name',split(empDF['Name'],' ').getItem(1)).withColumn('Joining_Year',split(empDF['doj'],'-').getItem(0)).withColumn('Joining_Month',split(empDF['doj'],'-').getItem(1)).withColumn('Joining_Day',split(empDF['doj'],'-').getItem(2)).drop(empDF['Name']).drop(empDF['doj'])\n",
    "\n",
    "\n",
    "\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f65c81-bafd-43e7-b2fc-e524d8945272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f6b9593-aa15-4f44-b2dc-075e2c03b7b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8429c323-6868-4c62-9267-954a6426a244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################      Arrays_zip            ###########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d387758-3dc6-415d-a02c-3f1126621915",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##  In Apache Spark, the arrays_zip function is used to merge multiple arrays into a single array of structures (structs). Each element of the resulting array is a struct that contains the corresponding elements from the input arrays. This function is useful when you need to combine arrays element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644d8512-1ff5-4316-9853-b020ebff4fd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Score_1</th><th>Score_2</th></tr></thead><tbody><tr><td>Jhon</td><td>4</td><td>1</td></tr><tr><td>Jhon</td><td>6</td><td>2</td></tr><tr><td>David</td><td>7</td><td>3</td></tr><tr><td>Mike</td><td>3</td><td>4</td></tr><tr><td>David</td><td>5</td><td>2</td></tr><tr><td>Jhon</td><td>7</td><td>3</td></tr><tr><td>Jhon</td><td>9</td><td>7</td></tr><tr><td>David</td><td>1</td><td>8</td></tr><tr><td>David</td><td>4</td><td>9</td></tr><tr><td>David</td><td>7</td><td>4</td></tr><tr><td>Mike</td><td>8</td><td>5</td></tr><tr><td>Mike</td><td>5</td><td>2</td></tr><tr><td>Mike</td><td>3</td><td>8</td></tr><tr><td>Jhon</td><td>2</td><td>7</td></tr><tr><td>David</td><td>1</td><td>9</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jhon",
         4,
         1
        ],
        [
         "Jhon",
         6,
         2
        ],
        [
         "David",
         7,
         3
        ],
        [
         "Mike",
         3,
         4
        ],
        [
         "David",
         5,
         2
        ],
        [
         "Jhon",
         7,
         3
        ],
        [
         "Jhon",
         9,
         7
        ],
        [
         "David",
         1,
         8
        ],
        [
         "David",
         4,
         9
        ],
        [
         "David",
         7,
         4
        ],
        [
         "Mike",
         8,
         5
        ],
        [
         "Mike",
         5,
         2
        ],
        [
         "Mike",
         3,
         8
        ],
        [
         "Jhon",
         2,
         7
        ],
        [
         "David",
         1,
         9
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Score_1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Score_2",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### create a sample dataframe\n",
    "\n",
    "array_data = [\n",
    "    (\"Jhon\",4,1),\n",
    "    (\"Jhon\",6,2),\n",
    "    (\"David\",7,3),\n",
    "    (\"Mike\",3,4),\n",
    "    (\"David\",5,2),\n",
    "    (\"Jhon\",7,3),\n",
    "    (\"Jhon\",9,7),\n",
    "    (\"David\",1,8),\n",
    "    (\"David\",4,9),\n",
    "    (\"David\",7,4),\n",
    "    (\"Mike\",8,5),\n",
    "    (\"Mike\",5,2),\n",
    "    (\"Mike\",3,8),\n",
    "    (\"Jhon\",2,7),\n",
    "    (\"David\",1,9),\n",
    "]\n",
    "\n",
    "array_schema = ['Name','Score_1','Score_2']\n",
    "arrayDF = spark.createDataFrame(data=array_data,schema=array_schema)\n",
    "display(arrayDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ed5292-fb08-42cd-9658-698b93795c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Array_Score_1</th><th>Array_Score_2</th></tr></thead><tbody><tr><td>Jhon</td><td>List(4, 6, 7, 9, 2)</td><td>List(1, 2, 3, 7, 7)</td></tr><tr><td>David</td><td>List(7, 5, 1, 4, 7, 1)</td><td>List(3, 2, 8, 9, 4, 9)</td></tr><tr><td>Mike</td><td>List(3, 8, 5, 3)</td><td>List(4, 5, 2, 8)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jhon",
         [
          4,
          6,
          7,
          9,
          2
         ],
         [
          1,
          2,
          3,
          7,
          7
         ]
        ],
        [
         "David",
         [
          7,
          5,
          1,
          4,
          7,
          1
         ],
         [
          3,
          2,
          8,
          9,
          4,
          9
         ]
        ],
        [
         "Mike",
         [
          3,
          8,
          5,
          3
         ],
         [
          4,
          5,
          2,
          8
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Array_Score_1",
         "type": "{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":false}"
        },
        {
         "metadata": "{}",
         "name": "Array_Score_2",
         "type": "{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Array_Score_1: array (nullable = false)\n |    |-- element: long (containsNull = false)\n |-- Array_Score_2: array (nullable = false)\n |    |-- element: long (containsNull = false)\n\n"
     ]
    }
   ],
   "source": [
    "####  Convert sample dataframe into array dataframe\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "masterDF = arrayDF.groupby(\"Name\").agg(F.collect_list('Score_1').alias('Array_Score_1'),F.collect_list('Score_2').alias('Array_Score_2'))\n",
    "display(masterDF)\n",
    "masterDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad03b87-a25e-4843-a970-55937e90c78f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------------------------------------+\n|Name |Array_Score_1     |Array_Score_2     |Zipped_value                                    |\n+-----+------------------+------------------+------------------------------------------------+\n|Jhon |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[{4, 1}, {6, 2}, {7, 3}, {9, 7}, {2, 7}]        |\n|David|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[{7, 3}, {5, 2}, {1, 8}, {4, 9}, {7, 4}, {1, 9}]|\n|Mike |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[{3, 4}, {8, 5}, {5, 2}, {3, 8}]                |\n+-----+------------------+------------------+------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#####  Apply arrays_zip function on Array DF\n",
    "arr_zip_df = masterDF.withColumn('Zipped_value',F.arrays_zip('Array_Score_1','Array_Score_2'))\n",
    "arr_zip_df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354a66ef-2776-4003-a2f4-350fed8acf5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Department: string (nullable = true)\n |-- Employee: array (nullable = true)\n |    |-- element: map (containsNull = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Department</th><th>Employee</th></tr></thead><tbody><tr><td>Sales_dept</td><td>List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33), Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40), Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45), Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30), Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32))</td></tr><tr><td>HR_dept</td><td>List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31), Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26), Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49), Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52), Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sales_dept",
         [
          {
           "Age": "33",
           "emp_name": "Jhon",
           "salary": "1000",
           "yrs_of_service": "10"
          },
          {
           "Age": "40",
           "emp_name": "David",
           "salary": "2000",
           "yrs_of_service": "15"
          },
          {
           "Age": "45",
           "emp_name": "Nancy",
           "salary": "8000",
           "yrs_of_service": "20"
          },
          {
           "Age": "30",
           "emp_name": "Mike",
           "salary": "3000",
           "yrs_of_service": "6"
          },
          {
           "Age": "32",
           "emp_name": "Rosy",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ]
        ],
        [
         "HR_dept",
         [
          {
           "Age": "31",
           "emp_name": "Edwin",
           "salary": "6000",
           "yrs_of_service": "8"
          },
          {
           "Age": "26",
           "emp_name": "Tomas",
           "salary": "3000",
           "yrs_of_service": "4"
          },
          {
           "Age": "49",
           "emp_name": "Sarah",
           "salary": "12000",
           "yrs_of_service": "22"
          },
          {
           "Age": "52",
           "emp_name": "Stella",
           "salary": "15000",
           "yrs_of_service": "25"
          },
          {
           "Age": "27",
           "emp_name": "Kevin",
           "salary": "4000",
           "yrs_of_service": "5"
          }
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Employee",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true},\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Practical Use Case to flatten data using arrays_sip and explode \n",
    "\n",
    "empDF = [\n",
    "    ('Sales_dept',[{'emp_name':'Jhon','salary':'1000','yrs_of_service':'10','Age':'33'},\n",
    "                   {'emp_name':'David','salary':'2000','yrs_of_service':'15','Age':'40'},\n",
    "                   {'emp_name':'Nancy','salary':'8000','yrs_of_service':'20','Age':'45'},\n",
    "                   {'emp_name':'Mike','salary':'3000','yrs_of_service':'6','Age':'30'},\n",
    "                   {'emp_name':'Rosy','salary':'6000','yrs_of_service':'8','Age':'32'}]),\n",
    "    ('HR_dept',[{'emp_name':'Edwin','salary':'6000','yrs_of_service':'8','Age':'31'},\n",
    "                {'emp_name':'Tomas','salary':'3000','yrs_of_service':'4','Age':'26'},\n",
    "                {'emp_name':'Sarah','salary':'12000','yrs_of_service':'22','Age':'49'},\n",
    "                {'emp_name':'Stella','salary':'15000','yrs_of_service':'25','Age':'52'},\n",
    "                {'emp_name':'Kevin','salary':'4000','yrs_of_service':'5','Age':'27'}])\n",
    "]\n",
    "\n",
    "df_brand = spark.createDataFrame(data=empDF,schema=['Department','Employee'])\n",
    "df_brand.printSchema()\n",
    "display(df_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893ed262-9cff-409d-9c01-dae4141400fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Department</th><th>Employee</th><th>Zip</th></tr></thead><tbody><tr><td>Sales_dept</td><td>List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33), Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40), Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45), Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30), Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32))</td><td>List(List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33)), List(Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40)), List(Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45)), List(Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30)), List(Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32)))</td></tr><tr><td>HR_dept</td><td>List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31), Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26), Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49), Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52), Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27))</td><td>List(List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31)), List(Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26)), List(Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49)), List(Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52)), List(Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27)))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sales_dept",
         [
          {
           "Age": "33",
           "emp_name": "Jhon",
           "salary": "1000",
           "yrs_of_service": "10"
          },
          {
           "Age": "40",
           "emp_name": "David",
           "salary": "2000",
           "yrs_of_service": "15"
          },
          {
           "Age": "45",
           "emp_name": "Nancy",
           "salary": "8000",
           "yrs_of_service": "20"
          },
          {
           "Age": "30",
           "emp_name": "Mike",
           "salary": "3000",
           "yrs_of_service": "6"
          },
          {
           "Age": "32",
           "emp_name": "Rosy",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ],
         [
          [
           {
            "Age": "33",
            "emp_name": "Jhon",
            "salary": "1000",
            "yrs_of_service": "10"
           }
          ],
          [
           {
            "Age": "40",
            "emp_name": "David",
            "salary": "2000",
            "yrs_of_service": "15"
           }
          ],
          [
           {
            "Age": "45",
            "emp_name": "Nancy",
            "salary": "8000",
            "yrs_of_service": "20"
           }
          ],
          [
           {
            "Age": "30",
            "emp_name": "Mike",
            "salary": "3000",
            "yrs_of_service": "6"
           }
          ],
          [
           {
            "Age": "32",
            "emp_name": "Rosy",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ]
         ]
        ],
        [
         "HR_dept",
         [
          {
           "Age": "31",
           "emp_name": "Edwin",
           "salary": "6000",
           "yrs_of_service": "8"
          },
          {
           "Age": "26",
           "emp_name": "Tomas",
           "salary": "3000",
           "yrs_of_service": "4"
          },
          {
           "Age": "49",
           "emp_name": "Sarah",
           "salary": "12000",
           "yrs_of_service": "22"
          },
          {
           "Age": "52",
           "emp_name": "Stella",
           "salary": "15000",
           "yrs_of_service": "25"
          },
          {
           "Age": "27",
           "emp_name": "Kevin",
           "salary": "4000",
           "yrs_of_service": "5"
          }
         ],
         [
          [
           {
            "Age": "31",
            "emp_name": "Edwin",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ],
          [
           {
            "Age": "26",
            "emp_name": "Tomas",
            "salary": "3000",
            "yrs_of_service": "4"
           }
          ],
          [
           {
            "Age": "49",
            "emp_name": "Sarah",
            "salary": "12000",
            "yrs_of_service": "22"
           }
          ],
          [
           {
            "Age": "52",
            "emp_name": "Stella",
            "salary": "15000",
            "yrs_of_service": "25"
           }
          ],
          [
           {
            "Age": "27",
            "emp_name": "Kevin",
            "salary": "4000",
            "yrs_of_service": "5"
           }
          ]
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Employee",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true},\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "Zip",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"Employee\",\"type\":{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true},\"nullable\":true,\"metadata\":{}}]},\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## apply arrays_zip \n",
    "df_brandZip = df_brand.withColumn('Zip',F.arrays_zip(df_brand['Employee']))\n",
    "display(df_brandZip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f14e527-7fe2-4d9a-93df-3d78dd0c73a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Department</th><th>Employee</th><th>Zip</th><th>Explode</th></tr></thead><tbody><tr><td>Sales_dept</td><td>List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33), Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40), Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45), Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30), Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32))</td><td>List(List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33)), List(Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40)), List(Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45)), List(Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30)), List(Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32)))</td><td>List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33))</td></tr><tr><td>Sales_dept</td><td>List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33), Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40), Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45), Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30), Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32))</td><td>List(List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33)), List(Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40)), List(Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45)), List(Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30)), List(Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32)))</td><td>List(Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40))</td></tr><tr><td>Sales_dept</td><td>List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33), Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40), Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45), Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30), Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32))</td><td>List(List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33)), List(Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40)), List(Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45)), List(Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30)), List(Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32)))</td><td>List(Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45))</td></tr><tr><td>Sales_dept</td><td>List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33), Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40), Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45), Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30), Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32))</td><td>List(List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33)), List(Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40)), List(Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45)), List(Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30)), List(Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32)))</td><td>List(Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30))</td></tr><tr><td>Sales_dept</td><td>List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33), Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40), Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45), Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30), Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32))</td><td>List(List(Map(emp_name -> Jhon, yrs_of_service -> 10, salary -> 1000, Age -> 33)), List(Map(emp_name -> David, yrs_of_service -> 15, salary -> 2000, Age -> 40)), List(Map(emp_name -> Nancy, yrs_of_service -> 20, salary -> 8000, Age -> 45)), List(Map(emp_name -> Mike, yrs_of_service -> 6, salary -> 3000, Age -> 30)), List(Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32)))</td><td>List(Map(emp_name -> Rosy, yrs_of_service -> 8, salary -> 6000, Age -> 32))</td></tr><tr><td>HR_dept</td><td>List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31), Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26), Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49), Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52), Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27))</td><td>List(List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31)), List(Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26)), List(Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49)), List(Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52)), List(Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27)))</td><td>List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31))</td></tr><tr><td>HR_dept</td><td>List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31), Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26), Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49), Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52), Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27))</td><td>List(List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31)), List(Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26)), List(Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49)), List(Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52)), List(Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27)))</td><td>List(Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26))</td></tr><tr><td>HR_dept</td><td>List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31), Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26), Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49), Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52), Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27))</td><td>List(List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31)), List(Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26)), List(Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49)), List(Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52)), List(Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27)))</td><td>List(Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49))</td></tr><tr><td>HR_dept</td><td>List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31), Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26), Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49), Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52), Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27))</td><td>List(List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31)), List(Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26)), List(Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49)), List(Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52)), List(Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27)))</td><td>List(Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52))</td></tr><tr><td>HR_dept</td><td>List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31), Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26), Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49), Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52), Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27))</td><td>List(List(Map(emp_name -> Edwin, yrs_of_service -> 8, salary -> 6000, Age -> 31)), List(Map(emp_name -> Tomas, yrs_of_service -> 4, salary -> 3000, Age -> 26)), List(Map(emp_name -> Sarah, yrs_of_service -> 22, salary -> 12000, Age -> 49)), List(Map(emp_name -> Stella, yrs_of_service -> 25, salary -> 15000, Age -> 52)), List(Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27)))</td><td>List(Map(emp_name -> Kevin, yrs_of_service -> 5, salary -> 4000, Age -> 27))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sales_dept",
         [
          {
           "Age": "33",
           "emp_name": "Jhon",
           "salary": "1000",
           "yrs_of_service": "10"
          },
          {
           "Age": "40",
           "emp_name": "David",
           "salary": "2000",
           "yrs_of_service": "15"
          },
          {
           "Age": "45",
           "emp_name": "Nancy",
           "salary": "8000",
           "yrs_of_service": "20"
          },
          {
           "Age": "30",
           "emp_name": "Mike",
           "salary": "3000",
           "yrs_of_service": "6"
          },
          {
           "Age": "32",
           "emp_name": "Rosy",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ],
         [
          [
           {
            "Age": "33",
            "emp_name": "Jhon",
            "salary": "1000",
            "yrs_of_service": "10"
           }
          ],
          [
           {
            "Age": "40",
            "emp_name": "David",
            "salary": "2000",
            "yrs_of_service": "15"
           }
          ],
          [
           {
            "Age": "45",
            "emp_name": "Nancy",
            "salary": "8000",
            "yrs_of_service": "20"
           }
          ],
          [
           {
            "Age": "30",
            "emp_name": "Mike",
            "salary": "3000",
            "yrs_of_service": "6"
           }
          ],
          [
           {
            "Age": "32",
            "emp_name": "Rosy",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ]
         ],
         [
          {
           "Age": "33",
           "emp_name": "Jhon",
           "salary": "1000",
           "yrs_of_service": "10"
          }
         ]
        ],
        [
         "Sales_dept",
         [
          {
           "Age": "33",
           "emp_name": "Jhon",
           "salary": "1000",
           "yrs_of_service": "10"
          },
          {
           "Age": "40",
           "emp_name": "David",
           "salary": "2000",
           "yrs_of_service": "15"
          },
          {
           "Age": "45",
           "emp_name": "Nancy",
           "salary": "8000",
           "yrs_of_service": "20"
          },
          {
           "Age": "30",
           "emp_name": "Mike",
           "salary": "3000",
           "yrs_of_service": "6"
          },
          {
           "Age": "32",
           "emp_name": "Rosy",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ],
         [
          [
           {
            "Age": "33",
            "emp_name": "Jhon",
            "salary": "1000",
            "yrs_of_service": "10"
           }
          ],
          [
           {
            "Age": "40",
            "emp_name": "David",
            "salary": "2000",
            "yrs_of_service": "15"
           }
          ],
          [
           {
            "Age": "45",
            "emp_name": "Nancy",
            "salary": "8000",
            "yrs_of_service": "20"
           }
          ],
          [
           {
            "Age": "30",
            "emp_name": "Mike",
            "salary": "3000",
            "yrs_of_service": "6"
           }
          ],
          [
           {
            "Age": "32",
            "emp_name": "Rosy",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ]
         ],
         [
          {
           "Age": "40",
           "emp_name": "David",
           "salary": "2000",
           "yrs_of_service": "15"
          }
         ]
        ],
        [
         "Sales_dept",
         [
          {
           "Age": "33",
           "emp_name": "Jhon",
           "salary": "1000",
           "yrs_of_service": "10"
          },
          {
           "Age": "40",
           "emp_name": "David",
           "salary": "2000",
           "yrs_of_service": "15"
          },
          {
           "Age": "45",
           "emp_name": "Nancy",
           "salary": "8000",
           "yrs_of_service": "20"
          },
          {
           "Age": "30",
           "emp_name": "Mike",
           "salary": "3000",
           "yrs_of_service": "6"
          },
          {
           "Age": "32",
           "emp_name": "Rosy",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ],
         [
          [
           {
            "Age": "33",
            "emp_name": "Jhon",
            "salary": "1000",
            "yrs_of_service": "10"
           }
          ],
          [
           {
            "Age": "40",
            "emp_name": "David",
            "salary": "2000",
            "yrs_of_service": "15"
           }
          ],
          [
           {
            "Age": "45",
            "emp_name": "Nancy",
            "salary": "8000",
            "yrs_of_service": "20"
           }
          ],
          [
           {
            "Age": "30",
            "emp_name": "Mike",
            "salary": "3000",
            "yrs_of_service": "6"
           }
          ],
          [
           {
            "Age": "32",
            "emp_name": "Rosy",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ]
         ],
         [
          {
           "Age": "45",
           "emp_name": "Nancy",
           "salary": "8000",
           "yrs_of_service": "20"
          }
         ]
        ],
        [
         "Sales_dept",
         [
          {
           "Age": "33",
           "emp_name": "Jhon",
           "salary": "1000",
           "yrs_of_service": "10"
          },
          {
           "Age": "40",
           "emp_name": "David",
           "salary": "2000",
           "yrs_of_service": "15"
          },
          {
           "Age": "45",
           "emp_name": "Nancy",
           "salary": "8000",
           "yrs_of_service": "20"
          },
          {
           "Age": "30",
           "emp_name": "Mike",
           "salary": "3000",
           "yrs_of_service": "6"
          },
          {
           "Age": "32",
           "emp_name": "Rosy",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ],
         [
          [
           {
            "Age": "33",
            "emp_name": "Jhon",
            "salary": "1000",
            "yrs_of_service": "10"
           }
          ],
          [
           {
            "Age": "40",
            "emp_name": "David",
            "salary": "2000",
            "yrs_of_service": "15"
           }
          ],
          [
           {
            "Age": "45",
            "emp_name": "Nancy",
            "salary": "8000",
            "yrs_of_service": "20"
           }
          ],
          [
           {
            "Age": "30",
            "emp_name": "Mike",
            "salary": "3000",
            "yrs_of_service": "6"
           }
          ],
          [
           {
            "Age": "32",
            "emp_name": "Rosy",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ]
         ],
         [
          {
           "Age": "30",
           "emp_name": "Mike",
           "salary": "3000",
           "yrs_of_service": "6"
          }
         ]
        ],
        [
         "Sales_dept",
         [
          {
           "Age": "33",
           "emp_name": "Jhon",
           "salary": "1000",
           "yrs_of_service": "10"
          },
          {
           "Age": "40",
           "emp_name": "David",
           "salary": "2000",
           "yrs_of_service": "15"
          },
          {
           "Age": "45",
           "emp_name": "Nancy",
           "salary": "8000",
           "yrs_of_service": "20"
          },
          {
           "Age": "30",
           "emp_name": "Mike",
           "salary": "3000",
           "yrs_of_service": "6"
          },
          {
           "Age": "32",
           "emp_name": "Rosy",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ],
         [
          [
           {
            "Age": "33",
            "emp_name": "Jhon",
            "salary": "1000",
            "yrs_of_service": "10"
           }
          ],
          [
           {
            "Age": "40",
            "emp_name": "David",
            "salary": "2000",
            "yrs_of_service": "15"
           }
          ],
          [
           {
            "Age": "45",
            "emp_name": "Nancy",
            "salary": "8000",
            "yrs_of_service": "20"
           }
          ],
          [
           {
            "Age": "30",
            "emp_name": "Mike",
            "salary": "3000",
            "yrs_of_service": "6"
           }
          ],
          [
           {
            "Age": "32",
            "emp_name": "Rosy",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ]
         ],
         [
          {
           "Age": "32",
           "emp_name": "Rosy",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ]
        ],
        [
         "HR_dept",
         [
          {
           "Age": "31",
           "emp_name": "Edwin",
           "salary": "6000",
           "yrs_of_service": "8"
          },
          {
           "Age": "26",
           "emp_name": "Tomas",
           "salary": "3000",
           "yrs_of_service": "4"
          },
          {
           "Age": "49",
           "emp_name": "Sarah",
           "salary": "12000",
           "yrs_of_service": "22"
          },
          {
           "Age": "52",
           "emp_name": "Stella",
           "salary": "15000",
           "yrs_of_service": "25"
          },
          {
           "Age": "27",
           "emp_name": "Kevin",
           "salary": "4000",
           "yrs_of_service": "5"
          }
         ],
         [
          [
           {
            "Age": "31",
            "emp_name": "Edwin",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ],
          [
           {
            "Age": "26",
            "emp_name": "Tomas",
            "salary": "3000",
            "yrs_of_service": "4"
           }
          ],
          [
           {
            "Age": "49",
            "emp_name": "Sarah",
            "salary": "12000",
            "yrs_of_service": "22"
           }
          ],
          [
           {
            "Age": "52",
            "emp_name": "Stella",
            "salary": "15000",
            "yrs_of_service": "25"
           }
          ],
          [
           {
            "Age": "27",
            "emp_name": "Kevin",
            "salary": "4000",
            "yrs_of_service": "5"
           }
          ]
         ],
         [
          {
           "Age": "31",
           "emp_name": "Edwin",
           "salary": "6000",
           "yrs_of_service": "8"
          }
         ]
        ],
        [
         "HR_dept",
         [
          {
           "Age": "31",
           "emp_name": "Edwin",
           "salary": "6000",
           "yrs_of_service": "8"
          },
          {
           "Age": "26",
           "emp_name": "Tomas",
           "salary": "3000",
           "yrs_of_service": "4"
          },
          {
           "Age": "49",
           "emp_name": "Sarah",
           "salary": "12000",
           "yrs_of_service": "22"
          },
          {
           "Age": "52",
           "emp_name": "Stella",
           "salary": "15000",
           "yrs_of_service": "25"
          },
          {
           "Age": "27",
           "emp_name": "Kevin",
           "salary": "4000",
           "yrs_of_service": "5"
          }
         ],
         [
          [
           {
            "Age": "31",
            "emp_name": "Edwin",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ],
          [
           {
            "Age": "26",
            "emp_name": "Tomas",
            "salary": "3000",
            "yrs_of_service": "4"
           }
          ],
          [
           {
            "Age": "49",
            "emp_name": "Sarah",
            "salary": "12000",
            "yrs_of_service": "22"
           }
          ],
          [
           {
            "Age": "52",
            "emp_name": "Stella",
            "salary": "15000",
            "yrs_of_service": "25"
           }
          ],
          [
           {
            "Age": "27",
            "emp_name": "Kevin",
            "salary": "4000",
            "yrs_of_service": "5"
           }
          ]
         ],
         [
          {
           "Age": "26",
           "emp_name": "Tomas",
           "salary": "3000",
           "yrs_of_service": "4"
          }
         ]
        ],
        [
         "HR_dept",
         [
          {
           "Age": "31",
           "emp_name": "Edwin",
           "salary": "6000",
           "yrs_of_service": "8"
          },
          {
           "Age": "26",
           "emp_name": "Tomas",
           "salary": "3000",
           "yrs_of_service": "4"
          },
          {
           "Age": "49",
           "emp_name": "Sarah",
           "salary": "12000",
           "yrs_of_service": "22"
          },
          {
           "Age": "52",
           "emp_name": "Stella",
           "salary": "15000",
           "yrs_of_service": "25"
          },
          {
           "Age": "27",
           "emp_name": "Kevin",
           "salary": "4000",
           "yrs_of_service": "5"
          }
         ],
         [
          [
           {
            "Age": "31",
            "emp_name": "Edwin",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ],
          [
           {
            "Age": "26",
            "emp_name": "Tomas",
            "salary": "3000",
            "yrs_of_service": "4"
           }
          ],
          [
           {
            "Age": "49",
            "emp_name": "Sarah",
            "salary": "12000",
            "yrs_of_service": "22"
           }
          ],
          [
           {
            "Age": "52",
            "emp_name": "Stella",
            "salary": "15000",
            "yrs_of_service": "25"
           }
          ],
          [
           {
            "Age": "27",
            "emp_name": "Kevin",
            "salary": "4000",
            "yrs_of_service": "5"
           }
          ]
         ],
         [
          {
           "Age": "49",
           "emp_name": "Sarah",
           "salary": "12000",
           "yrs_of_service": "22"
          }
         ]
        ],
        [
         "HR_dept",
         [
          {
           "Age": "31",
           "emp_name": "Edwin",
           "salary": "6000",
           "yrs_of_service": "8"
          },
          {
           "Age": "26",
           "emp_name": "Tomas",
           "salary": "3000",
           "yrs_of_service": "4"
          },
          {
           "Age": "49",
           "emp_name": "Sarah",
           "salary": "12000",
           "yrs_of_service": "22"
          },
          {
           "Age": "52",
           "emp_name": "Stella",
           "salary": "15000",
           "yrs_of_service": "25"
          },
          {
           "Age": "27",
           "emp_name": "Kevin",
           "salary": "4000",
           "yrs_of_service": "5"
          }
         ],
         [
          [
           {
            "Age": "31",
            "emp_name": "Edwin",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ],
          [
           {
            "Age": "26",
            "emp_name": "Tomas",
            "salary": "3000",
            "yrs_of_service": "4"
           }
          ],
          [
           {
            "Age": "49",
            "emp_name": "Sarah",
            "salary": "12000",
            "yrs_of_service": "22"
           }
          ],
          [
           {
            "Age": "52",
            "emp_name": "Stella",
            "salary": "15000",
            "yrs_of_service": "25"
           }
          ],
          [
           {
            "Age": "27",
            "emp_name": "Kevin",
            "salary": "4000",
            "yrs_of_service": "5"
           }
          ]
         ],
         [
          {
           "Age": "52",
           "emp_name": "Stella",
           "salary": "15000",
           "yrs_of_service": "25"
          }
         ]
        ],
        [
         "HR_dept",
         [
          {
           "Age": "31",
           "emp_name": "Edwin",
           "salary": "6000",
           "yrs_of_service": "8"
          },
          {
           "Age": "26",
           "emp_name": "Tomas",
           "salary": "3000",
           "yrs_of_service": "4"
          },
          {
           "Age": "49",
           "emp_name": "Sarah",
           "salary": "12000",
           "yrs_of_service": "22"
          },
          {
           "Age": "52",
           "emp_name": "Stella",
           "salary": "15000",
           "yrs_of_service": "25"
          },
          {
           "Age": "27",
           "emp_name": "Kevin",
           "salary": "4000",
           "yrs_of_service": "5"
          }
         ],
         [
          [
           {
            "Age": "31",
            "emp_name": "Edwin",
            "salary": "6000",
            "yrs_of_service": "8"
           }
          ],
          [
           {
            "Age": "26",
            "emp_name": "Tomas",
            "salary": "3000",
            "yrs_of_service": "4"
           }
          ],
          [
           {
            "Age": "49",
            "emp_name": "Sarah",
            "salary": "12000",
            "yrs_of_service": "22"
           }
          ],
          [
           {
            "Age": "52",
            "emp_name": "Stella",
            "salary": "15000",
            "yrs_of_service": "25"
           }
          ],
          [
           {
            "Age": "27",
            "emp_name": "Kevin",
            "salary": "4000",
            "yrs_of_service": "5"
           }
          ]
         ],
         [
          {
           "Age": "27",
           "emp_name": "Kevin",
           "salary": "4000",
           "yrs_of_service": "5"
          }
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Employee",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true},\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "Zip",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"Employee\",\"type\":{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true},\"nullable\":true,\"metadata\":{}}]},\"containsNull\":false}"
        },
        {
         "metadata": "{}",
         "name": "Explode",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"Employee\",\"type\":{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true},\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Apply explode \n",
    "df_brand_exp = df_brandZip.withColumn('Explode',F.explode(df_brandZip.Zip))\n",
    "display(df_brand_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229aeb8b-fa7c-4bfe-ac9f-bcb211d5b5fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Flatten fields from explode list\n",
    "\n",
    "df_brand_output = df_brand_exp.withColumn('employee_emp_name',df_brand_exp['Explode.Employee.emp_name']).withColumn('employee_yrs_of_service',df_brand_exp['Explode.Employee.yrs_of_service']).withColumn('employee_salary',df_brand_exp['Explode.Employee.saary']).withColumn('employee_Age',df_brand_exp['Explode.Employee.Age']).drop('Explode').drop('Zip').drop('Employee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8671cb43-675c-4f87-b2f7-dbe0a2f79c69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cecdd9c2-f294-4b79-a711-8b707c47e651",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Department</th><th>employee_emp_name</th><th>employee_yrs_of_service</th><th>employee_salary</th><th>employee_Age</th></tr></thead><tbody><tr><td>Sales_dept</td><td>Jhon</td><td>10</td><td>null</td><td>33</td></tr><tr><td>Sales_dept</td><td>David</td><td>15</td><td>null</td><td>40</td></tr><tr><td>Sales_dept</td><td>Nancy</td><td>20</td><td>null</td><td>45</td></tr><tr><td>Sales_dept</td><td>Mike</td><td>6</td><td>null</td><td>30</td></tr><tr><td>Sales_dept</td><td>Rosy</td><td>8</td><td>null</td><td>32</td></tr><tr><td>HR_dept</td><td>Edwin</td><td>8</td><td>null</td><td>31</td></tr><tr><td>HR_dept</td><td>Tomas</td><td>4</td><td>null</td><td>26</td></tr><tr><td>HR_dept</td><td>Sarah</td><td>22</td><td>null</td><td>49</td></tr><tr><td>HR_dept</td><td>Stella</td><td>25</td><td>null</td><td>52</td></tr><tr><td>HR_dept</td><td>Kevin</td><td>5</td><td>null</td><td>27</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sales_dept",
         "Jhon",
         "10",
         null,
         "33"
        ],
        [
         "Sales_dept",
         "David",
         "15",
         null,
         "40"
        ],
        [
         "Sales_dept",
         "Nancy",
         "20",
         null,
         "45"
        ],
        [
         "Sales_dept",
         "Mike",
         "6",
         null,
         "30"
        ],
        [
         "Sales_dept",
         "Rosy",
         "8",
         null,
         "32"
        ],
        [
         "HR_dept",
         "Edwin",
         "8",
         null,
         "31"
        ],
        [
         "HR_dept",
         "Tomas",
         "4",
         null,
         "26"
        ],
        [
         "HR_dept",
         "Sarah",
         "22",
         null,
         "49"
        ],
        [
         "HR_dept",
         "Stella",
         "25",
         null,
         "52"
        ],
        [
         "HR_dept",
         "Kevin",
         "5",
         null,
         "27"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_emp_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_yrs_of_service",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_salary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_Age",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_brand_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cef6d734-e37c-4e05-9cc9-a72ece8a6a3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e4d577d-95dd-4a77-ac4c-524e454c5a3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################      Array_intersects           ###########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4967920-610a-4c3b-9e2a-e12ff66691f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n| Name|           Array_1|           Array_2|\n+-----+------------------+------------------+\n| John|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|\n|David|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|\n| Mike|   [3, 9, 1, 6, 2]|   [1, 2, 3, 5, 8]|\n+-----+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "## Array_intersects : returns common elements across 2 arrays without any duplications \n",
    "\n",
    "empDF = [\n",
    "  ('John',[4,6,7,9,2],[1,2,3,7,7]),\n",
    "  ('David',[7,5,1,4,7,1],[3,2,8,9,4,9]),\n",
    "  ('Mike',[3,9,1,6,2],[1,2,3,5,8])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=empDF , schema=['Name','Array_1','Array_2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a540159a-de75-4440-9c0c-61e25bfafcd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+---------+\n| Name|           Array_1|           Array_2|Intersect|\n+-----+------------------+------------------+---------+\n| John|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|   [7, 2]|\n|David|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|      [4]|\n| Mike|   [3, 9, 1, 6, 2]|   [1, 2, 3, 5, 8]|[3, 1, 2]|\n+-----+------------------+------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "### apply array intersect \n",
    "from pyspark.sql import functions as F\n",
    "df_intersect = df.withColumn('Intersect',F.array_intersect(df['Array_1'],df['Array_2']))\n",
    "df_intersect.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34e92c6a-6514-4a02-a359-94fe1437f3ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "381cff83-a0ff-4f75-9935-ba00c3fa06b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " ################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################      Array_Except           ###########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecfaf5c0-ce4c-4dd0-92b2-d032e869c20b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## returns a list of elements which are present in first array but not in second array \n",
    "\n",
    "empDF = [\n",
    "  ('John',[4,6,7,9,2],[1,2,3,7,7]),\n",
    "  ('David',[7,5,1,4,7,1],[3,2,8,9,4,9]),\n",
    "  ('Mike',[3,9,1,6,2],[1,2,3,5,8])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=empDF , schema=['Name','Array_1','Array_2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a431ac7d-3fe2-40e4-bb42-249914140dc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+---------+\n| Name|           Array_1|           Array_2|Intersect|\n+-----+------------------+------------------+---------+\n| John|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|[4, 6, 9]|\n|David|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[7, 5, 1]|\n| Mike|   [3, 9, 1, 6, 2]|   [1, 2, 3, 5, 8]|   [9, 6]|\n+-----+------------------+------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_except = df.withColumn('Intersect',F.array_except(df['Array_1'],df['Array_2']))\n",
    "df_except.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96676095-3690-45a6-b9d6-2002c0ecf1b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f5596c-8d32-46b8-a655-208b1be61f89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################      Array_Sort         ###########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354777de-8ed4-494d-9aec-7acdf36f6ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n| Name|         Array_col|\n+-----+------------------+\n| John|   [4, 6, 7, 9, 2]|\n|David|[7, 5, 1, 4, 7, 1]|\n| Mike|   [3, 9, 1, 6, 2]|\n+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF = [\n",
    "  ('John',[4,6,7,9,2]),\n",
    "  ('David',[7,5,1,4,7,1]),\n",
    "  ('Mike',[3,9,1,6,2])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=empDF , schema=['Name','Array_col'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b281d86-c2ad-4114-9475-ecded38dffbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n| Name|         Array_col|            sorted|\n+-----+------------------+------------------+\n| John|   [4, 6, 7, 9, 2]|   [2, 4, 6, 7, 9]|\n|David|[7, 5, 1, 4, 7, 1]|[1, 1, 4, 5, 7, 7]|\n| Mike|   [3, 9, 1, 6, 2]|   [1, 2, 3, 6, 9]|\n+-----+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_sort = df.withColumn('sorted',F.array_sort(df['Array_col']))\n",
    "df_sort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5148898-f756-49a3-8ddc-d07120c876ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503e69a1-dd24-4db4-ade1-ef42c1859e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: 4"
     ]
    }
   ],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################     join        ############################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345f8865-51d8-419d-84aa-3db828fb1211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## join : Python function that returns a string by joining all the elements of an iterable data such as list,tuple, etc. , seperated by a string seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b665563-4083-4640-bb79-5dcab325f829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>Name</th><th>doj</th><th>employee_dept_id</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Michael Robinson</td><td>1999-06-01</td><td>100</td><td>2000</td></tr><tr><td>20</td><td>James Wood</td><td>2003-01-01</td><td>200</td><td>8000</td></tr><tr><td>30</td><td>Chris Andrews</td><td>2005-04-01</td><td>100</td><td>6000</td></tr><tr><td>40</td><td>Mark Bond</td><td>2008-10-01</td><td>100</td><td>7000</td></tr><tr><td>50</td><td>Steve Watson</td><td>1996-02-01</td><td>400</td><td>1000</td></tr><tr><td>60</td><td>Mathews Simon</td><td>1998-11-01</td><td>500</td><td>5000</td></tr><tr><td>70</td><td>Peter Paul</td><td>2011-04-01</td><td>600</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Michael Robinson",
         "1999-06-01",
         "100",
         2000
        ],
        [
         20,
         "James Wood",
         "2003-01-01",
         "200",
         8000
        ],
        [
         30,
         "Chris Andrews",
         "2005-04-01",
         "100",
         6000
        ],
        [
         40,
         "Mark Bond",
         "2008-10-01",
         "100",
         7000
        ],
        [
         50,
         "Steve Watson",
         "1996-02-01",
         "400",
         1000
        ],
        [
         60,
         "Mathews Simon",
         "1998-11-01",
         "500",
         5000
        ],
        [
         70,
         "Peter Paul",
         "2011-04-01",
         "600",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data= [\n",
    "    (10,'Michael Robinson',\"1999-06-01\",\"100\",2000),\n",
    "    (20,'James Wood',\"2003-01-01\",\"200\",8000),\n",
    "    (30,'Chris Andrews',\"2005-04-01\",\"100\",6000),\n",
    "    (40,'Mark Bond',\"2008-10-01\",\"100\",7000),\n",
    "    (50,'Steve Watson',\"1996-02-01\",\"400\",1000),\n",
    "    (60,'Mathews Simon',\"1998-11-01\",\"500\",5000),\n",
    "    (70,'Peter Paul',\"2011-04-01\",\"600\",5000),\n",
    "]\n",
    "\n",
    "employee_schema = ['employee_id','Name','doj','employee_dept_id','salary']\n",
    "empDF = spark.createDataFrame(data=employee_data,schema=employee_schema)\n",
    "display(empDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b962b0-8964-4e8b-9415-0b43bd459f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['employee_id', 'Name', 'doj', 'employee_dept_id', 'salary']\n"
     ]
    }
   ],
   "source": [
    "#### Create iterable data\n",
    "column_list =  empDF.columns\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344fd6c0-6312-4110-b552-3b4992dd04f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee_id,Name,doj,employee_dept_id,salary\n"
     ]
    }
   ],
   "source": [
    "###### join the strings \n",
    "joined_string = ','.join(column_list)\n",
    "print(joined_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b669cf8-0a15-4453-b0b0-4d509cc2139a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target.employee_id = sourceemployee_id AND Target.Name = sourceName AND Target.doj = sourcedoj AND Target.employee_dept_id = sourceemployee_dept_id AND Target.salary = sourcesalary\nTarget.employee_id = Source .employee_id,Target.Name = Source .Name,Target.doj = Source .doj,Target.employee_dept_id = Source .employee_dept_id,Target.salary = Source .salary\nsource.employee_id,source.Name,source.doj,source.employee_dept_id,source.salary\n"
     ]
    }
   ],
   "source": [
    "################  Few othee use cases  ############\n",
    "\n",
    "joinstring = \" AND \".join(list(map(lambda x:(\"Target.\"+ x + \" = source\"+x),column_list)))\n",
    "print(joinstring)\n",
    "\n",
    "\n",
    "updatestring = ','.join(list(map(lambda x:(\"Target.\"+x + \" = Source .\"+ x),column_list)))\n",
    "print(updatestring)\n",
    "\n",
    "sourceinsertstring = ','.join(list(map(lambda x: ('source.' + x ),column_list)))\n",
    "print(sourceinsertstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ea2e2d-1abf-4ab7-8ff5-fc2b8a12cdd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b6a2b22-17e0-4a4f-b8ad-a7463dfd1ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0a0321b-606e-4916-8b16-1d4e2ee65c20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "####################     PartionBy       ########################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c6b92f-7d1f-44dc-a8bb-ded9a6742e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## partitionBy = Function is used to write the dataframe ino disc partitioned by specific key(s)\n",
    "\n",
    "## syntax \n",
    "'''\n",
    "df.write.option('header',True).partitionBy('Year').mode('overwrite').csv(\"/FileStore/tables/folder-name\")\n",
    "\n",
    "df.write.option('header',True).partitionBy('Year','Sex').mode('overwrite').csv(\"/FileStore/tables/folder-name\")\n",
    "\n",
    "df.write.option('header',True).option('maxRecordsPerFile',4200).partitionBy('Year').mode('overwrite').csv(\"/FileStore/tables/folder-name\")\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a205d1e4-52a2-4858-b9cf-3a8ba2a75591",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_123636-1.jpg', name='Screenshot_2024_07_17_123636-1.jpg', size=113360, modificationTime=1721211103000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_123636.jpg', name='Screenshot_2024_07_17_123636.jpg', size=113360, modificationTime=1721210897000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_154750.jpg', name='Screenshot_2024_07_17_154750.jpg', size=118510, modificationTime=1721211558000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_154902.jpg', name='Screenshot_2024_07_17_154902.jpg', size=129421, modificationTime=1721211558000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_155307.jpg', name='Screenshot_2024_07_17_155307.jpg', size=130558, modificationTime=1721211864000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_155326.jpg', name='Screenshot_2024_07_17_155326.jpg', size=107170, modificationTime=1721211864000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_155841.jpg', name='Screenshot_2024_07_17_155841.jpg', size=16150, modificationTime=1721212135000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_221116.jpg', name='Screenshot_2024_07_17_221116.jpg', size=133666, modificationTime=1721234567000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_221318.jpg', name='Screenshot_2024_07_17_221318.jpg', size=58635, modificationTime=1721234617000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_222722.jpg', name='Screenshot_2024_07_17_222722.jpg', size=83013, modificationTime=1721235455000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_222751.jpg', name='Screenshot_2024_07_17_222751.jpg', size=107412, modificationTime=1721235494000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_222802.jpg', name='Screenshot_2024_07_17_222802.jpg', size=78527, modificationTime=1721235494000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_223117.jpg', name='Screenshot_2024_07_17_223117.jpg', size=53280, modificationTime=1721235684000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_223206.jpg', name='Screenshot_2024_07_17_223206.jpg', size=75230, modificationTime=1721235737000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_233542.jpg', name='Screenshot_2024_07_17_233542.jpg', size=109318, modificationTime=1721239552000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_233559.jpg', name='Screenshot_2024_07_17_233559.jpg', size=124337, modificationTime=1721239569000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_233625.jpg', name='Screenshot_2024_07_17_233625.jpg', size=156282, modificationTime=1721239593000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_233847.jpg', name='Screenshot_2024_07_17_233847.jpg', size=121098, modificationTime=1721239787000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_233859.jpg', name='Screenshot_2024_07_17_233859.jpg', size=99328, modificationTime=1721239786000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_17_233910.jpg', name='Screenshot_2024_07_17_233910.jpg', size=243225, modificationTime=1721239787000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_002851.jpg', name='Screenshot_2024_07_18_002851.jpg', size=130284, modificationTime=1721242860000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_003006.jpg', name='Screenshot_2024_07_18_003006.jpg', size=236942, modificationTime=1721242859000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_003023.jpg', name='Screenshot_2024_07_18_003023.jpg', size=104149, modificationTime=1721242859000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_003559.jpg', name='Screenshot_2024_07_18_003559.jpg', size=131671, modificationTime=1721243175000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_012217.jpg', name='Screenshot_2024_07_18_012217.jpg', size=229878, modificationTime=1721245949000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_012314.jpg', name='Screenshot_2024_07_18_012314.jpg', size=163815, modificationTime=1721246005000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_012416.jpg', name='Screenshot_2024_07_18_012416.jpg', size=121816, modificationTime=1721246067000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_012456.jpg', name='Screenshot_2024_07_18_012456.jpg', size=128081, modificationTime=1721246107000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_015330.jpg', name='Screenshot_2024_07_18_015330.jpg', size=159002, modificationTime=1721247845000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_015421.jpg', name='Screenshot_2024_07_18_015421.jpg', size=88738, modificationTime=1721247878000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_015944.jpg', name='Screenshot_2024_07_18_015944.jpg', size=205022, modificationTime=1721248195000),\n FileInfo(path='dbfs:/FileStore/tables/Screenshot_2024_07_18_020131.jpg', name='Screenshot_2024_07_18_020131.jpg', size=87139, modificationTime=1721248301000),\n FileInfo(path='dbfs:/FileStore/tables/baby_names-1.csv', name='baby_names-1.csv', size=7447879, modificationTime=1721198190000),\n FileInfo(path='dbfs:/FileStore/tables/baby_names-2.csv', name='baby_names-2.csv', size=7447879, modificationTime=1721198338000),\n FileInfo(path='dbfs:/FileStore/tables/baby_names-3.csv', name='baby_names-3.csv', size=7447879, modificationTime=1721236442000),\n FileInfo(path='dbfs:/FileStore/tables/baby_names.csv', name='baby_names.csv', size=7447879, modificationTime=1721197911000),\n FileInfo(path='dbfs:/FileStore/tables/corrupt.csv', name='corrupt.csv', size=267, modificationTime=1721212539000),\n FileInfo(path='dbfs:/FileStore/tables/corrupt.txt', name='corrupt.txt', size=0, modificationTime=1721212486000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19821b52-c942-4964-85c3-467bb0ae780e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+---+\n|year|   name| percent|sex|\n+----+-------+--------+---+\n|1880|   John|0.081541|boy|\n|1880|William|0.080511|boy|\n|1880|  James|0.050057|boy|\n|1880|Charles|0.045167|boy|\n|1880| George|0.043292|boy|\n|1880|  Frank| 0.02738|boy|\n|1880| Joseph|0.022229|boy|\n|1880| Thomas|0.021401|boy|\n|1880|  Henry|0.020641|boy|\n|1880| Robert|0.020404|boy|\n|1880| Edward|0.019965|boy|\n|1880|  Harry|0.018175|boy|\n|1880| Walter|0.014822|boy|\n|1880| Arthur|0.013504|boy|\n|1880|   Fred|0.013251|boy|\n|1880| Albert|0.012609|boy|\n|1880| Samuel|0.008648|boy|\n|1880|  David|0.007339|boy|\n|1880|  Louis|0.006993|boy|\n|1880|    Joe|0.006174|boy|\n+----+-------+--------+---+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('inferSchema',True).option('header',True).option('sep',\",\").load('/FileStore/tables')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec0d77c-37e5-4384-88c4-1109982bef30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n|Year|count|\n+----+-----+\n|1903|8000 |\n|1953|8000 |\n|1897|8000 |\n|1957|8000 |\n|1880|8000 |\n|1987|8000 |\n|1956|8000 |\n|1936|8000 |\n|1958|8000 |\n|1910|8000 |\n|1943|8000 |\n|1915|8000 |\n|1972|8000 |\n|1931|8000 |\n|1911|8000 |\n|1926|8000 |\n|1938|8000 |\n|1988|8000 |\n|1918|8000 |\n|1882|8000 |\n+----+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#### Distinct year list and count for each year\n",
    "df.groupBy(\"Year\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc5a7fa-1792-405b-aa10-59638968a236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-461411582351070>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m### Partition by one key column\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mYear\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/FileStore/tables\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o819.csv.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 12.0 failed 1 times, most recent failure: Lost task 7.0 in stage 12.0 (TID 118) (ip-10-172-195-128.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to dbfs:/FileStore/tables.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:905)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:515)\n",
       "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:113)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:929)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:929)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:406)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:370)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.rmi.RemoteException: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; nested exception is: \n",
       "\tcom.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendNonIdempotent(DbfsClient.scala:82)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.createUnbuffered(DatabricksFileSystemV1.scala:127)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.create(DatabricksFileSystemV1.scala:109)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:139)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
       "\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:135)\n",
       "\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:108)\n",
       "\tat org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.renewCurrentWriter(FileFormatDataWriter.scala:296)\n",
       "\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:373)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:495)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1774)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:502)\n",
       "\t... 26 more\n",
       "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.listObjectsV2(EnforcingDatabricksS3Client.scala:214)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$9(S3AFileSystem.java:2663)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2652)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listOneKeyWithPrefix(S3AFileSystem.java:6023)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getExistingFileSizeByListing(S3AFileSystem.java:5972)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1837)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1231)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1208)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1163)\n",
       "\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.create(HadoopFSBackend.scala:153)\n",
       "\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.create(RootFileSystemBackend.scala:264)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$10(FileSystemRequestHandler.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:637)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:542)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperationWithResultTags(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:534)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:502)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:79)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:472)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:472)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:365)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:637)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:534)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:502)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3424)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3346)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3335)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3335)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1444)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1444)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1444)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3635)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3573)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3561)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1193)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1181)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2741)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$5(FileFormatWriter.scala:372)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:336)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:369)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:249)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)\n",
       "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:207)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$3(commands.scala:132)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:130)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:129)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:144)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:955)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to dbfs:/FileStore/tables.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:905)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:515)\n",
       "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:113)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:929)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:929)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:406)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:370)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: java.rmi.RemoteException: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; nested exception is: \n",
       "\tcom.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendNonIdempotent(DbfsClient.scala:82)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.createUnbuffered(DatabricksFileSystemV1.scala:127)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.create(DatabricksFileSystemV1.scala:109)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:139)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
       "\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:135)\n",
       "\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:108)\n",
       "\tat org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.renewCurrentWriter(FileFormatDataWriter.scala:296)\n",
       "\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:373)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:495)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1774)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:502)\n",
       "\t... 26 more\n",
       "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.listObjectsV2(EnforcingDatabricksS3Client.scala:214)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$9(S3AFileSystem.java:2663)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2652)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listOneKeyWithPrefix(S3AFileSystem.java:6023)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getExistingFileSizeByListing(S3AFileSystem.java:5972)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1837)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1231)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1208)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1163)\n",
       "\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.create(HadoopFSBackend.scala:153)\n",
       "\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.create(RootFileSystemBackend.scala:264)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$10(FileSystemRequestHandler.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:637)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:542)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperationWithResultTags(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:534)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:502)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:79)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:472)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:472)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:365)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:637)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:534)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:502)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-461411582351070>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m### Partition by one key column\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mYear\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/FileStore/tables\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o819.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 12.0 failed 1 times, most recent failure: Lost task 7.0 in stage 12.0 (TID 118) (ip-10-172-195-128.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to dbfs:/FileStore/tables.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:905)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:515)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:113)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:929)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:929)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:370)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.rmi.RemoteException: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; nested exception is: \n\tcom.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendNonIdempotent(DbfsClient.scala:82)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.createUnbuffered(DatabricksFileSystemV1.scala:127)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.create(DatabricksFileSystemV1.scala:109)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:139)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:135)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:108)\n\tat org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.renewCurrentWriter(FileFormatDataWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:495)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:502)\n\t... 26 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.listObjectsV2(EnforcingDatabricksS3Client.scala:214)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$9(S3AFileSystem.java:2663)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2652)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listOneKeyWithPrefix(S3AFileSystem.java:6023)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getExistingFileSizeByListing(S3AFileSystem.java:5972)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1837)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1231)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1208)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1163)\n\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.create(HadoopFSBackend.scala:153)\n\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.create(RootFileSystemBackend.scala:264)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$10(FileSystemRequestHandler.scala:80)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:637)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:632)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:542)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperationWithResultTags(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:534)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:502)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:23)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:79)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:472)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:472)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:365)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:637)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:632)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:542)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:534)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:502)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3424)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3346)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3335)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3335)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1444)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3635)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3573)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3561)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1193)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1181)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2741)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$5(FileFormatWriter.scala:372)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:336)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:369)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:249)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:207)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$3(commands.scala:132)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:130)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:129)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:955)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to dbfs:/FileStore/tables.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:905)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:515)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:113)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:929)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:929)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:370)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.rmi.RemoteException: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; nested exception is: \n\tcom.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendNonIdempotent(DbfsClient.scala:82)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.createUnbuffered(DatabricksFileSystemV1.scala:127)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.create(DatabricksFileSystemV1.scala:109)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:139)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:135)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:108)\n\tat org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.renewCurrentWriter(FileFormatDataWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:495)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:502)\n\t... 26 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request; request: GET https://databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[devtierprod1/1898845374819542/FileStore/tables/year=\u0000Z)7%0E9�ғz��y��%07QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14\u0000QE%14%01�%27�σ���%2F�%1F%10%2F�%5C%5B�/_started_4060261291005476726], key=[false]} Hadoop 3.4.0, aws-sdk-java/1.12.728 Linux/5.15.0-1062-aws OpenJDK_64-Bit_Server_VM/17.0.11+9-Ubuntu-120.04.2 java/17.0.11 scala/2.12.18-bin-db-3-f50da8f kotlin/1.6.21 vendor/Ubuntu cfg/retry-mode/legacy cfg/auth-source#unknown com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: REJSPE2P7P9Z9GH2, Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=, Cloud Provider: AWS, Instance ID: i-0acc7ccbf8c3eafc6 (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: REJSPE2P7P9Z9GH2; S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=; Proxy: null), S3 Extended Request ID: 6MbUWkNiYuB7MntKsfarYqeYyR8Vu49NDBIOOKUXkEHJ6ezXr6UBd8kimI0EKp9oY78PWVA9cpo=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1880)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.listObjectsV2(EnforcingDatabricksS3Client.scala:214)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$9(S3AFileSystem.java:2663)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2652)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listOneKeyWithPrefix(S3AFileSystem.java:6023)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getExistingFileSizeByListing(S3AFileSystem.java:5972)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1837)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1231)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1208)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1163)\n\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.create(HadoopFSBackend.scala:153)\n\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.create(RootFileSystemBackend.scala:264)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$10(FileSystemRequestHandler.scala:80)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:637)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:632)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:542)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperationWithResultTags(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:534)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:502)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:23)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:79)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:472)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:472)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:365)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:637)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:632)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:542)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:534)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:502)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:261)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:257)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 12.0 failed 1 times, most recent failure: Lost task 7.0 in stage 12.0 (TID 118) (ip-10-172-195-128.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to dbfs:/FileStore/tables.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Partition by one key column\n",
    "df.write.option('header',True).partitionBy('year').mode('overwrite').csv('/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "048aea66-c9f5-492e-b3cf-92bad1b1022c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Partition by multipe key columns\n",
    "dbutils.fs.rm('/FileStore/tables/',True)\n",
    "dbutils.fs.mkdir('/FileStore/tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06cab7ca-eb75-4062-b712-8eb1c9996ee6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.option('header',True).partitionBy('year','sex').mode('overwrite').csv('/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3373145-9672-4411-8bcd-3401a30798ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Partition by key column along with maximum number of records for each partition\n",
    "dbutils.fs.rm('/FileStore/tables/',True)\n",
    "dbutils.fs.mkdir('/FileStore/tables/')\n",
    "\n",
    "df.write.option('header',True).option('maxRecordsPerFile',4200).partitionBy('year','sex').mode('overwrite').csv('/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "712e470a-52d0-4522-b251-9b41a91fcafc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff36194-4ad5-4334-95db-de04da5807f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "#########  How to calculate number of records per  partitions  in df  ############################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732e1a65-6110-4626-946c-8b21dd9c80f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+---+\n|year|   name| percent|sex|\n+----+-------+--------+---+\n|1880|   John|0.081541|boy|\n|1880|William|0.080511|boy|\n|1880|  James|0.050057|boy|\n|1880|Charles|0.045167|boy|\n|1880| George|0.043292|boy|\n|1880|  Frank| 0.02738|boy|\n|1880| Joseph|0.022229|boy|\n|1880| Thomas|0.021401|boy|\n|1880|  Henry|0.020641|boy|\n|1880| Robert|0.020404|boy|\n|1880| Edward|0.019965|boy|\n|1880|  Harry|0.018175|boy|\n|1880| Walter|0.014822|boy|\n|1880| Arthur|0.013504|boy|\n|1880|   Fred|0.013251|boy|\n|1880| Albert|0.012609|boy|\n|1880| Samuel|0.008648|boy|\n|1880|  David|0.007339|boy|\n|1880|  Louis|0.006993|boy|\n|1880|    Joe|0.006174|boy|\n+----+-------+--------+---+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('inferSchema',True).option('header',True).option('sep',',').load('/FileStore/tables/baby_names.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52e21b6-541d-496e-badc-6f49c07ccb76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258000\n"
     ]
    }
   ],
   "source": [
    "##### Number of records in the df\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81dcefe0-d2b5-4fc5-b726-b35448c90212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "##### default partition count\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a93f4ec-1336-463d-809a-771c8ee10064",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|partitionId| count|\n+-----------+------+\n|          0|148060|\n|          1|109940|\n+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "##### Number of records per partition\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "df.withColumn('partitionId',spark_partition_id()).groupBy('partitionId').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61317ac-d6da-4681-8d05-cb241e783b14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###### Repartition the df to 5  ####\n",
    "df_5 = df.select(df.year,df.percent,df.sex,df.name).repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3f2e4e-52d4-459b-a62b-f139b18fe5df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "##### Get number of partitions in the dataframe\n",
    "print(df_5.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad8bcfe-a75b-4a66-bcf8-7f8a77c4f645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|partitionId|count|\n+-----------+-----+\n|          0|51600|\n|          1|51600|\n|          2|51600|\n|          3|51600|\n|          4|51600|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "####### Get number of records per partition\n",
    "df_5.withColumn('partitionId',spark_partition_id()).groupBy('partitionId').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7edd1a2-26e5-4e0d-8438-81a0ed385221",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18318f9b-b487-447b-b686-f712e10b287c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "##############  Null count of al columns in Dataframe      #########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c27c26-7136-4974-bcfa-256b2b1f4d0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th></tr></thead><tbody><tr><td>Michael</td><td>science</td><td>80</td><td>p</td><td>90</td></tr><tr><td>Nancy</td><td>Mathematics</td><td>90</td><td>p</td><td>null</td></tr><tr><td>David</td><td>English</td><td>20</td><td>F</td><td>80</td></tr><tr><td>Jhon</td><td>science</td><td>null</td><td>F</td><td>null</td></tr><tr><td>Martin</td><td>Mathematics</td><td>null</td><td>null</td><td>70</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Michael",
         "science",
         80,
         "p",
         90
        ],
        [
         "Nancy",
         "Mathematics",
         90,
         "p",
         null
        ],
        [
         "David",
         "English",
         20,
         "F",
         80
        ],
        [
         "Jhon",
         "science",
         null,
         "F",
         null
        ],
        [
         "Martin",
         "Mathematics",
         null,
         null,
         70
        ],
        [
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_student = [\n",
    "    (\"Michael\",'science',80,\"p\",90),\n",
    "    (\"Nancy\",\"Mathematics\",90,\"p\",None),\n",
    "    (\"David\",\"English\",20,\"F\",80),\n",
    "    (\"Jhon\",\"science\",None,\"F\",None),\n",
    "    (\"Martin\",\"Mathematics\",None,None,70),\n",
    "    (None,None,None,None,None)\n",
    "]\n",
    "\n",
    "schema = ['name','Subject','Mark','Status','Attendance']\n",
    "df = spark.createDataFrame(data=data_student,schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc527225-e94c-4b13-9019-6056faf917f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>Subject</th><th>Mark</th><th>Status</th><th>Attendance</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>3</td><td>2</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1,
         3,
         2,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Attendance",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########  Find the Null occurences of each column in dataframe\n",
    "from pyspark.sql.functions import col,count,when\n",
    "result = df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns])\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a96e542-60c5-4846-8d6f-0162ee9a1e49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "##############     Top or Bottom N rows per group       #########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a8550c-cad1-4ce6-83b4-f7316de6f58a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>subject</th><th>mark</th><th>status</th><th>attendance</th></tr></thead><tbody><tr><td>Michael</td><td>science</td><td>80</td><td>p</td><td>90</td></tr><tr><td>Nancy</td><td>Mathematics</td><td>90</td><td>p</td><td>null</td></tr><tr><td>David</td><td>English</td><td>20</td><td>F</td><td>80</td></tr><tr><td>Jhon</td><td>science</td><td>null</td><td>F</td><td>null</td></tr><tr><td>Martin</td><td>Mathematics</td><td>null</td><td>null</td><td>70</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Michael",
         "science",
         80,
         "p",
         90
        ],
        [
         "Nancy",
         "Mathematics",
         90,
         "p",
         null
        ],
        [
         "David",
         "English",
         20,
         "F",
         80
        ],
        [
         "Jhon",
         "science",
         null,
         "F",
         null
        ],
        [
         "Martin",
         "Mathematics",
         null,
         null,
         70
        ],
        [
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attendance",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_student = [('Michael','physics',80,'p',90),\n",
    "                  ('Michael','chemistry',67,'p',90),\n",
    "                  ('Michael','Mathematics',78,'p',90),\n",
    "                  ('Nancy','physics',30,'p',80),\n",
    "                  ('Nancy','chemsitry',59,'p',80),\n",
    "                  ('Nancy','Maths',75,'p',90),\n",
    "                  ('Michael','physics',90,'p',70),\n",
    "                  ('Michael','chemsitry',87,'p',70),\n",
    "                  ('Michael','Maths',97,'p',70),\n",
    "                  ('Michael','physics',33,'p',60),\n",
    "                  ('Michael','chemistry',28,'p',60),\n",
    "                  ('Michael','Maths',52,'p',60),\n",
    "                  ('Michael','physics',89,'p',75),\n",
    "                  ('Michael','chemistry',76,'p',75),\n",
    "                  ('Michael','Maths',63,'p',75),\n",
    "                  ]\n",
    "\n",
    "Schema = ['name','subject','mark','status','attendance']\n",
    "df = spark.createDataFrame(data=data_student,schema=Schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9eba72-b5fb-46e9-9a49-ad017dec0632",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>subject</th><th>mark</th><th>status</th><th>attendance</th><th>row</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td></tr><tr><td>David</td><td>English</td><td>20</td><td>F</td><td>80</td><td>1</td></tr><tr><td>Jhon</td><td>science</td><td>null</td><td>F</td><td>null</td><td>1</td></tr><tr><td>Martin</td><td>Mathematics</td><td>null</td><td>null</td><td>70</td><td>1</td></tr><tr><td>Michael</td><td>science</td><td>80</td><td>p</td><td>90</td><td>1</td></tr><tr><td>Nancy</td><td>Mathematics</td><td>90</td><td>p</td><td>null</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         null,
         null,
         1
        ],
        [
         "David",
         "English",
         20,
         "F",
         80,
         1
        ],
        [
         "Jhon",
         "science",
         null,
         "F",
         null,
         1
        ],
        [
         "Martin",
         "Mathematics",
         null,
         null,
         70,
         1
        ],
        [
         "Michael",
         "science",
         80,
         "p",
         90,
         1
        ],
        [
         "Nancy",
         "Mathematics",
         90,
         "p",
         null,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Create rank within each group of Name\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,row_number\n",
    "windowDept = Window.partitionBy('name').orderBy(col('Mark').desc())\n",
    "df2 = df.withColumn('row',row_number().over(windowDept)).orderBy('name','row')\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4fe37b-8e9e-4bf1-95bb-0eaedbf6ae65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>subject</th><th>mark</th><th>status</th><th>attendance</th><th>row</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td></tr><tr><td>David</td><td>English</td><td>20</td><td>F</td><td>80</td><td>1</td></tr><tr><td>Jhon</td><td>science</td><td>null</td><td>F</td><td>null</td><td>1</td></tr><tr><td>Martin</td><td>Mathematics</td><td>null</td><td>null</td><td>70</td><td>1</td></tr><tr><td>Michael</td><td>science</td><td>80</td><td>p</td><td>90</td><td>1</td></tr><tr><td>Nancy</td><td>Mathematics</td><td>90</td><td>p</td><td>null</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         null,
         null,
         1
        ],
        [
         "David",
         "English",
         20,
         "F",
         80,
         1
        ],
        [
         "Jhon",
         "science",
         null,
         "F",
         null,
         1
        ],
        [
         "Martin",
         "Mathematics",
         null,
         null,
         70,
         1
        ],
        [
         "Michael",
         "science",
         80,
         "p",
         90,
         1
        ],
        [
         "Nancy",
         "Mathematics",
         90,
         "p",
         null,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>subject</th><th>mark</th><th>status</th><th>attendance</th><th>row</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td></tr><tr><td>David</td><td>English</td><td>20</td><td>F</td><td>80</td><td>1</td></tr><tr><td>Jhon</td><td>science</td><td>null</td><td>F</td><td>null</td><td>2</td></tr><tr><td>Martin</td><td>Mathematics</td><td>null</td><td>null</td><td>70</td><td>2</td></tr><tr><td>Michael</td><td>science</td><td>80</td><td>p</td><td>90</td><td>1</td></tr><tr><td>Nancy</td><td>Mathematics</td><td>90</td><td>p</td><td>null</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         null,
         null,
         1
        ],
        [
         "David",
         "English",
         20,
         "F",
         80,
         1
        ],
        [
         "Jhon",
         "science",
         null,
         "F",
         null,
         2
        ],
        [
         "Martin",
         "Mathematics",
         null,
         null,
         70,
         2
        ],
        [
         "Michael",
         "science",
         80,
         "p",
         90,
         1
        ],
        [
         "Nancy",
         "Mathematics",
         90,
         "p",
         null,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########  Create top N rows per Group of Name\n",
    "df3 = df2.filter(col('row')<=1)\n",
    "df3.display()\n",
    "\n",
    "\n",
    "#### create rank within each group of Subject\n",
    "windowDept = Window.partitionBy('Subject').orderBy(col(\"Mark\").desc())\n",
    "df4 = df.withColumn('row',row_number().over(windowDept)).orderBy('name','row')\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d248802b-6dfe-4759-8c8a-71bbb76f1d36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>subject</th><th>mark</th><th>status</th><th>attendance</th><th>row</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td></tr><tr><td>David</td><td>English</td><td>20</td><td>F</td><td>80</td><td>1</td></tr><tr><td>Jhon</td><td>science</td><td>null</td><td>F</td><td>null</td><td>1</td></tr><tr><td>Martin</td><td>Mathematics</td><td>null</td><td>null</td><td>70</td><td>1</td></tr><tr><td>Michael</td><td>science</td><td>80</td><td>p</td><td>90</td><td>1</td></tr><tr><td>Nancy</td><td>Mathematics</td><td>90</td><td>p</td><td>null</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         null,
         null,
         1
        ],
        [
         "David",
         "English",
         20,
         "F",
         80,
         1
        ],
        [
         "Jhon",
         "science",
         null,
         "F",
         null,
         1
        ],
        [
         "Martin",
         "Mathematics",
         null,
         null,
         70,
         1
        ],
        [
         "Michael",
         "science",
         80,
         "p",
         90,
         1
        ],
        [
         "Nancy",
         "Mathematics",
         90,
         "p",
         null,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Get top N rows per group of Name\n",
    "df3 = df2.filter(col('row')<=1)\n",
    "df3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3443274f-2b56-46b3-8f3e-058b1b63e18a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>subject</th><th>mark</th><th>status</th><th>attendance</th><th>row</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td></tr><tr><td>David</td><td>English</td><td>20</td><td>F</td><td>80</td><td>1</td></tr><tr><td>Jhon</td><td>science</td><td>null</td><td>F</td><td>null</td><td>1</td></tr><tr><td>Martin</td><td>Mathematics</td><td>null</td><td>null</td><td>70</td><td>1</td></tr><tr><td>Michael</td><td>science</td><td>80</td><td>p</td><td>90</td><td>2</td></tr><tr><td>Nancy</td><td>Mathematics</td><td>90</td><td>p</td><td>null</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         null,
         null,
         1
        ],
        [
         "David",
         "English",
         20,
         "F",
         80,
         1
        ],
        [
         "Jhon",
         "science",
         null,
         "F",
         null,
         1
        ],
        [
         "Martin",
         "Mathematics",
         null,
         null,
         70,
         1
        ],
        [
         "Michael",
         "science",
         80,
         "p",
         90,
         2
        ],
        [
         "Nancy",
         "Mathematics",
         90,
         "p",
         null,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### REverse the Rank to Get Bottom N rows per Group\n",
    "windowDept = Window.partitionBy('Subject').orderBy(col(\"Mark\"))\n",
    "df6 = df.withColumn('row',row_number().over(windowDept)).orderBy('name','row')\n",
    "display(df6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41172837-8ece-4307-9016-f4c01612100c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>subject</th><th>mark</th><th>status</th><th>attendance</th><th>row</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td></tr><tr><td>David</td><td>English</td><td>20</td><td>F</td><td>80</td><td>1</td></tr><tr><td>Jhon</td><td>science</td><td>null</td><td>F</td><td>null</td><td>1</td></tr><tr><td>Martin</td><td>Mathematics</td><td>null</td><td>null</td><td>70</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         null,
         null,
         1
        ],
        [
         "David",
         "English",
         20,
         "F",
         80,
         1
        ],
        [
         "Jhon",
         "science",
         null,
         "F",
         null,
         1
        ],
        [
         "Martin",
         "Mathematics",
         null,
         null,
         70,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mark",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attendance",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Get Bottom N Rows Per group\n",
    "df7 = df6.filter(col('row')<=1)\n",
    "df7.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ea5386-d5ca-4073-b5ec-763d86b7bf6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109cae7a-e121-407b-919d-f9711f691568",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "##############     Greatest / Least vs Max/Min          #########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a86dfd1-1bdb-474f-8d06-882d6b99e549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Student</th><th>Subject_1</th><th>Subject_2</th><th>Subject_3</th><th>Subject_4</th><th>Subject_5</th></tr></thead><tbody><tr><td>David</td><td>70</td><td>68</td><td>89</td><td>40</td><td>74</td></tr><tr><td>Kevin</td><td>90</td><td>67</td><td>87</td><td>79</td><td>74</td></tr><tr><td>Natalia</td><td>66</td><td>88</td><td>49</td><td>65</td><td>72</td></tr><tr><td>Roger</td><td>78</td><td>73</td><td>82</td><td>89</td><td>67</td></tr><tr><td>Micheal</td><td>80</td><td>86</td><td>69</td><td>78</td><td>92</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "David",
         70,
         68,
         89,
         40,
         74
        ],
        [
         "Kevin",
         90,
         67,
         87,
         79,
         74
        ],
        [
         "Natalia",
         66,
         88,
         49,
         65,
         72
        ],
        [
         "Roger",
         78,
         73,
         82,
         89,
         67
        ],
        [
         "Micheal",
         80,
         86,
         69,
         78,
         92
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Student",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject_1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_4",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_5",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_data = [\n",
    "    ('David',70,68,89,40,74),\n",
    "    ('Kevin',90,67,87,79,74),\n",
    "    ('Natalia',66,88,49,65,72),\n",
    "    ('Roger',78,73,82,89,67),\n",
    "    ('Micheal',80,86,69,78,92)\n",
    "]\n",
    "\n",
    "schema = ['Student','Subject_1','Subject_2','Subject_3','Subject_4','Subject_5']\n",
    "df = spark.createDataFrame(input_data,schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85261598-85b0-4fa8-8cff-1d41590339c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Student</th><th>Subject_1</th><th>Subject_2</th><th>Subject_3</th><th>Subject_4</th><th>Subject_5</th><th>Greatest</th></tr></thead><tbody><tr><td>David</td><td>70</td><td>68</td><td>89</td><td>40</td><td>74</td><td>89</td></tr><tr><td>Kevin</td><td>90</td><td>67</td><td>87</td><td>79</td><td>74</td><td>90</td></tr><tr><td>Natalia</td><td>66</td><td>88</td><td>49</td><td>65</td><td>72</td><td>88</td></tr><tr><td>Roger</td><td>78</td><td>73</td><td>82</td><td>89</td><td>67</td><td>89</td></tr><tr><td>Micheal</td><td>80</td><td>86</td><td>69</td><td>78</td><td>92</td><td>92</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "David",
         70,
         68,
         89,
         40,
         74,
         89
        ],
        [
         "Kevin",
         90,
         67,
         87,
         79,
         74,
         90
        ],
        [
         "Natalia",
         66,
         88,
         49,
         65,
         72,
         88
        ],
        [
         "Roger",
         78,
         73,
         82,
         89,
         67,
         89
        ],
        [
         "Micheal",
         80,
         86,
         69,
         78,
         92,
         92
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Student",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject_1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_4",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_5",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Greatest",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### Greatest of Columns \n",
    "\n",
    "from pyspark.sql.functions import greatest\n",
    "\n",
    "greatDF = df.withColumn('Greatest',greatest('Subject_1','Subject_2','Subject_3','Subject_4','Subject_5'))\n",
    "greatDF.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96642b8b-4da7-4f99-8661-722057a58daf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Student</th><th>Subject_1</th><th>Subject_2</th><th>Subject_3</th><th>Subject_4</th><th>Subject_5</th><th>Greatest</th></tr></thead><tbody><tr><td>David</td><td>70</td><td>68</td><td>89</td><td>40</td><td>74</td><td>40</td></tr><tr><td>Kevin</td><td>90</td><td>67</td><td>87</td><td>79</td><td>74</td><td>67</td></tr><tr><td>Natalia</td><td>66</td><td>88</td><td>49</td><td>65</td><td>72</td><td>49</td></tr><tr><td>Roger</td><td>78</td><td>73</td><td>82</td><td>89</td><td>67</td><td>67</td></tr><tr><td>Micheal</td><td>80</td><td>86</td><td>69</td><td>78</td><td>92</td><td>69</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "David",
         70,
         68,
         89,
         40,
         74,
         40
        ],
        [
         "Kevin",
         90,
         67,
         87,
         79,
         74,
         67
        ],
        [
         "Natalia",
         66,
         88,
         49,
         65,
         72,
         49
        ],
        [
         "Roger",
         78,
         73,
         82,
         89,
         67,
         67
        ],
        [
         "Micheal",
         80,
         86,
         69,
         78,
         92,
         69
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Student",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject_1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_4",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Subject_5",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Greatest",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######### Least of columns\n",
    "\n",
    "from pyspark.sql.functions import least\n",
    "greatDF = df.withColumn('Greatest',least('Subject_1','Subject_2','Subject_3','Subject_4','Subject_5'))\n",
    "greatDF.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99aaa06a-ceac-493d-a0d9-1273f748e778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>max(Subject_1)</th></tr></thead><tbody><tr><td>90</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         90
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "max(Subject_1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Max or rows\n",
    "df.agg({'Subject_1':'max'}).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52fed33-8245-4117-ac34-0ee3795e5aad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: DataFrame[min(Subject_2): bigint, min(Subject_1): bigint, min(Subject_4): bigint, min(Subject_3): bigint]"
     ]
    }
   ],
   "source": [
    "#### Min of rows\n",
    "df.agg({'Subject_1':'min','Subject_2':'min','Subject_3':'min','Subject_4':'min'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e0379cd-61f4-4b25-bc07-e30c24ae0457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaed5bfb-505b-483b-bf73-9b511582b1a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eefbae4-c05e-4034-a1d2-27cdfd8c60d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "##############     DELTA   LAKE         #########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f5d06b-a172-47d9-8b68-098b183122de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Refer the hand written notes first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d745fc-28dc-4448-be5e-88df0040e7e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[39]: <delta.tables.DeltaTable at 0x7f54cb96cc40>"
     ]
    }
   ],
   "source": [
    "###### Create Delta Table\n",
    "from delta.tables import *\n",
    "\n",
    "DeltaTable.create(spark).tableName('delta__internal_demo').addColumn('emp_id',\"INT\").addColumn('emp_name',\"STRING\").addColumn('gender',\"STRING\").addColumn('salary','INT').addColumn('Dept',\"STRING\").property('description','table created for demo purpoes').location('/FileStore/tables/delta/arch_demo').execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d69b64-cf6f-4024-b3cc-97290b6961bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/delta/arch_demo/_delta_log/.s3-optimization-0</td><td>.s3-optimization-0</td><td>0</td><td>1721326463000</td></tr><tr><td>dbfs:/FileStore/tables/delta/arch_demo/_delta_log/.s3-optimization-1</td><td>.s3-optimization-1</td><td>0</td><td>1721326463000</td></tr><tr><td>dbfs:/FileStore/tables/delta/arch_demo/_delta_log/.s3-optimization-2</td><td>.s3-optimization-2</td><td>0</td><td>1721326464000</td></tr><tr><td>dbfs:/FileStore/tables/delta/arch_demo/_delta_log/00000000000000000000.crc</td><td>00000000000000000000.crc</td><td>2233</td><td>1721326468000</td></tr><tr><td>dbfs:/FileStore/tables/delta/arch_demo/_delta_log/00000000000000000000.json</td><td>00000000000000000000.json</td><td>1251</td><td>1721326464000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/delta/arch_demo/_delta_log/.s3-optimization-0",
         ".s3-optimization-0",
         0,
         1721326463000
        ],
        [
         "dbfs:/FileStore/tables/delta/arch_demo/_delta_log/.s3-optimization-1",
         ".s3-optimization-1",
         0,
         1721326463000
        ],
        [
         "dbfs:/FileStore/tables/delta/arch_demo/_delta_log/.s3-optimization-2",
         ".s3-optimization-2",
         0,
         1721326464000
        ],
        [
         "dbfs:/FileStore/tables/delta/arch_demo/_delta_log/00000000000000000000.crc",
         "00000000000000000000.crc",
         2233,
         1721326468000
        ],
        [
         "dbfs:/FileStore/tables/delta/arch_demo/_delta_log/00000000000000000000.json",
         "00000000000000000000.json",
         1251,
         1721326464000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables/delta/arch_demo/_delta_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d371a523-7a48-417a-90ee-9dcdebee662b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e49be5-dc1e-456b-bf26-4828410d5a94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{&quot;commitInfo&quot;:{&quot;timestamp&quot;:1721326463028,&quot;userId&quot;:&quot;981225998217974&quot;,&quot;userName&quot;:&quot;2020s18036@stu.cmb.ac.lk&quot;,&quot;operation&quot;:&quot;CREATE TABLE&quot;,&quot;operationParameters&quot;:{&quot;isManaged&quot;:&quot;false&quot;,&quot;description&quot;:null,&quot;partitionBy&quot;:&quot;[]&quot;,&quot;properties&quot;:&quot;{\\&quot;description\\&quot;:\\&quot;table created for demo purpoes\\&quot;}&quot;},&quot;notebook&quot;:{&quot;notebookId&quot;:&quot;73937790193476&quot;},&quot;clusterId&quot;:&quot;0718-172437-6b7a7w9x&quot;,&quot;isolationLevel&quot;:&quot;WriteSerializable&quot;,&quot;isBlindAppend&quot;:true,&quot;operationMetrics&quot;:{},&quot;engineInfo&quot;:&quot;Databricks-Runtime/12.2.x-scala2.12&quot;,&quot;txnId&quot;:&quot;04be0049-2273-4332-b0cc-9d4d42d41acd&quot;}}\n",
       "{&quot;protocol&quot;:{&quot;minReaderVersion&quot;:1,&quot;minWriterVersion&quot;:2}}\n",
       "{&quot;metaData&quot;:{&quot;id&quot;:&quot;1a649c56-cdb6-4a06-ae10-b201a74a1078&quot;,&quot;format&quot;:{&quot;provider&quot;:&quot;parquet&quot;,&quot;options&quot;:{}},&quot;schemaString&quot;:&quot;{\\&quot;type\\&quot;:\\&quot;struct\\&quot;,\\&quot;fields\\&quot;:[{\\&quot;name\\&quot;:\\&quot;emp_id\\&quot;,\\&quot;type\\&quot;:\\&quot;integer\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}},{\\&quot;name\\&quot;:\\&quot;emp_name\\&quot;,\\&quot;type\\&quot;:\\&quot;string\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}},{\\&quot;name\\&quot;:\\&quot;gender\\&quot;,\\&quot;type\\&quot;:\\&quot;string\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}},{\\&quot;name\\&quot;:\\&quot;salary\\&quot;,\\&quot;type\\&quot;:\\&quot;integer\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}},{\\&quot;name\\&quot;:\\&quot;Dept\\&quot;,\\&quot;type\\&quot;:\\&quot;string\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}}]}&quot;,&quot;partitionColumns&quot;:[],&quot;configuration&quot;:{&quot;description&quot;:&quot;table created for demo purpoes&quot;},&quot;createdTime&quot;:1721326462638}}\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">{&quot;commitInfo&quot;:{&quot;timestamp&quot;:1721326463028,&quot;userId&quot;:&quot;981225998217974&quot;,&quot;userName&quot;:&quot;2020s18036@stu.cmb.ac.lk&quot;,&quot;operation&quot;:&quot;CREATE TABLE&quot;,&quot;operationParameters&quot;:{&quot;isManaged&quot;:&quot;false&quot;,&quot;description&quot;:null,&quot;partitionBy&quot;:&quot;[]&quot;,&quot;properties&quot;:&quot;{\\&quot;description\\&quot;:\\&quot;table created for demo purpoes\\&quot;}&quot;},&quot;notebook&quot;:{&quot;notebookId&quot;:&quot;73937790193476&quot;},&quot;clusterId&quot;:&quot;0718-172437-6b7a7w9x&quot;,&quot;isolationLevel&quot;:&quot;WriteSerializable&quot;,&quot;isBlindAppend&quot;:true,&quot;operationMetrics&quot;:{},&quot;engineInfo&quot;:&quot;Databricks-Runtime/12.2.x-scala2.12&quot;,&quot;txnId&quot;:&quot;04be0049-2273-4332-b0cc-9d4d42d41acd&quot;}}\n{&quot;protocol&quot;:{&quot;minReaderVersion&quot;:1,&quot;minWriterVersion&quot;:2}}\n{&quot;metaData&quot;:{&quot;id&quot;:&quot;1a649c56-cdb6-4a06-ae10-b201a74a1078&quot;,&quot;format&quot;:{&quot;provider&quot;:&quot;parquet&quot;,&quot;options&quot;:{}},&quot;schemaString&quot;:&quot;{\\&quot;type\\&quot;:\\&quot;struct\\&quot;,\\&quot;fields\\&quot;:[{\\&quot;name\\&quot;:\\&quot;emp_id\\&quot;,\\&quot;type\\&quot;:\\&quot;integer\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}},{\\&quot;name\\&quot;:\\&quot;emp_name\\&quot;,\\&quot;type\\&quot;:\\&quot;string\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}},{\\&quot;name\\&quot;:\\&quot;gender\\&quot;,\\&quot;type\\&quot;:\\&quot;string\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}},{\\&quot;name\\&quot;:\\&quot;salary\\&quot;,\\&quot;type\\&quot;:\\&quot;integer\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}},{\\&quot;name\\&quot;:\\&quot;Dept\\&quot;,\\&quot;type\\&quot;:\\&quot;string\\&quot;,\\&quot;nullable\\&quot;:true,\\&quot;metadata\\&quot;:{}}]}&quot;,&quot;partitionColumns&quot;:[],&quot;configuration&quot;:{&quot;description&quot;:&quot;table created for demo purpoes&quot;},&quot;createdTime&quot;:1721326462638}}\n\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "head /FileStore/tables/delta/arch_demo/_delta_log/00000000000000000000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d9b86f-5a48-4a32-b70b-34961ecf1475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>1</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "insert into delta__internal_demo values(100,'Stephan',\"M\",2000,'IT');\n",
    "insert into delta__internal_demo values(200,'Philip',\"M\",2000,'HR');\n",
    "insert into delta__internal_demo values(300,'Lara',\"F\",2000,'SALES');\n",
    "insert into delta__internal_demo values(100,'Stephan',\"M\",2000,'IT');\n",
    "insert into delta__internal_demo values(200,'Philip',\"M\",2000,'HR');\n",
    "insert into delta__internal_demo values(100,'Lara',\"F\",2000,'SALES');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef9d680-43f1-4f9a-89d3-4cacc681ea4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>emp_name</th><th>gender</th><th>salary</th><th>Dept</th></tr></thead><tbody><tr><td>100</td><td>Stephan</td><td>M</td><td>2000</td><td>IT</td></tr><tr><td>300</td><td>Lara</td><td>F</td><td>2000</td><td>SALES</td></tr><tr><td>100</td><td>Stephan</td><td>M</td><td>2000</td><td>IT</td></tr><tr><td>100</td><td>Lara</td><td>F</td><td>2000</td><td>SALES</td></tr><tr><td>200</td><td>Philip</td><td>M</td><td>2000</td><td>HR</td></tr><tr><td>200</td><td>Philip</td><td>M</td><td>2000</td><td>HR</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Stephan",
         "M",
         2000,
         "IT"
        ],
        [
         300,
         "Lara",
         "F",
         2000,
         "SALES"
        ],
        [
         100,
         "Stephan",
         "M",
         2000,
         "IT"
        ],
        [
         100,
         "Lara",
         "F",
         2000,
         "SALES"
        ],
        [
         200,
         "Philip",
         "M",
         2000,
         "HR"
        ],
        [
         200,
         "Philip",
         "M",
         2000,
         "HR"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "emp_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Dept",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from delta__internal_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741b75f5-7c2b-4516-840c-73515d6062bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-1081461319282680>:2\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    delete from delta__internal_demo where emp_id =100;\u001B[0m\n",
       "\u001B[0m           ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-1081461319282680>:2\u001B[0;36m\u001B[0m\n\u001B[0;31m    delete from delta__internal_demo where emp_id =100;\u001B[0m\n\u001B[0m           ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-1081461319282680>, line 2)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "% sql\n",
    "delete from delta__internal_demo where emp_id =100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49cd5b54-ab67-451c-956a-65ae01804cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c28b8ad-cb82-42a6-a1ac-b30be8c31e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>emp_name</th><th>gender</th><th>salary</th><th>Dept</th></tr></thead><tbody><tr><td>100</td><td>Stephan</td><td>M</td><td>2000</td><td>IT</td></tr><tr><td>300</td><td>Lara</td><td>F</td><td>2000</td><td>SALES</td></tr><tr><td>100</td><td>Stephan</td><td>M</td><td>2000</td><td>IT</td></tr><tr><td>100</td><td>Lara</td><td>F</td><td>2000</td><td>SALES</td></tr><tr><td>200</td><td>Philip</td><td>M</td><td>2000</td><td>HR</td></tr><tr><td>200</td><td>Philip</td><td>M</td><td>2000</td><td>HR</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Stephan",
         "M",
         2000,
         "IT"
        ],
        [
         300,
         "Lara",
         "F",
         2000,
         "SALES"
        ],
        [
         100,
         "Stephan",
         "M",
         2000,
         "IT"
        ],
        [
         100,
         "Lara",
         "F",
         2000,
         "SALES"
        ],
        [
         200,
         "Philip",
         "M",
         2000,
         "HR"
        ],
        [
         200,
         "Philip",
         "M",
         2000,
         "HR"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "emp_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Dept",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.read.format('delta').load('/FileStore/tables/delta/arch_demo/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade82b6e-2361-471b-80c7-95a913da6834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table delta__internal_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11e39bec-c623-46d3-bb78-27eac59e4c5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "##############     Creating DELTA Tables                #########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ae9eda7-5ebd-4842-aa7d-9cac5147a988",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Method 01 : create \n",
    "DeltaTable.create(spark).tableName('delta__internal_demo').addColumn('emp_id',\"INT\").addColumn('emp_name',\"STRING\").addColumn('gender',\"STRING\").addColumn('salary','INT').addColumn('Dept',\"STRING\").property('description','table created for demo purpoes').location('/FileStore/tables/delta/arch_demo').execute()\n",
    "\n",
    "##### Method 02 : createIfNotExists \n",
    "DeltaTable.createIfNotExists(spark).tableName('delta__internal_demo').addColumn('emp_id',\"INT\").addColumn('emp_name',\"STRING\").addColumn('gender',\"STRING\").addColumn('salary','INT').addColumn('Dept',\"STRING\").property('description','table created for demo purpoes').location('/FileStore/tables/delta/arch_demo').execute()\n",
    "\n",
    "\n",
    "\n",
    "##### Method 03 : createOrReplace\n",
    "DeltaTable.createOrReplace(spark).tableName('delta__internal_demo').addColumn('emp_id',\"INT\").addColumn('emp_name',\"STRING\").addColumn('gender',\"STRING\").addColumn('salary','INT').addColumn('Dept',\"STRING\").property('description','table created for demo purpoes').location('/FileStore/tables/delta/arch_demo').execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf85e01-96a9-4b2d-affa-7de92c68eb75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE employee_demo (\n",
    "    emp_id INT,\n",
    "    emp_Name STRING,\n",
    "    gender STRING,\n",
    "    salary INT,\n",
    "    dept STRING,\n",
    ") USING DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "614b7b58-cb83-44e9-91a3-a3e6cc714fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS \n",
    "\n",
    "employee_demo (\n",
    "    emp_id INT,\n",
    "    emp_Name STRING,\n",
    "    gender STRING,\n",
    "    salary INT,\n",
    "    dept STRING,\n",
    ") USING DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142b4cfc-d270-42f5-9bfd-ada82965d6f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>emp_name</th><th>gender</th><th>salary</th><th>dept</th></tr></thead><tbody><tr><td>100</td><td>Stephan</td><td>M</td><td>2000</td><td>IT</td></tr><tr><td>200</td><td>Philip</td><td>M</td><td>8000</td><td>HR</td></tr><tr><td>300</td><td>Lara</td><td>F</td><td>6000</td><td>SALES</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Stephan",
         "M",
         2000,
         "IT"
        ],
        [
         200,
         "Philip",
         "M",
         8000,
         "HR"
        ],
        [
         300,
         "Lara",
         "F",
         6000,
         "SALES"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "emp_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Another method is using dataframe\n",
    "employee_data = [\n",
    "    (100,'Stephan','M',2000,\"IT\"),\n",
    "    (200,\"Philip\",'M',8000,\"HR\"),\n",
    "    (300,\"Lara\",'F',6000,\"SALES\")\n",
    "]\n",
    "employee_schema = ['emp_id','emp_name','gender','salary','dept']\n",
    "df = spark.createDataFrame(data=employee_data,schema=employee_schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4490dd65-837e-47d3-bb1a-c9fa0a6817b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Create table in the metastore using DataFrame's schema and write data to it\n",
    "df.write.format('delta').saveAsTable('default.employee_demo1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c8feec0-750f-4faf-8b28-9fdaac102647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee9dc730-1714-4c50-ba57-55d6a9eb38cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7037063-2173-4823-aa17-514578fdfa81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "##############     Delta Table Instance               #########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269391bb-cf7d-44dc-a000-1a701584ecf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###  What is Delta Table Instance ?\n",
    "##      * To make soft link(replica) to actual delta table\n",
    "\n",
    "\n",
    "## What is the Usage of it ?\n",
    "##     * To perform DML operations using Pyspark languages\n",
    "\n",
    "\n",
    "### Syntaxes \n",
    "'''\n",
    "DeltaTable.forPath(spark,'/path/to/table')\n",
    "DeltaTable.forName(spark,'table_name')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e8dd14-163c-4cbd-b637-ac996cf01b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[63]: <delta.tables.DeltaTable at 0x7f54e0264880>"
     ]
    }
   ],
   "source": [
    "DeltaTable.create(spark).tableName('employee__demo').addColumn('emp_id',\"INT\").addColumn('emp_name',\"STRING\").addColumn('gender',\"STRING\").addColumn('salary','INT').addColumn('Dept',\"STRING\").property('description','table created for demo purpoes').location('/FileStore/tables/delta/createtable').execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c25cc0-3cef-4517-8a0a-11bfb48e6307",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>1</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "insert into employee__demo values(100,'Stephan',\"M\",2000,'IT');\n",
    "insert into employee__demo values(200,'Philip',\"M\",2000,'HR');\n",
    "insert into employee__demo values(300,'Lara',\"F\",2000,'SALES');\n",
    "insert into employee__demo values(100,'Stephan',\"M\",2000,'IT');\n",
    "insert into employee__demo values(200,'Philip',\"M\",2000,'HR');\n",
    "insert into employee__demo values(100,'Lara',\"F\",2000,'SALES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a4a642-53f2-41e0-81ae-c0c8c981d652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaInstance1 = DeltaTable.forPath(spark,'/FileStore/tables/delta/createtable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219410b7-fc75-4062-bb0b-d530d6ac8ed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>emp_name</th><th>gender</th><th>salary</th><th>Dept</th></tr></thead><tbody><tr><td>100</td><td>Stephan</td><td>M</td><td>2000</td><td>IT</td></tr><tr><td>300</td><td>Lara</td><td>F</td><td>2000</td><td>SALES</td></tr><tr><td>100</td><td>Stephan</td><td>M</td><td>2000</td><td>IT</td></tr><tr><td>100</td><td>Lara</td><td>F</td><td>2000</td><td>SALES</td></tr><tr><td>200</td><td>Philip</td><td>M</td><td>2000</td><td>HR</td></tr><tr><td>200</td><td>Philip</td><td>M</td><td>2000</td><td>HR</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Stephan",
         "M",
         2000,
         "IT"
        ],
        [
         300,
         "Lara",
         "F",
         2000,
         "SALES"
        ],
        [
         100,
         "Stephan",
         "M",
         2000,
         "IT"
        ],
        [
         100,
         "Lara",
         "F",
         2000,
         "SALES"
        ],
        [
         200,
         "Philip",
         "M",
         2000,
         "HR"
        ],
        [
         200,
         "Philip",
         "M",
         2000,
         "HR"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "emp_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Dept",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(deltaInstance1.toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6044823a-2ee6-43a9-82fc-7684f4ee396e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[67]: 4"
     ]
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87b711f-b92c-42ef-8728-dd0cbf4eb06d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "##############       Different approaches to Insert and Delete Data from a DELTA LAKE   ##########\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff23cac-4338-4747-8c7e-deab063ad1af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DeltaTable.create(spark).tableName('employee__demo').addColumn('emp_id',\"INT\").addColumn('emp_name',\"STRING\").addColumn('gender',\"STRING\").addColumn('salary','INT').addColumn('Dept',\"STRING\").property('description','table created for demo purpoes').location('/FileStore/tables/delta/createtable').execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5942c27-e4ee-4af9-93ca-b17db757bc8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "insert into employee__demo values(100,'Stephan',\"M\",2000,'IT');\n",
    "insert into employee__demo values(200,'Philip',\"M\",2000,'HR');\n",
    "insert into employee__demo values(300,'Lara',\"F\",2000,'SALES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e610757-bc8c-4acd-9ff6-daedbd302a0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>emp_name</th><th>gender</th><th>salary</th><th>dept</th></tr></thead><tbody><tr><td>200</td><td>Philipp</td><td>M</td><td>8000</td><td>HR</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         200,
         "Philipp",
         "M",
         8000,
         "HR"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "emp_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Dataframe Insert \n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "employee_data = [(200,'Philipp',\"M\",8000,\"HR\")]\n",
    "\n",
    "employee_schema = StructType([\n",
    "    StructField('emp_id',IntegerType(),False),\n",
    "    StructField('emp_name',StringType(),True),\n",
    "    StructField('gender',StringType(),True),\n",
    "    StructField('salary',IntegerType(),True),\n",
    "    StructField('dept',StringType(),True),\n",
    "\n",
    "    \n",
    "])\n",
    "df = spark.createDataFrame(data=employee_data,schema=employee_schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b67a2f-2ea5-4aa8-9f07-049cdf27c48b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('delta').mode('append').saveAsTable('employee_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9cf63c2-8113-4729-be24-753d420a1db2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##########################  DataFrame Insert Into Method ###################\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "employee_data = [(200,'Lara',\"M\",8000,\"HR\")]\n",
    "\n",
    "employee_schema = StructType([\n",
    "    StructField('emp_id',IntegerType(),False),\n",
    "    StructField('emp_name',StringType(),True),\n",
    "    StructField('gender',StringType(),True),\n",
    "    StructField('salary',IntegerType(),True),\n",
    "    StructField('dept',StringType(),True),\n",
    "\n",
    "    \n",
    "])\n",
    "df1 = spark.createDataFrame(data=employee_data,schema=employee_schema)\n",
    "df1.write.insertInto('employee_demo',overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802572b9-4f6b-4f86-8395-72702a0ad00b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################# Spark SQL Insert\n",
    "spark.sql(\"insert into employee_demo select * from delta_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f5c6d85-95bf-4fc3-8ccd-c7b6c5e24463",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12842dfc-4813-4fc6-9adc-573d911b69e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################################  Delete data from Delta Table   ############################# \n",
    "\n",
    "######### Create delta table ###### \n",
    "from delta.tables import *\n",
    "\n",
    "DeltaTable.create(spark).tableName('employee_demo').addColumn(('emp_id','INT').addColumn('emp_name',\"STRING\").addColumn('gender',\"STRING\").addColumn('salary',\"INT\").addColumn('Dept','STRING').property('description','table created for demo purpose').location('/FileStore/tables/delta/path_employee_demo').execute())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c0bc4d4-025e-48af-928c-8f939523ddae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################  populate sample data ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4361dc8f-d83f-41d6-aeb7-0fe99eb6726d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from employee_demo\n",
    "insert into employee_demo values(100,'Stephen','M',2000,\"IT\");\n",
    "insert into employee_demo values(200,'Philip','M',8000,'HR');\n",
    "insert into employee_demo values(300,'lara',\"F\",6000,'SALES');\n",
    "insert into employee_demo values(400,'Mike','M',4000,\"IT\");\n",
    "insert into employee_demo values(500,'Sarah',\"F\",900,\"HR\");\n",
    "insert into employee_demo values(600,'Serena',\"F\",5000,'SALES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "470b3ce7-e96d-420f-93fd-bb1fb483e7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "delete from employee_demo where emp_id=100;\n",
    "DELETE FROM delta .'/FileStore/tables/delta/path_employee_demo' WHERE emp_id = 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721f71ae-c23d-45be-bbad-b3ea7b8eee0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('delete from employee__demo where emp_id=300')\n",
    "\n",
    "\n",
    "\n",
    "####### Py spark Delta Table Instance \n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.format(spark,'employee_demo')\n",
    "deltaTable.delete('emp_id=400')\n",
    "\n",
    "\n",
    "\n",
    "######### Multiple consitions using SQL predicate ######\n",
    "deltaTable.delete(\"emp_id=500 and gender='F'\")\n",
    "\n",
    "\n",
    "########## pyspark delta table instance \n",
    "deltaTable.delete(col('emp)id')=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3f71852-bd55-4214-948b-9a4f8d322cc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eb63827-3d0b-4bdc-8162-b0d7eb89c1c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c6c2818-991c-4857-bfc9-46785e2215e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "##########################        Update Delta Table          ###################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54036152-e728-44e1-ac67-64f8b41f526d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE employee_demo(emp_id INT, emp_name STRING, gender STRING , salary INT , Dept STRING) USING DELTA LOCATION '/FileStore/tables/delta/path_employee_demo'\n",
    "\n",
    "insert into employee_demo values(100,'Stephen','M',2000,\"IT\");\n",
    "insert into employee_demo values(200,'Philip','M',8000,'HR');\n",
    "insert into employee_demo values(300,'lara',\"F\",6000,'SALES');\n",
    "insert into employee_demo values(400,'Mike','M',4000,\"IT\");\n",
    "insert into employee_demo values(500,'Sarah',\"F\",900,\"HR\");\n",
    "insert into employee_demo values(600,'Serena',\"F\",5000,'SALES');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b4754d-9d41-4654-add2-163b51a2b077",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE employee_demo SET salary = 10000 WHERE emp_name = 'Mark';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "918f07eb-93e2-4990-a1a1-bee2385efdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE delta.'/FileStore/tables/delta/path_employee_demo' SET salary=12000 WHERE emp_name 'Mark';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d17a477c-8236-4d97-9fb0-1d12a60dee85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##############  Method 3 : Pyspark Standard using Table Instance ######\n",
    "\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark,'/FileStore/tables/delta/path_to_employee')\n",
    "\n",
    "#### declare the predicate by using SQL-formatted string\n",
    "deltaTable.update(\n",
    "    condition=\"emp_name= 'Mark'\",\n",
    "    set = {\"salary\":\"15000\"}\n",
    ")\n",
    "\n",
    "\n",
    "#### declare the predicate by using Spark SQL functions\n",
    "deltaTable.update(\n",
    "    condition= col('emp_name')=='Mark'\n",
    "    set = {\"Dept\":lit('IT')}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaa91ca1-6115-446a-9c9d-7085103407ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "665098e7-1c10-4c02-9966-82c9af9d2ecb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "###############################        Merge           ########################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc3c6a8-950a-45f8-bab4-e33f849154cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michael</td><td>Columbus</td><td>USA</td><td>704898508</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michael",
         "Columbus",
         "USA",
         704898508
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([StructField('emp_id',IntegerType(),True),StructField('name',StringType(),True),StructField('city',StringType(),True) ,StructField('country',StringType(),True) , StructField('contact_no',IntegerType(),True)])\n",
    "\n",
    "\n",
    "data = [(1000,'Michael','Columbus',\"USA\",704898508)]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df292cb-e401-4133-b534-45ea2c5c6b97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE dim_employee(\n",
    "  emp_id INT,\n",
    "  name STRING,\n",
    "  city STRING,\n",
    "  country STRING,\n",
    "  contact_no INT\n",
    ")\n",
    "\n",
    "USING DELTA\n",
    "LOCATION '/FileStore/tables/delta_merge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4edee4a-0dff-4cf6-a496-0fd8aaab0a4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dim_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca6c5c7-4ab4-4fd6-9c1e-c94145677780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################### Method : Spark SQL ######\n",
    "df.createOrReplaceTempView('source_view')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdfd38eb-ad29-46b3-b65e-61b868a3691f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michael</td><td>Columbus</td><td>USA</td><td>704898508</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michael",
         "Columbus",
         "USA",
         704898508
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from source_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7110fe-615b-4ff2-86c7-ddcc30aa02a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dim_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ef9235d-5eee-4287-b59f-0a4b32e332c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         0,
         0,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_updated_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_deleted_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "MERGE INTO dim_employee as target \n",
    "USING source_view as source\n",
    " ON target.emp_id = source.emp_id\n",
    " WHEN MATCHED \n",
    "THEN UPDATE SET \n",
    "  target.name = source.name,\n",
    "  target.city = source.city,\n",
    "  target.country = source.country,\n",
    "  target.contact_no = source.contact_no\n",
    "WHEN NOT MATCHED THEN\n",
    "INSERT (emp_id,name,city,country,contact_no) VALUES (emp_id,name,city,country,contact_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb7a7de-12de-4947-b859-95ee282da965",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michael</td><td>Columbus</td><td>USA</td><td>704898508</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michael",
         "Columbus",
         "USA",
         704898508
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dim_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74c7ab48-a269-4727-a67a-4e5519a72827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michel</td><td>Chicago</td><td>USA</td><td>574078374</td></tr><tr><td>2000</td><td>Nancy</td><td>New York</td><td>USA</td><td>5787232</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michel",
         "Chicago",
         "USA",
         574078374
        ],
        [
         2000,
         "Nancy",
         "New York",
         "USA",
         5787232
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(1000,\"Michel\",\"Chicago\",\"USA\",574078374),(2000,\"Nancy\",\"New York\",\"USA\",5787232)]\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7274a807-62a8-4ad0-9a7e-6becc1fe225c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('source_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801db346-7ccd-4537-b81d-fc98ee908672",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michel</td><td>Chicago</td><td>USA</td><td>574078374</td></tr><tr><td>2000</td><td>Nancy</td><td>New York</td><td>USA</td><td>5787232</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michel",
         "Chicago",
         "USA",
         574078374
        ],
        [
         2000,
         "Nancy",
         "New York",
         "USA",
         5787232
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from source_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0450b7c-547c-42fc-9a98-a8b9c5bb46f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michel</td><td>Chicago</td><td>USA</td><td>574078374</td></tr><tr><td>2000</td><td>Nancy</td><td>New York</td><td>USA</td><td>5787232</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michel",
         "Chicago",
         "USA",
         574078374
        ],
        [
         2000,
         "Nancy",
         "New York",
         "USA",
         5787232
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from source_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e501f7c-8c4a-4f7f-8ee8-ee891044c214",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michael</td><td>Columbus</td><td>USA</td><td>704898508</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michael",
         "Columbus",
         "USA",
         704898508
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dim_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dea48f0-5766-424a-a8b9-2aee7652f8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4ea654-a338-46a0-9486-c8a6bf2b2c08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "############  Delta Lake Audit Log Table with Operation Metrics      ############################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa9d61b-aa75-4f5e-aad3-9115ff85c14c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>2000</td><td>Kevin</td><td>New York</td><td>USA</td><td>545345</td></tr><tr><td>3000</td><td>David</td><td>Dallas</td><td>USA</td><td>535345345</td></tr><tr><td>4000</td><td>Peter</td><td>colombus</td><td>USA</td><td>435345</td></tr><tr><td>5000</td><td>Rosy</td><td>Columbus</td><td>USA</td><td>525243</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2000,
         "Kevin",
         "New York",
         "USA",
         545345
        ],
        [
         3000,
         "David",
         "Dallas",
         "USA",
         535345345
        ],
        [
         4000,
         "Peter",
         "colombus",
         "USA",
         435345
        ],
        [
         5000,
         "Rosy",
         "Columbus",
         "USA",
         525243
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([StructField('emp_id',IntegerType(),True),StructField('name',StringType(),True),StructField('city',StringType(),True),StructField('country',StringType(),True),StructField('contact_no',IntegerType(),True)])\n",
    "\n",
    "data = [(2000,'Kevin','New York','USA',545345),(3000,\"David\",\"Dallas\",\"USA\",535345345),(4000,\"Peter\",\"colombus\",\"USA\",435345),(5000,\"Rosy\",\"Columbus\",\"USA\",525243)]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14e25f61-088a-44b0-8f0d-5cba25108f50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE dim_employee(\n",
    "  emp_id INT,\n",
    "  name STRING,\n",
    "  city STRING,\n",
    "  country STRING,\n",
    "  contact_no INT\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/FileStore/tables/delta_merge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b823916-c558-40cc-8eb0-6030279e31c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michael</td><td>Columbus</td><td>USA</td><td>704898508</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michael",
         "Columbus",
         "USA",
         704898508
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dim_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ea834d-3a2e-4171-a59b-bc37f4343bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>2000</td><td>Kevin</td><td>New York</td><td>USA</td><td>545345</td></tr><tr><td>3000</td><td>David</td><td>Dallas</td><td>USA</td><td>535345345</td></tr><tr><td>4000</td><td>Peter</td><td>colombus</td><td>USA</td><td>435345</td></tr><tr><td>5000</td><td>Rosy</td><td>Columbus</td><td>USA</td><td>525243</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2000,
         "Kevin",
         "New York",
         "USA",
         545345
        ],
        [
         3000,
         "David",
         "Dallas",
         "USA",
         535345345
        ],
        [
         4000,
         "Peter",
         "colombus",
         "USA",
         435345
        ],
        [
         5000,
         "Rosy",
         "Columbus",
         "USA",
         525243
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889e8182-1b7e-44c5-82df-544fd172743b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "######### Method 01 #########\n",
    "df.createOrReplaceTempView('source_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f08890-0262-4cc6-8a72-14de48c075ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>2000</td><td>Kevin</td><td>New York</td><td>USA</td><td>545345</td></tr><tr><td>3000</td><td>David</td><td>Dallas</td><td>USA</td><td>535345345</td></tr><tr><td>4000</td><td>Peter</td><td>colombus</td><td>USA</td><td>435345</td></tr><tr><td>5000</td><td>Rosy</td><td>Columbus</td><td>USA</td><td>525243</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2000,
         "Kevin",
         "New York",
         "USA",
         545345
        ],
        [
         3000,
         "David",
         "Dallas",
         "USA",
         535345345
        ],
        [
         4000,
         "Peter",
         "colombus",
         "USA",
         435345
        ],
        [
         5000,
         "Rosy",
         "Columbus",
         "USA",
         525243
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from source_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace5c93a-2fb2-47b7-863f-f4191bd50401",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>4</td><td>0</td><td>0</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         0,
         0,
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_updated_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_deleted_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "MERGE INTO dim_employee as target\n",
    "USING source_view as source\n",
    " ON target.emp_id = source.emp_id\n",
    " WHEN MATCHED\n",
    "THEN UPDATE SET\n",
    " target.name = source.name,\n",
    " target.city = source.city,\n",
    " target.country = source.country,\n",
    " target.contact_no = source.contact_no\n",
    "WHEN NOT MATCHED THEN\n",
    "INSERT (emp_id,name,city,country,contact_no) VALUES (emp_id,name,city,country,contact_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480eb5cb-ef39-4c98-ac78-c280c3d8dc22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>city</th><th>country</th><th>contact_no</th></tr></thead><tbody><tr><td>1000</td><td>Michael</td><td>Columbus</td><td>USA</td><td>704898508</td></tr><tr><td>2000</td><td>Kevin</td><td>New York</td><td>USA</td><td>545345</td></tr><tr><td>4000</td><td>Peter</td><td>colombus</td><td>USA</td><td>435345</td></tr><tr><td>5000</td><td>Rosy</td><td>Columbus</td><td>USA</td><td>525243</td></tr><tr><td>3000</td><td>David</td><td>Dallas</td><td>USA</td><td>535345345</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1000,
         "Michael",
         "Columbus",
         "USA",
         704898508
        ],
        [
         2000,
         "Kevin",
         "New York",
         "USA",
         545345
        ],
        [
         4000,
         "Peter",
         "colombus",
         "USA",
         435345
        ],
        [
         5000,
         "Rosy",
         "Columbus",
         "USA",
         525243
        ],
        [
         3000,
         "David",
         "Dallas",
         "USA",
         535345345
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "contact_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dim_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f208823-7db1-4163-b6c0-86ac7962453c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "create table audit_log( operation STRING, updated_time timestamp , user_name string, notebook_name string, numTargetRowsUpdated int , numTargetRowsInserted int, numTargetRowsDeleted int)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3306b671-c8e6-4cc6-8380-9f64ce2ff957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>operation</th><th>updated_time</th><th>user_name</th><th>notebook_name</th><th>numTargetRowsUpdated</th><th>numTargetRowsInserted</th><th>numTargetRowsDeleted</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "updated_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "user_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "notebook_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsUpdated",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsInserted",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsDeleted",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "select * from audit_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccec122-c962-4e07-9a1a-fff874998182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>3</td><td>2024-07-22T10:06:15.000+0000</td><td>981225998217974</td><td>2020s18036@stu.cmb.ac.lk</td><td>MERGE</td><td>Map(predicate -> [\"(emp_id#6123 = emp_id#3750)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> [])</td><td>null</td><td>List(73937790193476)</td><td>0722-075607-sl9jzrxc</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 4, numTargetBytesAdded -> 6242, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 4294, numTargetRowsInserted -> 4, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 1899, numTargetRowsUpdated -> 0, numOutputRows -> 4, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1466)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "2024-07-22T10:06:15.000+0000",
         "981225998217974",
         "2020s18036@stu.cmb.ac.lk",
         "MERGE",
         {
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedBySourcePredicates": "[]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(emp_id#6123 = emp_id#3750)\"]"
         },
         null,
         [
          "73937790193476"
         ],
         "0722-075607-sl9jzrxc",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "4294",
          "numOutputRows": "4",
          "numSourceRows": "4",
          "numTargetBytesAdded": "6242",
          "numTargetBytesRemoved": "0",
          "numTargetChangeFilesAdded": "0",
          "numTargetDeletionVectorsAdded": "0",
          "numTargetDeletionVectorsRemoved": "0",
          "numTargetFilesAdded": "4",
          "numTargetFilesRemoved": "0",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "4",
          "numTargetRowsMatchedDeleted": "0",
          "numTargetRowsMatchedUpdated": "0",
          "numTargetRowsNotMatchedBySourceDeleted": "0",
          "numTargetRowsNotMatchedBySourceUpdated": "0",
          "numTargetRowsUpdated": "0",
          "rewriteTimeMs": "1466",
          "scanTimeMs": "1899"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Create dataframe with Last Operation in Delta table\n",
    "from delta.tables import *\n",
    "delta_df = DeltaTable.forPath(spark,'/FileStore/tables/delta_merge')\n",
    "\n",
    "lastOperationDF = delta_df.history(1) # get the last operation\n",
    "display(lastOperationDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce0e8cb-f966-41f6-8db9-b548603dc117",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>operation</th><th>key</th><th>value</th></tr></thead><tbody><tr><td>MERGE</td><td>numTargetRowsCopied</td><td>0</td></tr><tr><td>MERGE</td><td>numTargetRowsDeleted</td><td>0</td></tr><tr><td>MERGE</td><td>numTargetFilesAdded</td><td>4</td></tr><tr><td>MERGE</td><td>numTargetBytesAdded</td><td>6242</td></tr><tr><td>MERGE</td><td>numTargetBytesRemoved</td><td>0</td></tr><tr><td>MERGE</td><td>numTargetDeletionVectorsAdded</td><td>0</td></tr><tr><td>MERGE</td><td>numTargetRowsMatchedUpdated</td><td>0</td></tr><tr><td>MERGE</td><td>executionTimeMs</td><td>4294</td></tr><tr><td>MERGE</td><td>numTargetRowsInserted</td><td>4</td></tr><tr><td>MERGE</td><td>numTargetRowsMatchedDeleted</td><td>0</td></tr><tr><td>MERGE</td><td>scanTimeMs</td><td>1899</td></tr><tr><td>MERGE</td><td>numTargetRowsUpdated</td><td>0</td></tr><tr><td>MERGE</td><td>numOutputRows</td><td>4</td></tr><tr><td>MERGE</td><td>numTargetDeletionVectorsRemoved</td><td>0</td></tr><tr><td>MERGE</td><td>numTargetRowsNotMatchedBySourceUpdated</td><td>0</td></tr><tr><td>MERGE</td><td>numTargetChangeFilesAdded</td><td>0</td></tr><tr><td>MERGE</td><td>numSourceRows</td><td>4</td></tr><tr><td>MERGE</td><td>numTargetFilesRemoved</td><td>0</td></tr><tr><td>MERGE</td><td>numTargetRowsNotMatchedBySourceDeleted</td><td>0</td></tr><tr><td>MERGE</td><td>rewriteTimeMs</td><td>1466</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "MERGE",
         "numTargetRowsCopied",
         0
        ],
        [
         "MERGE",
         "numTargetRowsDeleted",
         0
        ],
        [
         "MERGE",
         "numTargetFilesAdded",
         4
        ],
        [
         "MERGE",
         "numTargetBytesAdded",
         6242
        ],
        [
         "MERGE",
         "numTargetBytesRemoved",
         0
        ],
        [
         "MERGE",
         "numTargetDeletionVectorsAdded",
         0
        ],
        [
         "MERGE",
         "numTargetRowsMatchedUpdated",
         0
        ],
        [
         "MERGE",
         "executionTimeMs",
         4294
        ],
        [
         "MERGE",
         "numTargetRowsInserted",
         4
        ],
        [
         "MERGE",
         "numTargetRowsMatchedDeleted",
         0
        ],
        [
         "MERGE",
         "scanTimeMs",
         1899
        ],
        [
         "MERGE",
         "numTargetRowsUpdated",
         0
        ],
        [
         "MERGE",
         "numOutputRows",
         4
        ],
        [
         "MERGE",
         "numTargetDeletionVectorsRemoved",
         0
        ],
        [
         "MERGE",
         "numTargetRowsNotMatchedBySourceUpdated",
         0
        ],
        [
         "MERGE",
         "numTargetChangeFilesAdded",
         0
        ],
        [
         "MERGE",
         "numSourceRows",
         4
        ],
        [
         "MERGE",
         "numTargetFilesRemoved",
         0
        ],
        [
         "MERGE",
         "numTargetRowsNotMatchedBySourceDeleted",
         0
        ],
        [
         "MERGE",
         "rewriteTimeMs",
         1466
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Explode Operation Metrics Column\n",
    "explode_df = lastOperationDF.select(lastOperationDF.operation,explode(lastOperationDF.operationMetrics))\n",
    "explode_df_select = explode_df.select(explode_df.operation,explode_df.key,explode_df.value.cast('int'))\n",
    "display(explode_df_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "071cc48f-709e-42c7-b0e2-fec3e628c234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>operation</th><th>executionTimeMs</th><th>numOutputRows</th><th>numSourceRows</th><th>numTargetBytesAdded</th><th>numTargetBytesRemoved</th><th>numTargetChangeFilesAdded</th><th>numTargetDeletionVectorsAdded</th><th>numTargetDeletionVectorsRemoved</th><th>numTargetFilesAdded</th><th>numTargetFilesRemoved</th><th>numTargetRowsCopied</th><th>numTargetRowsDeleted</th><th>numTargetRowsInserted</th><th>numTargetRowsMatchedDeleted</th><th>numTargetRowsMatchedUpdated</th><th>numTargetRowsNotMatchedBySourceDeleted</th><th>numTargetRowsNotMatchedBySourceUpdated</th><th>numTargetRowsUpdated</th><th>rewriteTimeMs</th><th>scanTimeMs</th></tr></thead><tbody><tr><td>MERGE</td><td>4294</td><td>4</td><td>4</td><td>6242</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4</td><td>0</td><td>0</td><td>0</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1466</td><td>1899</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "MERGE",
         4294,
         4,
         4,
         6242,
         0,
         0,
         0,
         0,
         4,
         0,
         0,
         0,
         4,
         0,
         0,
         0,
         0,
         0,
         1466,
         1899
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "executionTimeMs",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numOutputRows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numSourceRows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetBytesAdded",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetBytesRemoved",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetChangeFilesAdded",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetDeletionVectorsAdded",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetDeletionVectorsRemoved",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetFilesAdded",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetFilesRemoved",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsCopied",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsDeleted",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsInserted",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsMatchedDeleted",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsMatchedUpdated",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsNotMatchedBySourceDeleted",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsNotMatchedBySourceUpdated",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsUpdated",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "rewriteTimeMs",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "scanTimeMs",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Pivot Operation to COnvert Rows to Columns\n",
    "Pivot_DF = explode_df_select.groupBy('operation').pivot('key').sum('value')\n",
    "display(Pivot_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c76d758c-615c-4a11-b724-a3d7b37169a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>operation</th><th>numTargetRowsUpdated</th><th>numTargetRowsInserted</th><th>numTargetRowsDeleted</th></tr></thead><tbody><tr><td>MERGE</td><td>0</td><td>4</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "MERGE",
         0,
         4,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsUpdated",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsInserted",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "numTargetRowsDeleted",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Select only columns Needed for our Audit Log Table\n",
    "Pivot_DF_select = Pivot_DF.select(Pivot_DF.operation,Pivot_DF.numTargetRowsUpdated,Pivot_DF.numTargetRowsInserted,Pivot_DF.numTargetRowsDeleted)\n",
    "display(Pivot_DF_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66852f68-7f8f-47fb-8def-98517b9d3150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cec08da-daae-4a5b-8ae1-71876db4f710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "###################        Slowly Changing Dimension          ####################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db26c0d9-b5f1-47a3-b5b8-79ab9ac6c59f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### A Slowly Changing Dimension (SCD) is a concept in data warehousing that refers to the way dimensions (attributes or features of data) change over time. In Spark, managing SCDs involves keeping track of historical data in a dimension table to capture changes accurately. There are several types of SCDs, but the most common ones are Type 1, Type 2, and Type 3.\n",
    "\n",
    "###################   Types of Slowly Changing Dimensions  ######################\n",
    "\n",
    "####  SCD Type 1: Overwrite\n",
    "#Description: This type simply overwrites the old data with the new data. No historical data is retained.\n",
    "#Use Case: When you don't need to keep track of historical changes.\n",
    "\n",
    "\n",
    "### SCD Type 2: Add New Row\n",
    "# Description: This type adds a new row with a new version to represent the changed data. It also usually includes additional columns like valid_from and valid_to to track the validity period of each row.\n",
    "#Use Case: When you need to keep a full history of changes.\n",
    "\n",
    "\n",
    "### SCD Type 3: Add New Attribute\n",
    "# Description: This type adds a new column to store the previous value of the changing attribute.\n",
    "# Use Case: When you only need to keep track of the current and previous values.\n",
    "\n",
    "### Summary\n",
    "# SCD Type 1: Overwrites old data with new data.\n",
    "# SCD Type 2: Adds new rows to keep a full history of changes.\n",
    "#SCD Type 3: Adds new columns to store previous values of attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf58be7-8d3a-4ad9-a29d-17a930d1c2f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n| id|      name|state|\n+---+----------+-----+\n|  1|  John Doe|   NJ|\n|  2|Jane Smith|   CA|\n+---+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "############# SCD Type 01 #################\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SCDType1\").getOrCreate()\n",
    "\n",
    "# Example DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"NY\"),\n",
    "    (2, \"Jane Smith\", \"CA\")\n",
    "], [\"id\", \"name\", \"state\"])\n",
    "\n",
    "# New data to be merged\n",
    "new_df = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"NJ\"),  # John's state changed\n",
    "    (2, \"Jane Smith\", \"CA\")\n",
    "], [\"id\", \"name\", \"state\"])\n",
    "\n",
    "# Overwrite old data\n",
    "df = new_df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a68a6c6d-9b0c-4aa1-9c6e-717da13faa4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+----------+----------+\n| id|      name|state|valid_from|  valid_to|\n+---+----------+-----+----------+----------+\n|  1|  John Doe|   NY|2023-01-01|2023-06-30|\n|  2|Jane Smith|   CA|2023-01-01|9999-12-31|\n|  1|  John Doe|   NJ|2023-07-01|9999-12-31|\n|  2|Jane Smith|   CA|2023-01-01|9999-12-31|\n+---+----------+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "###################  SCD Type 2  ######################\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SCDType2\").getOrCreate()\n",
    "\n",
    "# Example DataFrame with history\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"NY\", \"2023-01-01\", \"9999-12-31\"),\n",
    "    (2, \"Jane Smith\", \"CA\", \"2023-01-01\", \"9999-12-31\")\n",
    "], [\"id\", \"name\", \"state\", \"valid_from\", \"valid_to\"])\n",
    "\n",
    "# New data to be merged\n",
    "new_df = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"NJ\", \"2023-07-01\", \"9999-12-31\"),  # John's state changed\n",
    "    (2, \"Jane Smith\", \"CA\", \"2023-01-01\", \"9999-12-31\")\n",
    "], [\"id\", \"name\", \"state\", \"valid_from\", \"valid_to\"])\n",
    "\n",
    "# Close the old record by setting valid_to to new record's valid_from date minus one day\n",
    "df = df.withColumn(\"valid_to\", when(df.id == 1, lit(\"2023-06-30\")).otherwise(df.valid_to))\n",
    "# Append the new record\n",
    "df = df.union(new_df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ee14ea-f5ba-4591-a325-42638420f590",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+--------------+\n| id|      name|current_state|previous_state|\n+---+----------+-------------+--------------+\n|  1|  John Doe|           NJ|            NY|\n|  2|Jane Smith|           CA|          null|\n+---+----------+-------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "################# SCD Type 3 ###########################\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SCDType3\").getOrCreate()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"current_state\", StringType(), True),\n",
    "    StructField(\"previous_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Example DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"NY\", None),\n",
    "    (2, \"Jane Smith\", \"CA\", None)\n",
    "], schema)\n",
    "\n",
    "# New data to be merged\n",
    "new_df = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"NJ\", \"NY\"),  # John's state changed\n",
    "    (2, \"Jane Smith\", \"CA\", None)\n",
    "], schema)\n",
    "\n",
    "# Merge the new data\n",
    "df = new_df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84795a4c-5d0e-4147-9ec8-35af5878c80c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5995e47a-5969-41a0-859e-fb6818bb9d27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1716fec9-78fe-4285-8107-ef928dffa310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "###################        Time Travelling            ############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9914617a-b8ec-43f2-8174-d82653cd41f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Time travel in Spark refers to the ability to query historical versions of data stored in a Delta Lake table. Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. With Delta Lake, you can use time travel to access previous versions of the data, which is useful for auditing, debugging, and data versioning.\n",
    "\n",
    "#####Key Features of Time Travel in Delta Lake:\n",
    "# Historical Data Access:\n",
    "# Query previous versions of your data by specifying a particular timestamp or version number.\n",
    "#Data Versioning:\n",
    "#Delta Lake automatically versions the data as it is written, making it possible to retrieve historical data.\n",
    "\n",
    "## ACID Transactions:\n",
    "#Ensure data integrity and consistency with atomic commits, allowing safe access to historical data.\n",
    "\n",
    "## Practical Applications\n",
    "## Data Auditing:Inspect and verify the state of the data at any point in time.\n",
    "## Debugging:Compare the current data with previous versions to identify and debug issues.\n",
    "## Reproducibility: Reproduce experiments or analyses with the exact same data version used originally.\n",
    "## Accidental Deletion Recovery: Recover data that might have been accidentally deleted or updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47fd7edf-04cb-47f5-92e1-836fe4bec6a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE scd2demo(\n",
    "    pk1 INT,\n",
    "    pk2 STRING,\n",
    "    dim1 INT,\n",
    "    dim2 INT,\n",
    "    dim3 INT,\n",
    "    dim4 INT,\n",
    "    active_status STRING,\n",
    "    start_date TIMESTAMP,\n",
    "    end_data TIMESTAMP)\n",
    " USING DELTA\n",
    " LOCATION '/FileStore/tables/new/scd2demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89a3416-113a-4cc1-8d37-a15bf4ad79ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>pk1</th><th>pk2</th><th>dim1</th><th>dim2</th><th>dim3</th><th>dim4</th><th>active_status</th><th>start_date</th><th>end_data</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "pk1",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "pk2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dim1",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dim2",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dim3",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dim4",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "active_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "end_data",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from scd2demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23c81bf-a551-433f-98f0-6ec58b0fb73e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>0</td><td>2024-07-22T10:43:01.000+0000</td><td>981225998217974</td><td>2020s18036@stu.cmb.ac.lk</td><td>CREATE OR REPLACE TABLE</td><td>Map(isManaged -> false, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(73937790193476)</td><td>0722-075607-sl9jzrxc</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "2024-07-22T10:43:01.000+0000",
         "981225998217974",
         "2020s18036@stu.cmb.ac.lk",
         "CREATE OR REPLACE TABLE",
         {
          "description": null,
          "isManaged": "false",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         [
          "73937790193476"
         ],
         "0722-075607-sl9jzrxc",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe history scd2demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d152541f-3f57-45c9-97e6-074004801e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>pk1</th><th>pk2</th><th>dim1</th><th>dim2</th><th>dim3</th><th>dim4</th><th>active_status</th><th>start_date</th><th>end_data</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "pk1",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "pk2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dim1",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dim2",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dim3",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dim4",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "active_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "end_data",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### pyspark approaches\n",
    "#### Method 1 : Pyspark-Timestamp + Table\n",
    "df = spark.read.format('delta').option('timestampAsOf','2024-07-22T10:43:01.000+00:00').table('scd2demo')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6310c9bb-9f1d-434b-9eeb-d79ff33f59a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ab655d5-5040-474c-b0d7-96afbe5653fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "###################       Restore Commands         ############################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "621e9f28-741d-4464-b97d-2924682b5fad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In the context of Delta Lake in PySpark, \"restore commands\" are used to revert a Delta table to a previous version or a specific timestamp. This is useful when you need to undo recent changes, recover from accidental modifications, or revert to a known good state of the data.\n",
    "\n",
    "### Key Commands for Restore in Delta Lake:\n",
    "# RESTORE Version:Restores the table to a specific version number.\n",
    "# RESTORE Timestamp:Restores the table to the state it was at a specific timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e0a8f7-3d8d-4321-a6f4-6ac28125df08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeRestoreExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(1, \"John Doe\"), (2, \"Jane Doe\")]\n",
    "columns = [\"id\", \"name\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write DataFrame to a Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n",
    "\n",
    "# Make an update to the table\n",
    "df_updated = spark.createDataFrame([(1, \"John Doe Updated\"), (2, \"Jane Doe\")], columns)\n",
    "df_updated.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28489451-5bf9-450c-9ee4-bc932a45d4a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+--------------------+---------+--------------------+----+----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|         userId|            userName|operation| operationParameters| job|        notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+---------------+--------------------+---------+--------------------+----+----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      1|2024-07-22 10:55:38|981225998217974|2020s18036@stu.cm...|    WRITE|{mode -> Overwrit...|null|{73937790193476}|0722-075607-sl9jzrxc|          0|WriteSerializable|        false|{numFiles -> 2, n...|        null|Databricks-Runtim...|\n|      0|2024-07-22 10:55:33|981225998217974|2020s18036@stu.cm...|    WRITE|{mode -> Overwrit...|null|{73937790193476}|0722-075607-sl9jzrxc|       null|WriteSerializable|        false|{numFiles -> 2, n...|        null|Databricks-Runtim...|\n+-------+-------------------+---------------+--------------------+---------+--------------------+----+----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "###################### Step 1: Check the History of the Delta Table  #########\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n",
    "\n",
    "# Display the history of the table\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fafc50b-9aa0-498c-a9c9-9d556159228f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Step 2: Restore to a Previous Version\n",
    "# o restore the table to a specific version, use the RESTORE command. However, as of the latest Delta Lake releases, Delta Lake does not support a direct RESTORE command like in SQL databases. Instead, you use a combination of Delta Lake commands to achieve the same effect.\n",
    "\n",
    "# we can use the UPDATE command with a version or timestamp option to simulate restoring to a previous state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3235943d-bcf6-4990-b352-463156550641",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|    name|\n+---+--------+\n|  1|John Doe|\n|  2|Jane Doe|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Revert to a specific version\n",
    "version_to_restore = 0\n",
    "df_restore = spark.read.format(\"delta\").option(\"versionAsOf\", version_to_restore).load(\"/tmp/delta-table\")\n",
    "\n",
    "# Overwrite the table with the restored version\n",
    "df_restore.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n",
    "\n",
    "# Verify the restore\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78cc2340-c644-41af-9f06-295e04de5fec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1738046398906363>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## Step 3: Restore to a Specific Timestamp\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Define the timestamp to restore to\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m timestamp_to_restore \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-07-15T00:00:00.000Z\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m----> 4\u001B[0m df_restore \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimestampAsOf\u001B[39m\u001B[38;5;124m\"\u001B[39m, timestamp_to_restore)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/delta-table\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Overwrite the table with the restored timestamp version\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m df_restore\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/delta-table\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
       "\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: The provided timestamp (2024-07-15 00:00:00.0) is before the earliest version available to this\n",
       "table (2024-07-22 10:55:33.0). Please use a timestamp after 2024-07-22 10:55:33.\n",
       "         "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1738046398906363>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## Step 3: Restore to a Specific Timestamp\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Define the timestamp to restore to\u001B[39;00m\n\u001B[1;32m      3\u001B[0m timestamp_to_restore \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-07-15T00:00:00.000Z\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m df_restore \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimestampAsOf\u001B[39m\u001B[38;5;124m\"\u001B[39m, timestamp_to_restore)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/delta-table\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Overwrite the table with the restored timestamp version\u001B[39;00m\n\u001B[1;32m      7\u001B[0m df_restore\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/delta-table\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: The provided timestamp (2024-07-15 00:00:00.0) is before the earliest version available to this\ntable (2024-07-22 10:55:33.0). Please use a timestamp after 2024-07-22 10:55:33.\n         ",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: The provided timestamp (2024-07-15 00:00:00.0) is before the earliest version available to this\ntable (2024-07-22 10:55:33.0). Please use a timestamp after 2024-07-22 10:55:33.\n         ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Step 3: Restore to a Specific Timestamp\n",
    "# Define the timestamp to restore to\n",
    "timestamp_to_restore = \"2024-07-15T00:00:00.000Z\"\n",
    "df_restore = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_to_restore).load(\"/tmp/delta-table\")\n",
    "\n",
    "# Overwrite the table with the restored timestamp version\n",
    "df_restore.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n",
    "\n",
    "# Verify the restore\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "860dc632-0e90-457e-9145-aaf7df0f31d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbdc366d-4e12-4ee2-a039-b04f5f33af23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d2225a3-4e64-4e81-9328-f13df997abbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c9d151c-9abf-41fa-9460-be7dc3af2439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "###########    Optimize commands , Z-order commands , vacume commands      #######################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc4af2a-10db-4d2c-9e65-0eaa0e86ef8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|    name|\n+---+--------+\n|  1|John Doe|\n|  2|Jane Doe|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "##### The OPTIMIZE command is used to compact small files into larger ones, improving read performance.\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeOptimizeExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n",
    "\n",
    "# Perform optimization\n",
    "delta_table.optimize().executeCompaction()\n",
    "\n",
    "# Verify optimization\n",
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2d8bc40-fad3-422f-b2b1-011f2cd375e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## # Z-Order Command :The ZORDER command is used to co-locate related information in the same set of files. This improves the speed of range queries on those columns.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeZOrderExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a sample Delta table\n",
    "data = [(1, \"John Doe\", \"NY\"), (2, \"Jane Smith\", \"CA\")]\n",
    "columns = [\"id\", \"name\", \"state\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n",
    "\n",
    "# Perform Z-Ordering on the 'id' column\n",
    "delta_table.optimize().executeZOrderBy(\"id\")\n",
    "\n",
    "# Verify Z-Ordering\n",
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22c6b930-0aeb-460f-8e39-edb640171385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "558ce1ea-6d0e-440a-93a3-46fd894ad069",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## ###  Vacuum Command :  The VACUUM command is used to remove old data files and reduce storage costs. By default, Delta Lake retains data files for 30 days, but you can adjust this retention period.\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeVacuumExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n",
    "\n",
    "# Perform vacuum to delete files older than 7 days\n",
    "delta_table.vacuum(7)\n",
    "\n",
    "# Verify vacuum\n",
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd027875-61ad-48d9-9e29-727528cb13d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc85cc43-0dfc-400e-b817-d1254f7642b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ae73168-2281-4aaf-a89e-3049edf165f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "###########               Window  Functions                #####################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8e4230-ec52-4268-bfbf-316b92adb971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Window functions in PySpark are powerful tools that allow you to perform calculations across a set of table rows that are somehow related to the current row. These functions do not cause the rows to be collapsed into a single result like aggregate functions do. Instead, they calculate a value for each row based on the values of that row and the values of other rows within the same partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f566bf3-673a-438a-8626-294059e34118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Types of Window Functions\n",
    "## Ranking Functions: These include row_number(), rank(), dense_rank(), and ntile().\n",
    "## Analytic Functions: These include cume_dist(), percent_rank(), and ntile().\n",
    "## Aggregate Functions: These include sum(), avg(), min(), max(), and count().\n",
    "## Key Components of Window Functions\n",
    "## Partitioning: Defines how to divide the rows into groups, or partitions, to which the window function is applied.\n",
    "## Ordering: Specifies the order in which rows are processed within each partition.\n",
    "## Frame Specification: Defines the subset of rows in the window that is used for the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c1db4f-0719-49e7-9941-28b256905c85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+----+----------+----------+-----------------+----------+\n|   name| department|salary|rank|dense_rank|row_number|       avg_salary|sum_salary|\n+-------+-----------+------+----+----------+----------+-----------------+----------+\n|  Frank|Engineering|  5500|   1|         1|         1|           5500.0|      5500|\n|    Eve|Engineering|  5800|   2|         2|         2|           5650.0|     11300|\n|  David|Engineering|  6000|   3|         3|         3|5766.666666666667|     17300|\n|Charlie|      Sales|  4700|   1|         1|         1|           4700.0|      4700|\n|    Bob|      Sales|  4800|   2|         2|         2|           4750.0|      9500|\n|  Alice|      Sales|  5000|   3|         3|         3|4833.333333333333|     14500|\n+-------+-----------+------+----+----------+----------+-----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WindowFunctionExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", \"Sales\", 5000),\n",
    "    (\"Bob\", \"Sales\", 4800),\n",
    "    (\"Charlie\", \"Sales\", 4700),\n",
    "    (\"David\", \"Engineering\", 6000),\n",
    "    (\"Eve\", \"Engineering\", 5800),\n",
    "    (\"Frank\", \"Engineering\", 5500)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "# Apply window functions\n",
    "df = df.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "df = df.withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\n",
    "df = df.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "df = df.withColumn(\"avg_salary\", F.avg(\"salary\").over(window_spec))\n",
    "df = df.withColumn(\"sum_salary\", F.sum(\"salary\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3683db51-2387-473f-9faf-acde962c7cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ebe2995-c20e-440f-bacb-8ee3673b1085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217044c5-cf7a-4827-afc3-98a2cd1b3ccc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "################################################################################################## \n",
    "###########               Bucketing               #####################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a504c14-5216-44d3-9103-0f1639082631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Bucketing is a technique used in distributed data processing systems like Apache Spark to optimize query performance by physically partitioning data into a fixed number of \"buckets.\" Each bucket contains a subset of the data that shares the same bucket key, which is based on the value of one or more columns. This is particularly useful when you need to perform operations like joins and aggregations on large datasets, as it can significantly reduce the amount of data shuffled across the network.\n",
    "\n",
    "#####    How Bucketing Works   #########\n",
    "#   * Bucketing Columns: You specify one or more columns as the bucketing keys. The values in these columns determine # \n",
    "#                    how the data is distributed among the buckets.\n",
    "#    * Number of Buckets: You define a fixed number of buckets. This number should be chosen based on the size of the #                          dataset and the cluster's resources.\n",
    "#     * Hashing: Each row's bucket key is hashed, and the hash value is used to assign the row to a specific bucket. # \n",
    "#                Rows with the same bucket key values are placed in the same bucket.\n",
    "#      * Physical Layout: The data is physically stored in the file system such that all rows belonging to the same # \n",
    "#             bucket are stored together. This layout helps minimize data shuffling during query execution.\n",
    "\n",
    "########   Advantages of Bucketing  #######\n",
    "## Optimized Joins: When joining bucketed tables, Spark can avoid shuffling data if the tables are bucketed by the join key and have the same number of buckets.\n",
    "## Reduced Shuffling: Bucketing reduces the amount of data shuffled across the network during operations like joins and aggregations.\n",
    "## Improved Query Performance: By physically partitioning the data, bucketing can lead to more efficient query execution.\n",
    "\n",
    "#####  Considerations  #######\n",
    "# Fixed Number of Buckets: The number of buckets is fixed at the time of table creation and cannot be changed without rewriting the data.\n",
    "# Storage Overhead: Bucketing can introduce additional storage overhead due to the physical partitioning of data.\n",
    "# Resource Planning: The number of buckets should be chosen based on the dataset size and cluster resources to ensure optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d54e82-4433-485e-8694-435c4741c406",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### 01 : Creating a Bucketed Table #################\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BucketingExample\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 29),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Cathy\", 31),\n",
    "    (4, \"David\", 29),\n",
    "    (5, \"Eva\", 30)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "\n",
    "\n",
    "############ 02 : Write dataframe to bucketed parquet table ######\n",
    "df.write.bucketBy(3, \"age\").sortBy(\"name\").format(\"parquet\").saveAsTable(\"bucketed_parquet_table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af4a62ad-7a8a-4fd2-96d3-50f82b89c166",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using Partitioning and Z-Ordering in Delta Lake\n",
    "# If you're using Delta Lake and want to optimize performance, you can use partitioning and Z-ordering.\n",
    "\n",
    "#############  01 : Set up Spark Session and Create DataFrame: ######################\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakePartitioningExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 29),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Cathy\", 31),\n",
    "    (4, \"David\", 29),\n",
    "    (5, \"Eva\", 30)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "\n",
    "\n",
    "###################### 02 :  Write DataFrame to a Partitioned Delta Table:  ######################\n",
    "df.write.partitionBy(\"age\").format(\"delta\").save(\"/path/to/delta-table\")\n",
    "\n",
    "\n",
    "##################### 03  : Z-Order by Columns for Better Data Clustering  #####################\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/path/to/delta-table\")\n",
    "\n",
    "# Perform Z-Ordering on a specific column\n",
    "delta_table.optimize().executeZOrderBy(\"name\")\n",
    "\n",
    "# Verify Z-Ordering\n",
    "df = spark.read.format(\"delta\").load(\"/path/to/delta-table\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5232e22a-0e93-4dcd-a132-213aea382ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baedd1b6-3ef3-4f7b-a65a-ec719c5c5539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87fbeaea-094d-48a6-af4f-8d26cb2f8e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca1a6a37-9ace-4467-8a61-a10db00f8d33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "547ce336-b696-40eb-a285-95434a4121ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70cc8e88-53fc-48c0-a67c-feb517723a1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20e6f1d-6b4b-4742-8190-665201fbc7cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c17f89fe-beda-4034-9aee-dff1272a2109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2951eb91-7f46-4d58-a5b4-4066bf1be808",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e67a95eb-82a2-415d-aba1-5a8577ea5bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "214b1271-41bb-4170-a473-2bc43a45c938",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6976c660-f56b-4b19-81df-707dd837247d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f819e77-01eb-4146-98af-b8c4d9a9cd72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4635bc78-926e-48e9-a698-ac7c2b39e9eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eebffbec-b4e6-4cd3-acb4-97fdb484b4ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71f35710-8a17-4b87-8e57-758ef5388736",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98390d98-4da3-4e27-a9e9-e8ce8231a157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "158437a5-e5fc-41e9-b95e-5e7089724109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2626b7d-7b37-46ce-ad3f-99cf912f55b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f56c1d-d720-48b5-afd2-2b396e4a28c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0167d11c-e2fc-43f7-b1a9-44af3fd52080",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a03e5f42-8d05-4402-8a55-cbfc9c797570",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 377344365565916,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook 01 -2024-07-17 12:04:28",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
